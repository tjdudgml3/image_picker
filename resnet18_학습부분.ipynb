{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "resnet18.ipynb의 사본의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRAFFUPI5kPt",
        "outputId": "297b2fb0-d7da-401d-ff60-b669873456c2"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywuGE7J-5mWT"
      },
      "source": [
        "import os \n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import cv2\n",
        "import multiprocessing\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTBcjmKTPVWF",
        "outputId": "7a471d02-fa10-43c8-a2b1-5a6c9926ce87"
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/gdrive/MyDrive/resnet18_16_3\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G23JPw-5ncq",
        "outputId": "d711b7f9-6e1b-409e-e070-17592d56cd92"
      },
      "source": [
        "cpu = multiprocessing.cpu_count() \n",
        "print(\"cpu_count :\", cpu)\n",
        "\n",
        "# 파일에 들어있는 이미지 갯수 확인하기 \n",
        "os.chdir('/content/gdrive/MyDrive/projectimg')\n",
        "file_name = os.listdir()\n",
        "cwd = os.getcwd()\n",
        "\n",
        "img_path = [] \n",
        "for i in range(len(file_name)):\n",
        "  \n",
        "  path = os.path.join(cwd, file_name[i])\n",
        "  print(file_name[i],\":\",len(os.listdir(path)))\n",
        "  # 파일에 들어있는 이미지의 경로를 리스트 형태로 구현 \n",
        "  img_path.append(list(glob.glob(os.path.join(path,\"*\"))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu_count : 40\n",
            "Pet : 5000\n",
            "Person : 5526\n",
            "Kakao-talk : 5432\n",
            "Game : 5000\n",
            "Food : 5753\n",
            "Unknown : 17321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "새로운 데이터를 추가하고 기존데이터를 랜덤으로 골라서 5000개를 맞추는코드."
      ],
      "metadata": {
        "id": "qaLt0sWucG1v"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDPFI5wC2Xv2"
      },
      "source": [
        "\n",
        "# tmp_person = img_path[1][504:1004]\n",
        "# tmp_kakao = img_path[2][568:999]\n",
        "# tmp_food = img_path[4][231:999]\n",
        "# tmp_person2 = img_path[1][:503]\n",
        "# tmp_person3 = img_path[1][1005:5496]\n",
        "# tmp_kakao2 = img_path[2][:567]\n",
        "# print(len(tmp_kakao2))\n",
        "# tmp_kakao3 = img_path[2][1000:5431]\n",
        "# tmp_food2 = img_path[4][:230]\n",
        "# tmp_food3 = img_path[4][1000:5768]\n",
        "# img_path[1] = tmp_person2\n",
        "# img_path[2] = tmp_kakao2\n",
        "# img_path[4] = tmp_food2\n",
        "\n",
        "# for imgs in tmp_person3:\n",
        "#   img_path[1].append(imgs)\n",
        "  \n",
        "# for imgs in tmp_kakao3:\n",
        "#   img_path[2].append(imgs)\n",
        "  \n",
        "# for imgs in tmp_food3:\n",
        "#   img_path[4].append(imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "새로운데이터와 기존데이터를 전부 섞어서 5000개를 맞추는코드."
      ],
      "metadata": {
        "id": "E7AcGfBRcRXf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIylsEYv3aXu"
      },
      "source": [
        "random.shuffle(img_path[1])\n",
        "random.shuffle(img_path[2])\n",
        "random.shuffle(img_path[4])\n",
        "img_path[1] = img_path[1][0:5000]\n",
        "img_path[2] = img_path[2][0:5000]\n",
        "img_path[4] = img_path[4][0:5000]\n",
        "random.shuffle(img_path[3])\n",
        "img_path[3] = img_path[3][:5000]\n",
        "\n",
        "# for imgs in tmp_person:\n",
        "#   img_path[1].append(imgs)\n",
        "# for imgs in tmp_kakao:\n",
        "#   img_path[2].append(imgs)\n",
        "# for imgs in tmp_food:\n",
        "#   img_path[4].append(imgs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffabvAFh5ozK"
      },
      "source": [
        "# unknown에서 5000개 뽑는 과정\n",
        "count = 0\n",
        "random.shuffle(img_path[5])\n",
        "# tmp = img_path[5]\n",
        "# tmp = tmp[:5000]\n",
        "\n",
        "# for i in tmp:\n",
        "#   img = cv2.imread(i)\n",
        "#   if img is None:\n",
        "#       print(\"Img is None:\",i)\n",
        "#   count = count + 1\n",
        "#   if count % 500 == 0:\n",
        "#     print(count,\"th\")\n",
        "img_path[5] = img_path[5][:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAsHJANV5qXq",
        "outputId": "4ad7c2c2-6635-4bdf-8c82-0a2fdbac7d98"
      },
      "source": [
        "#데이터셋 스플릿\n",
        "def split(List):\n",
        "  # shuffle\n",
        "  random.shuffle(List)  \n",
        "\n",
        "  # train data : test data = 8 : 2\n",
        "  split_ratio = int(len(List) * 0.2)\n",
        "  test_set = List[ :split_ratio]\n",
        "  train_set = List[split_ratio: ]\n",
        "  return train_set, test_set\n",
        "\n",
        "train, test = [[],[],[],[],[],[]], [[],[],[],[],[],[]]\n",
        "train_len, test_len = 0, 0\n",
        "\n",
        "# img_path[0] ~ img_path[5]: 각 파일에 있는 이미지경로 \n",
        "for i in range(len(img_path)):  \n",
        "   train[i], test[i] = split(img_path[i])\n",
        "   \n",
        "   #print(file_name[i],\"- train_set:\",len(train[i]),\", test_set\",len(test[i]))\n",
        "   train_len += len(train[i])\n",
        "   test_len += len(test[i])\n",
        "   print(file_name[i],\"- train_set:\",len(train[i]),\", test_set\",len(test[i]))\n",
        "\n",
        "\n",
        "print(\"Num of train:\",train_len,\", Num of test:\",test_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pet - train_set: 4000 , test_set 1000\n",
            "Person - train_set: 4000 , test_set 1000\n",
            "Kakao-talk - train_set: 4000 , test_set 1000\n",
            "Game - train_set: 4000 , test_set 1000\n",
            "Food - train_set: 4000 , test_set 1000\n",
            "Unknown - train_set: 4000 , test_set 1000\n",
            "Num of train: 24000 , Num of test: 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bCYkpe1Fb-h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEkHjkg85sLq",
        "outputId": "c3db3c63-24d3-48c5-cff0-ccf7af1fcd88"
      },
      "source": [
        "#모델 구현\n",
        "#총 18개의 컨볼루션 래여, abced 5개의 블럭이 존재한다.\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, ZeroPadding2D, MaxPool2D,Add,Flatten,Dense,GlobalAveragePooling2D,Input\n",
        "def convA(x):\n",
        "  \n",
        "  x = Conv2D( 64, (7,7),strides = (2,2),padding = 'same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x=  Activation('relu')(x)\n",
        "  x = MaxPool2D((3,3),(1,1),padding ='same')(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def convB(x): \n",
        "  shortcut = x\n",
        "  x = Conv2D(64,(3,3),strides = (2,2),padding = 'same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(64,(3,3),strides = (1,1),padding = 'same')(x)\n",
        "  shortcut = Conv2D(64,(1,1),strides = (2,2), padding = 'valid')(shortcut)\n",
        "  shortcut = BatchNormalization()(shortcut) \n",
        "  x = Add()([x,shortcut])\n",
        "  x = Activation('relu')(x)\n",
        "  \n",
        "  shortcut = x\n",
        "\n",
        "  x = Conv2D(64,(3,3),strides = (1,1),padding ='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(64,(3,3),strides = (1,1),padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Add()([x,shortcut])\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def convC(x):\n",
        "  shortcut = x\n",
        "  x = Conv2D(128,(3,3),strides = (2,2),padding='same')(x) \n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(128,(3,3),strides = (1,1),padding='same')(x)\n",
        "  shortcut = Conv2D(128,(1,1),strides = (2,2), padding = 'valid')(shortcut)\n",
        "  shortcut = BatchNormalization()(shortcut)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Add()([x,shortcut])\n",
        "  x = Activation('relu')(x)\n",
        "  shortcut = x\n",
        "\n",
        "  x = Conv2D(128,(3,3),strides = (1,1),padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(128,(3,3),strides = (1,1),padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Add()([x,shortcut])\n",
        "  x = Activation('relu')(x)\n",
        "  return x\n",
        "\n",
        "def convD(x):\n",
        "  shortcut = x\n",
        "  x = Conv2D( 256,(3,3),strides = (2,2),padding='same')(x) \n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(256,(3,3),strides = (1,1),padding='same')(x)\n",
        "  shortcut = Conv2D(256,(1,1),strides = (2,2), padding = 'valid')(shortcut)\n",
        "  shortcut = BatchNormalization()(shortcut)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Add()([x,shortcut])\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  shortcut = x\n",
        "\n",
        "  x = Conv2D(256,(3,3),strides = (1,1),padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x =Conv2D(256,(3,3),strides = (1,1),padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Add()([x,shortcut])\n",
        "  x = Activation('relu')(x)\n",
        "  return x\n",
        "\n",
        "def convE(x):\n",
        "  shortcut = x\n",
        "  x = Conv2D(512,(3,3),strides = (2,2),padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D( 512,(3,3),strides = (1,1),padding='same')(x)\n",
        "  shortcut = Conv2D(512,(1,1),strides = (2,2), padding = 'valid')(shortcut)\n",
        "  shortcut = BatchNormalization()(shortcut)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Add()([x,shortcut])\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  shortcut = x\n",
        "\n",
        "  x = Conv2D( 512,(3,3),strides = (1,1),padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D( 512,(3,3),strides = (1,1),padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Add()([x,shortcut])\n",
        "  x = Activation('relu')(x)\n",
        "  return x\n",
        "\n",
        "input = Input(shape=(224, 224, 3), dtype='float32', name='input')\n",
        "x = convA(input)\n",
        "x = convB(x)\n",
        "x = convC(x)\n",
        "x = convD(x)\n",
        "x = convE(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Flatten()(x)\n",
        "output = Dense(6,activation= 'softmax')(x) #레이벨 카테고리가 6개\n",
        "model = tf.keras.Model(input,output)\n",
        "\n",
        "loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input (InputLayer)             [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 112, 112, 64  9472        ['input[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 112, 112, 64  256        ['conv2d[0][0]']                 \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 112, 112, 64  0           ['batch_normalization[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 112, 112, 64  0           ['activation[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 56, 56, 64)   36928       ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 56, 56, 64)  256         ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 56, 56, 64)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 56, 56, 64)   4160        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 56, 56, 64)   36928       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 56, 56, 64)  256         ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 56, 56, 64)   0           ['conv2d_2[0][0]',               \n",
            "                                                                  'batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 56, 56, 64)   0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 56, 56, 64)   36928       ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 56, 56, 64)  256         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 56, 56, 64)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 56, 56, 64)   36928       ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 56, 56, 64)  256         ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 56, 56, 64)   0           ['batch_normalization_4[0][0]',  \n",
            "                                                                  'activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 56, 56, 64)   0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 28, 28, 128)  73856       ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 28, 28, 128)  512        ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 28, 28, 128)  0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 28, 28, 128)  147584      ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 28, 28, 128)  8320        ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 28, 28, 128)  512        ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 28, 28, 128)  512        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 28, 28, 128)  0           ['batch_normalization_7[0][0]',  \n",
            "                                                                  'batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 28, 28, 128)  0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 28, 28, 128)  147584      ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 28, 28, 128)  512        ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 28, 28, 128)  0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 28, 28, 128)  147584      ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 28, 28, 128)  512        ['conv2d_10[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 28, 28, 128)  0           ['batch_normalization_9[0][0]',  \n",
            "                                                                  'activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 28, 28, 128)  0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 14, 14, 256)  295168      ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 14, 14, 256)  1024       ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 14, 14, 256)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 14, 14, 256)  590080      ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 14, 14, 256)  33024       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 14, 14, 256)  1024       ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 14, 14, 256)  1024       ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 14, 14, 256)  0           ['batch_normalization_12[0][0]', \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 14, 14, 256)  0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 14, 14, 256)  590080      ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 14, 14, 256)  1024       ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 14, 14, 256)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 14, 14, 256)  590080      ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 14, 14, 256)  1024       ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 14, 14, 256)  0           ['batch_normalization_14[0][0]', \n",
            "                                                                  'activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 14, 14, 256)  0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 7, 7, 512)    1180160     ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 7, 7, 512)    2359808     ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 7, 7, 512)    131584      ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 7, 7, 512)    0           ['batch_normalization_17[0][0]', \n",
            "                                                                  'batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 7, 7, 512)    0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 7, 7, 512)    2359808     ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 7, 7, 512)    2359808     ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 7, 7, 512)    0           ['batch_normalization_19[0][0]', \n",
            "                                                                  'activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 7, 7, 512)    0           ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 512)         0           ['activation_16[0][0]']          \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 512)          0           ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 6)            3078        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,198,150\n",
            "Trainable params: 11,188,550\n",
            "Non-trainable params: 9,600\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6SBLdYO5u-7"
      },
      "source": [
        "def label(List, what):\n",
        "   batch_y=[]\n",
        "   if what == 0: img_agu_num = 1  # train_data일때(데이터 증강 적용)\n",
        "   else: img_agu_num = 1  # test_data일때, train_data의 unknown부분 \n",
        "  \n",
        "   for i in List:\n",
        "\n",
        "     dir,_ = os.path.split(i)\n",
        "         \n",
        "     if dir == '/content/gdrive/.shortcut-targets-by-id/1Mcaf62r3wusYpkZ4Dptx9O4KN16V0-02/projectimg/Pet': batch_y += [0]*img_agu_num\n",
        "     elif dir == '/content/gdrive/.shortcut-targets-by-id/1Mcaf62r3wusYpkZ4Dptx9O4KN16V0-02/projectimg/Person': batch_y += [1]*img_agu_num\n",
        "     elif dir == '/content/gdrive/.shortcut-targets-by-id/1Mcaf62r3wusYpkZ4Dptx9O4KN16V0-02/projectimg/Kakao-talk': batch_y += [2]*img_agu_num\n",
        "     elif dir == '/content/gdrive/.shortcut-targets-by-id/1Mcaf62r3wusYpkZ4Dptx9O4KN16V0-02/projectimg/Game': batch_y += [3]*img_agu_num\n",
        "     elif dir == '/content/gdrive/.shortcut-targets-by-id/1Mcaf62r3wusYpkZ4Dptx9O4KN16V0-02/projectimg/Food': batch_y += [4]*img_agu_num\n",
        "     elif dir == '/content/gdrive/.shortcut-targets-by-id/1Mcaf62r3wusYpkZ4Dptx9O4KN16V0-02/projectimg/Unknown' : batch_y += [5]*img_agu_num\n",
        "    \n",
        "   return batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 증강을 지금은 뺏지만 만약 데이터 증강을 이용하려면 밑에있는 img1~6의 주석을 풀고 레이벨링 함수에서 img_agu_num을 증강하는 종류의 갯수에 따라 올린다."
      ],
      "metadata": {
        "id": "QRPNv1OMcwFX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcIZhnhr5wgr"
      },
      "source": [
        "def preprocess(List, what):\n",
        "  batch_x = []\n",
        "  for r in List:\n",
        "    img = cv2.imread(r)\n",
        "    try:\n",
        "      img = cv2.resize(img, (224,224), interpolation=cv2.INTER_AREA)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    if img is None:\n",
        "      print(\"Img is None:\",r)\n",
        "      pass\n",
        "\n",
        "    elif what == 0:\n",
        "      # img1 = rotation(img).astype(np.float32) / 255.\n",
        "      # img3 = zoom(img).astype(np.float32) / 255.\n",
        "      # img4 = brightness(img).astype(np.float32) / 255.\n",
        "      # #img5 = blur(img).astype(np.float32) / 255.\n",
        "      # img6 = flip(img).astype(np.float32) / 255.\n",
        "      img0 = img.astype(np.float32) / 255.\n",
        "      #batch_x += [img0,img1,img3,img4,img6]\n",
        "      batch_x += [img0]\n",
        "    \n",
        "    else:\n",
        "      img = img.astype(np.float32) / 255.\n",
        "      batch_x += [img]\n",
        "      \n",
        "  return batch_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIfhsn_u5zpb"
      },
      "source": [
        "# 1. Rotation \n",
        "def rotation(img):\n",
        "    h,w = img.shape[:2] \n",
        "    center = (int(w/2), int(h/2)) \n",
        "    move = cv2.getRotationMatrix2D(center, np.random.randint(-30,30), 1.0)\n",
        "    img = cv2.warpAffine(img,move,(w,h))\n",
        "    return img\n",
        "\n",
        "# 2. Flip \n",
        "def flip(img):\n",
        "    # -1:상하좌우, 0:상하, 1:좌우\n",
        "    case = np.random.randint(-1,1+1) \n",
        "    img = cv2.flip(img, 1)\n",
        "    return img\n",
        "\n",
        "# 3. Zoom \n",
        "def zoom(img):\n",
        "    h,w = img.shape[:2] \n",
        "    n = np.random.uniform(0.5, 1, 1)\n",
        "    # 확대할 부분의 시작 x좌표:w_start, y좌표:h_start \n",
        "    # 확대할 부분의 높이:n*h, 너비:n*w \n",
        "    h_start = np.random.randint(0, h - int(n*h))\n",
        "    w_start = np.random.randint(0, w - int(n*w))\n",
        "     \n",
        "    img = img[h_start : h_start+int(n*h), w_start : w_start+int(n*w), ]\n",
        "    img = cv2.resize(img, (h, w), cv2.INTER_CUBIC)\n",
        "    return img \n",
        "\n",
        "# 4. Brightness \n",
        "def brightness(img):\n",
        "    case = np.random.randint(1,2+1)\n",
        "    v = np.random.choice(range(30,50+10,10))\n",
        "    init = np.ones(img.shape, dtype = 'uint8') \n",
        "    \n",
        "    if case == 1:\n",
        "      img = cv2.add(img, init * v)\n",
        "    else: \n",
        "      img = cv2.subtract(img, init * v)\n",
        "    return img\n",
        "\n",
        "# 5. 1) GaussianBlur, 2) medianBlur \n",
        "def blur(img):\n",
        "    case = np.random.randint(1,2+1)\n",
        "    k_size = 3\n",
        "    if case == 1: \n",
        "        img = cv2.GaussianBlur(img, (k_size, k_size), 0)\n",
        "    else: \n",
        "        img = cv2.medianBlur(img, k_size)\n",
        "    return img\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR-dMnm2Yjqe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgI5U2UPmOVZ",
        "outputId": "8c125827-8e41-4f5d-9332-b635e03265d6"
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/gdrive/MyDrive/resnet18_16_4_1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjrDj7r_50Nc",
        "outputId": "280cade2-2b9d-479f-bd01-fa82c80e6b1a"
      },
      "source": [
        "for i in range(12):#한번에 총 12번의 학습. 하지만 데이터 증강을 했을경우엔 4~5번이 maximum 왜냐하면 코랩 연결타임이 24시간이다.\n",
        "  # 첫번째 epoch \n",
        "  batch_size = 60\n",
        "  slice_num, slice_num_test = 0, 0\n",
        "  # 배치사이즈로 나눠서 train하는 과정, 한 배치에 60\n",
        "\n",
        "  batch_num = round(train_len / batch_size)\n",
        "  tmp_slice = int(batch_size/len(train))\n",
        "\n",
        "  tmp_slice_test = int(((test_len / batch_num)*10)/len(test))\n",
        "  result_loss, train_a, test_a = [], [], [] \n",
        "\n",
        "  for i in range(batch_num):\n",
        "\n",
        "\n",
        "    # 0,1,2,3,4,5에서 각각 10개씩 pick, label함수를 통해서 라벨링 \n",
        "    batch_x = []\n",
        "    for j in range(len(train)):\n",
        "\n",
        "      tmp = train[j]\n",
        "      batch_x += tmp[0+slice_num : tmp_slice+slice_num]\n",
        "    slice_num += tmp_slice \n",
        "\n",
        "    random.shuffle(batch_x) \n",
        "\n",
        "    batch_y = np.array(label(batch_x,0))\n",
        "    batch_x = np.array(preprocess(batch_x,0))\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      \n",
        "      logits = model(batch_x)\n",
        "      loss_value = loss_func(batch_y, logits)\n",
        "\n",
        "      if i % 10 == 0:\n",
        "        result_loss.append(loss_value)\n",
        "\n",
        "          #train accruacy \n",
        "        tmp = np.argmax(logits, axis=1)\n",
        "        train_accuracy = np.sum(tmp==batch_y) / (batch_size*1)\n",
        "        train_a.append(train_accuracy)\n",
        "\n",
        "        #test accruacy\n",
        "        #(6000/400)*10=150개씩 삽입 \n",
        "        batch_tx = []\n",
        "        for k in range(len(test)):\n",
        "          tmp1 = test[k]\n",
        "          batch_tx += tmp1[0+slice_num_test : tmp_slice_test+slice_num_test]\n",
        "        slice_num_test += tmp_slice_test \n",
        "       \n",
        "        test_y = np.array(label(batch_tx,1))\n",
        "        test_x = np.array(preprocess(batch_tx,1))\n",
        "     \n",
        "        logit = model(test_x)\n",
        "        tmp1 = np.argmax(logit, axis=1)\n",
        "        test_accuracy = np.sum(tmp1==test_y) / int((test_len / batch_num)*10)\n",
        "        test_a.append(train_accuracy)\n",
        "        \n",
        "        print(\"#\",i,\"th batch - train_accuracy:\",train_accuracy, \", test_accuracy:\",test_accuracy)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    print(i,\"th batch - loss_value:\",np.array(loss_value))\n",
        "  \n",
        "model.save(\"resnet18_lr4\") #모델 세이브.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 0.95 , test_accuracy: 0.9133333333333333\n",
            "0 th batch - loss_value: 0.06804606\n",
            "1 th batch - loss_value: 0.17265606\n",
            "2 th batch - loss_value: 0.58567923\n",
            "3 th batch - loss_value: 0.2846904\n",
            "4 th batch - loss_value: 0.26656774\n",
            "5 th batch - loss_value: 0.21883732\n",
            "6 th batch - loss_value: 0.6599776\n",
            "7 th batch - loss_value: 0.5977543\n",
            "8 th batch - loss_value: 0.14606872\n",
            "9 th batch - loss_value: 0.34404126\n",
            "# 10 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.9533333333333334\n",
            "10 th batch - loss_value: 0.2246713\n",
            "11 th batch - loss_value: 0.14719374\n",
            "12 th batch - loss_value: 0.1278471\n",
            "13 th batch - loss_value: 0.14968814\n",
            "14 th batch - loss_value: 0.028808402\n",
            "15 th batch - loss_value: 0.60732925\n",
            "16 th batch - loss_value: 0.18164387\n",
            "17 th batch - loss_value: 0.32827932\n",
            "18 th batch - loss_value: 0.29623508\n",
            "19 th batch - loss_value: 0.13668174\n",
            "# 20 th batch - train_accuracy: 0.95 , test_accuracy: 0.92\n",
            "20 th batch - loss_value: 0.21575733\n",
            "21 th batch - loss_value: 0.11193379\n",
            "22 th batch - loss_value: 0.11875513\n",
            "23 th batch - loss_value: 0.26649153\n",
            "24 th batch - loss_value: 0.3226584\n",
            "25 th batch - loss_value: 0.08976254\n",
            "26 th batch - loss_value: 0.23348306\n",
            "27 th batch - loss_value: 0.24822491\n",
            "28 th batch - loss_value: 0.0638097\n",
            "29 th batch - loss_value: 0.25634062\n",
            "# 30 th batch - train_accuracy: 0.9 , test_accuracy: 0.9533333333333334\n",
            "30 th batch - loss_value: 0.3000676\n",
            "31 th batch - loss_value: 0.31344202\n",
            "32 th batch - loss_value: 0.4145467\n",
            "33 th batch - loss_value: 0.17180102\n",
            "34 th batch - loss_value: 0.20617215\n",
            "35 th batch - loss_value: 0.1824843\n",
            "36 th batch - loss_value: 0.2956254\n",
            "37 th batch - loss_value: 0.17753248\n",
            "38 th batch - loss_value: 0.1025835\n",
            "39 th batch - loss_value: 0.4797879\n",
            "# 40 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "40 th batch - loss_value: 0.19117482\n",
            "41 th batch - loss_value: 0.05524033\n",
            "42 th batch - loss_value: 0.4128985\n",
            "43 th batch - loss_value: 0.13013436\n",
            "44 th batch - loss_value: 0.16392316\n",
            "45 th batch - loss_value: 0.22586922\n",
            "46 th batch - loss_value: 0.1914648\n",
            "47 th batch - loss_value: 0.108287826\n",
            "48 th batch - loss_value: 0.14473969\n",
            "49 th batch - loss_value: 0.29049984\n",
            "# 50 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9333333333333333\n",
            "50 th batch - loss_value: 0.119078286\n",
            "51 th batch - loss_value: 0.29098818\n",
            "52 th batch - loss_value: 0.2274597\n",
            "53 th batch - loss_value: 0.13543656\n",
            "54 th batch - loss_value: 0.12918472\n",
            "55 th batch - loss_value: 0.14907217\n",
            "56 th batch - loss_value: 0.10190521\n",
            "57 th batch - loss_value: 0.22016834\n",
            "58 th batch - loss_value: 0.31877723\n",
            "59 th batch - loss_value: 0.16227183\n",
            "# 60 th batch - train_accuracy: 0.95 , test_accuracy: 0.9266666666666666\n",
            "60 th batch - loss_value: 0.1946795\n",
            "61 th batch - loss_value: 0.34522882\n",
            "62 th batch - loss_value: 0.29445603\n",
            "63 th batch - loss_value: 0.1232034\n",
            "64 th batch - loss_value: 0.10410872\n",
            "65 th batch - loss_value: 0.2018008\n",
            "66 th batch - loss_value: 0.1462581\n",
            "67 th batch - loss_value: 0.35217628\n",
            "68 th batch - loss_value: 0.29328182\n",
            "69 th batch - loss_value: 0.13882758\n",
            "# 70 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9666666666666667\n",
            "70 th batch - loss_value: 0.19401598\n",
            "71 th batch - loss_value: 0.11990894\n",
            "72 th batch - loss_value: 0.32033396\n",
            "73 th batch - loss_value: 0.2853383\n",
            "74 th batch - loss_value: 0.14501739\n",
            "75 th batch - loss_value: 0.106638454\n",
            "76 th batch - loss_value: 0.1846878\n",
            "77 th batch - loss_value: 0.148623\n",
            "78 th batch - loss_value: 0.18176956\n",
            "79 th batch - loss_value: 0.2347113\n",
            "# 80 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9266666666666666\n",
            "80 th batch - loss_value: 0.13098581\n",
            "81 th batch - loss_value: 0.17858367\n",
            "82 th batch - loss_value: 0.24764912\n",
            "83 th batch - loss_value: 0.25210208\n",
            "84 th batch - loss_value: 0.22859763\n",
            "85 th batch - loss_value: 0.15354405\n",
            "86 th batch - loss_value: 0.07922164\n",
            "87 th batch - loss_value: 0.2790967\n",
            "88 th batch - loss_value: 0.1315259\n",
            "89 th batch - loss_value: 0.11777272\n",
            "# 90 th batch - train_accuracy: 0.8833333333333333 , test_accuracy: 0.9666666666666667\n",
            "90 th batch - loss_value: 0.38420823\n",
            "91 th batch - loss_value: 0.19958247\n",
            "92 th batch - loss_value: 0.07650122\n",
            "93 th batch - loss_value: 0.24547632\n",
            "94 th batch - loss_value: 0.2094123\n",
            "95 th batch - loss_value: 0.24790019\n",
            "96 th batch - loss_value: 0.04830928\n",
            "97 th batch - loss_value: 0.25085837\n",
            "98 th batch - loss_value: 0.22396599\n",
            "99 th batch - loss_value: 0.113381535\n",
            "# 100 th batch - train_accuracy: 0.9 , test_accuracy: 0.9666666666666667\n",
            "100 th batch - loss_value: 0.2603893\n",
            "101 th batch - loss_value: 0.12538953\n",
            "102 th batch - loss_value: 0.14603475\n",
            "103 th batch - loss_value: 0.071111955\n",
            "104 th batch - loss_value: 0.31332815\n",
            "105 th batch - loss_value: 0.18027392\n",
            "106 th batch - loss_value: 0.13676137\n",
            "107 th batch - loss_value: 0.30582204\n",
            "108 th batch - loss_value: 0.17189054\n",
            "109 th batch - loss_value: 0.2138341\n",
            "# 110 th batch - train_accuracy: 0.95 , test_accuracy: 0.9533333333333334\n",
            "110 th batch - loss_value: 0.09450958\n",
            "111 th batch - loss_value: 0.24033208\n",
            "112 th batch - loss_value: 0.11252682\n",
            "113 th batch - loss_value: 0.24767795\n",
            "114 th batch - loss_value: 0.34917873\n",
            "115 th batch - loss_value: 0.2508522\n",
            "116 th batch - loss_value: 0.18931924\n",
            "117 th batch - loss_value: 0.23735194\n",
            "118 th batch - loss_value: 0.15479608\n",
            "119 th batch - loss_value: 0.11863042\n",
            "# 120 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9533333333333334\n",
            "120 th batch - loss_value: 0.113333\n",
            "121 th batch - loss_value: 0.21877874\n",
            "122 th batch - loss_value: 0.17848799\n",
            "123 th batch - loss_value: 0.25988874\n",
            "124 th batch - loss_value: 0.12740895\n",
            "125 th batch - loss_value: 0.1374639\n",
            "126 th batch - loss_value: 0.15768678\n",
            "127 th batch - loss_value: 0.11384628\n",
            "128 th batch - loss_value: 0.1106476\n",
            "129 th batch - loss_value: 0.123639986\n",
            "# 130 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.9533333333333334\n",
            "130 th batch - loss_value: 0.18258253\n",
            "131 th batch - loss_value: 0.16595516\n",
            "132 th batch - loss_value: 0.3020743\n",
            "133 th batch - loss_value: 0.30477458\n",
            "134 th batch - loss_value: 0.07438549\n",
            "135 th batch - loss_value: 0.35364947\n",
            "136 th batch - loss_value: 0.4103461\n",
            "137 th batch - loss_value: 0.22628714\n",
            "138 th batch - loss_value: 0.058842644\n",
            "139 th batch - loss_value: 0.10626726\n",
            "# 140 th batch - train_accuracy: 0.95 , test_accuracy: 0.94\n",
            "140 th batch - loss_value: 0.11692998\n",
            "141 th batch - loss_value: 0.11191113\n",
            "142 th batch - loss_value: 0.20839739\n",
            "143 th batch - loss_value: 0.116356306\n",
            "144 th batch - loss_value: 0.21678957\n",
            "145 th batch - loss_value: 0.122836865\n",
            "146 th batch - loss_value: 0.06262829\n",
            "147 th batch - loss_value: 0.34012762\n",
            "148 th batch - loss_value: 0.15610766\n",
            "149 th batch - loss_value: 0.06811739\n",
            "# 150 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.9466666666666667\n",
            "150 th batch - loss_value: 0.20642774\n",
            "151 th batch - loss_value: 0.09345721\n",
            "152 th batch - loss_value: 0.19850108\n",
            "153 th batch - loss_value: 0.1952279\n",
            "154 th batch - loss_value: 0.1035681\n",
            "155 th batch - loss_value: 0.14053692\n",
            "156 th batch - loss_value: 0.25721678\n",
            "157 th batch - loss_value: 0.24238278\n",
            "158 th batch - loss_value: 0.14552556\n",
            "159 th batch - loss_value: 0.12800325\n",
            "# 160 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.94\n",
            "160 th batch - loss_value: 0.14835678\n",
            "161 th batch - loss_value: 0.06837667\n",
            "162 th batch - loss_value: 0.20311697\n",
            "163 th batch - loss_value: 0.16417247\n",
            "164 th batch - loss_value: 0.09178172\n",
            "165 th batch - loss_value: 0.21428278\n",
            "166 th batch - loss_value: 0.15855584\n",
            "167 th batch - loss_value: 0.15440884\n",
            "168 th batch - loss_value: 0.11794322\n",
            "169 th batch - loss_value: 0.13427784\n",
            "# 170 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "170 th batch - loss_value: 0.05457587\n",
            "171 th batch - loss_value: 0.31652543\n",
            "172 th batch - loss_value: 0.104707554\n",
            "173 th batch - loss_value: 0.22486863\n",
            "174 th batch - loss_value: 0.12530872\n",
            "175 th batch - loss_value: 0.19817528\n",
            "176 th batch - loss_value: 0.1964454\n",
            "177 th batch - loss_value: 0.10159907\n",
            "178 th batch - loss_value: 0.09331637\n",
            "179 th batch - loss_value: 0.20175406\n",
            "# 180 th batch - train_accuracy: 0.95 , test_accuracy: 0.9533333333333334\n",
            "180 th batch - loss_value: 0.18602912\n",
            "181 th batch - loss_value: 0.15616168\n",
            "182 th batch - loss_value: 0.18876056\n",
            "183 th batch - loss_value: 0.08124991\n",
            "184 th batch - loss_value: 0.08535662\n",
            "185 th batch - loss_value: 0.13658153\n",
            "186 th batch - loss_value: 0.11874314\n",
            "187 th batch - loss_value: 0.12731151\n",
            "188 th batch - loss_value: 0.06890596\n",
            "189 th batch - loss_value: 0.1844899\n",
            "# 190 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "190 th batch - loss_value: 0.08664331\n",
            "191 th batch - loss_value: 0.21416311\n",
            "192 th batch - loss_value: 0.09260849\n",
            "193 th batch - loss_value: 0.25032753\n",
            "194 th batch - loss_value: 0.13787399\n",
            "195 th batch - loss_value: 0.11701522\n",
            "196 th batch - loss_value: 0.22799876\n",
            "197 th batch - loss_value: 0.07707581\n",
            "198 th batch - loss_value: 0.21024294\n",
            "199 th batch - loss_value: 0.24750306\n",
            "# 200 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "200 th batch - loss_value: 0.04151334\n",
            "201 th batch - loss_value: 0.149107\n",
            "202 th batch - loss_value: 0.1522056\n",
            "203 th batch - loss_value: 0.16902404\n",
            "204 th batch - loss_value: 0.5155942\n",
            "205 th batch - loss_value: 0.33848146\n",
            "206 th batch - loss_value: 0.1414202\n",
            "207 th batch - loss_value: 0.04644537\n",
            "208 th batch - loss_value: 0.021602925\n",
            "209 th batch - loss_value: 0.36845672\n",
            "# 210 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.98\n",
            "210 th batch - loss_value: 0.31090912\n",
            "211 th batch - loss_value: 0.099212885\n",
            "212 th batch - loss_value: 0.21106032\n",
            "213 th batch - loss_value: 0.13273391\n",
            "214 th batch - loss_value: 0.06674168\n",
            "215 th batch - loss_value: 0.096609704\n",
            "216 th batch - loss_value: 0.22524054\n",
            "217 th batch - loss_value: 0.2429341\n",
            "218 th batch - loss_value: 0.18647888\n",
            "219 th batch - loss_value: 0.21735828\n",
            "# 220 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.9266666666666666\n",
            "220 th batch - loss_value: 0.18715748\n",
            "221 th batch - loss_value: 0.10902914\n",
            "222 th batch - loss_value: 0.21241647\n",
            "223 th batch - loss_value: 0.112044245\n",
            "224 th batch - loss_value: 0.18739472\n",
            "225 th batch - loss_value: 0.05597005\n",
            "226 th batch - loss_value: 0.19529393\n",
            "227 th batch - loss_value: 0.31586653\n",
            "228 th batch - loss_value: 0.22646944\n",
            "229 th batch - loss_value: 0.05358111\n",
            "# 230 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.9266666666666666\n",
            "230 th batch - loss_value: 0.2045284\n",
            "231 th batch - loss_value: 0.05070152\n",
            "232 th batch - loss_value: 0.112681985\n",
            "233 th batch - loss_value: 0.11453716\n",
            "234 th batch - loss_value: 0.2839473\n",
            "235 th batch - loss_value: 0.15079604\n",
            "236 th batch - loss_value: 0.10454534\n",
            "237 th batch - loss_value: 0.13182256\n",
            "238 th batch - loss_value: 0.31010053\n",
            "239 th batch - loss_value: 0.14516412\n",
            "# 240 th batch - train_accuracy: 0.95 , test_accuracy: 0.92\n",
            "240 th batch - loss_value: 0.19178571\n",
            "241 th batch - loss_value: 0.100129336\n",
            "242 th batch - loss_value: 0.094216496\n",
            "243 th batch - loss_value: 0.14563356\n",
            "244 th batch - loss_value: 0.15898946\n",
            "245 th batch - loss_value: 0.46327883\n",
            "246 th batch - loss_value: 0.12602094\n",
            "247 th batch - loss_value: 0.15734074\n",
            "248 th batch - loss_value: 0.31964117\n",
            "249 th batch - loss_value: 0.12414756\n",
            "# 250 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.94\n",
            "250 th batch - loss_value: 0.300715\n",
            "251 th batch - loss_value: 0.4036628\n",
            "252 th batch - loss_value: 0.1804253\n",
            "253 th batch - loss_value: 0.13451582\n",
            "254 th batch - loss_value: 0.18123904\n",
            "255 th batch - loss_value: 0.22493008\n",
            "256 th batch - loss_value: 0.12683995\n",
            "257 th batch - loss_value: 0.17684405\n",
            "258 th batch - loss_value: 0.19936377\n",
            "259 th batch - loss_value: 0.24030717\n",
            "# 260 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9666666666666667\n",
            "260 th batch - loss_value: 0.13120121\n",
            "261 th batch - loss_value: 0.15998626\n",
            "262 th batch - loss_value: 0.3232779\n",
            "263 th batch - loss_value: 0.23623882\n",
            "264 th batch - loss_value: 0.14106275\n",
            "265 th batch - loss_value: 0.13187346\n",
            "266 th batch - loss_value: 0.12887007\n",
            "267 th batch - loss_value: 0.09593172\n",
            "268 th batch - loss_value: 0.055495664\n",
            "269 th batch - loss_value: 0.29203272\n",
            "# 270 th batch - train_accuracy: 0.9 , test_accuracy: 0.9666666666666667\n",
            "270 th batch - loss_value: 0.27887443\n",
            "271 th batch - loss_value: 0.0832406\n",
            "272 th batch - loss_value: 0.1750441\n",
            "273 th batch - loss_value: 0.18339011\n",
            "274 th batch - loss_value: 0.09812685\n",
            "275 th batch - loss_value: 0.20158723\n",
            "276 th batch - loss_value: 0.14387019\n",
            "277 th batch - loss_value: 0.31488526\n",
            "278 th batch - loss_value: 0.14888436\n",
            "279 th batch - loss_value: 0.3023256\n",
            "# 280 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9333333333333333\n",
            "280 th batch - loss_value: 0.13074142\n",
            "281 th batch - loss_value: 0.15289447\n",
            "282 th batch - loss_value: 0.34810245\n",
            "283 th batch - loss_value: 0.078507856\n",
            "284 th batch - loss_value: 0.1441361\n",
            "285 th batch - loss_value: 0.17671333\n",
            "286 th batch - loss_value: 0.2123464\n",
            "287 th batch - loss_value: 0.19121055\n",
            "288 th batch - loss_value: 0.12944375\n",
            "289 th batch - loss_value: 0.107047945\n",
            "# 290 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9733333333333334\n",
            "290 th batch - loss_value: 0.2514688\n",
            "291 th batch - loss_value: 0.21970704\n",
            "292 th batch - loss_value: 0.33515632\n",
            "293 th batch - loss_value: 0.19624004\n",
            "294 th batch - loss_value: 0.30439395\n",
            "295 th batch - loss_value: 0.2209579\n",
            "296 th batch - loss_value: 0.10172234\n",
            "297 th batch - loss_value: 0.068080716\n",
            "298 th batch - loss_value: 0.15744074\n",
            "299 th batch - loss_value: 0.28436393\n",
            "# 300 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9666666666666667\n",
            "300 th batch - loss_value: 0.2852258\n",
            "301 th batch - loss_value: 0.25218883\n",
            "302 th batch - loss_value: 0.14112417\n",
            "303 th batch - loss_value: 0.33782244\n",
            "304 th batch - loss_value: 0.29079008\n",
            "305 th batch - loss_value: 0.23717286\n",
            "306 th batch - loss_value: 0.085152\n",
            "307 th batch - loss_value: 0.11703944\n",
            "308 th batch - loss_value: 0.13604864\n",
            "309 th batch - loss_value: 0.114400774\n",
            "# 310 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9066666666666666\n",
            "310 th batch - loss_value: 0.10737577\n",
            "311 th batch - loss_value: 0.08926951\n",
            "312 th batch - loss_value: 0.18493453\n",
            "313 th batch - loss_value: 0.11600568\n",
            "314 th batch - loss_value: 0.15377194\n",
            "315 th batch - loss_value: 0.29306278\n",
            "316 th batch - loss_value: 0.22256033\n",
            "317 th batch - loss_value: 0.112785645\n",
            "318 th batch - loss_value: 0.08785019\n",
            "319 th batch - loss_value: 0.118852004\n",
            "# 320 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9466666666666667\n",
            "320 th batch - loss_value: 0.09194896\n",
            "321 th batch - loss_value: 0.21778889\n",
            "322 th batch - loss_value: 0.17329225\n",
            "323 th batch - loss_value: 0.08676492\n",
            "324 th batch - loss_value: 0.3169724\n",
            "325 th batch - loss_value: 0.19178186\n",
            "326 th batch - loss_value: 0.0834701\n",
            "327 th batch - loss_value: 0.15965246\n",
            "328 th batch - loss_value: 0.20245108\n",
            "329 th batch - loss_value: 0.11885311\n",
            "# 330 th batch - train_accuracy: 0.95 , test_accuracy: 0.9133333333333333\n",
            "330 th batch - loss_value: 0.15138823\n",
            "331 th batch - loss_value: 0.105458304\n",
            "332 th batch - loss_value: 0.29800108\n",
            "333 th batch - loss_value: 0.08893552\n",
            "334 th batch - loss_value: 0.17824404\n",
            "335 th batch - loss_value: 0.2505783\n",
            "336 th batch - loss_value: 0.13931233\n",
            "337 th batch - loss_value: 0.2842185\n",
            "338 th batch - loss_value: 0.1930027\n",
            "339 th batch - loss_value: 0.033357803\n",
            "# 340 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "340 th batch - loss_value: 0.085834175\n",
            "341 th batch - loss_value: 0.12274934\n",
            "342 th batch - loss_value: 0.11919557\n",
            "343 th batch - loss_value: 0.110366955\n",
            "344 th batch - loss_value: 0.36771664\n",
            "345 th batch - loss_value: 0.18312784\n",
            "346 th batch - loss_value: 0.08345691\n",
            "347 th batch - loss_value: 0.11868881\n",
            "348 th batch - loss_value: 0.27536124\n",
            "349 th batch - loss_value: 0.19794697\n",
            "# 350 th batch - train_accuracy: 0.95 , test_accuracy: 0.96\n",
            "350 th batch - loss_value: 0.195015\n",
            "351 th batch - loss_value: 0.14420684\n",
            "352 th batch - loss_value: 0.18656158\n",
            "353 th batch - loss_value: 0.2244367\n",
            "354 th batch - loss_value: 0.15024178\n",
            "355 th batch - loss_value: 0.138488\n",
            "356 th batch - loss_value: 0.1652972\n",
            "357 th batch - loss_value: 0.21484907\n",
            "358 th batch - loss_value: 0.08965459\n",
            "359 th batch - loss_value: 0.18998027\n",
            "# 360 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.9666666666666667\n",
            "360 th batch - loss_value: 0.23940808\n",
            "361 th batch - loss_value: 0.13752553\n",
            "362 th batch - loss_value: 0.16806006\n",
            "363 th batch - loss_value: 0.093066305\n",
            "364 th batch - loss_value: 0.09458154\n",
            "365 th batch - loss_value: 0.07781558\n",
            "366 th batch - loss_value: 0.21473505\n",
            "367 th batch - loss_value: 0.3611132\n",
            "368 th batch - loss_value: 0.085783996\n",
            "369 th batch - loss_value: 0.16803259\n",
            "# 370 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.94\n",
            "370 th batch - loss_value: 0.25416616\n",
            "371 th batch - loss_value: 0.09835593\n",
            "372 th batch - loss_value: 0.18691069\n",
            "373 th batch - loss_value: 0.18061526\n",
            "374 th batch - loss_value: 0.07721184\n",
            "375 th batch - loss_value: 0.14744951\n",
            "376 th batch - loss_value: 0.21354164\n",
            "377 th batch - loss_value: 0.2300035\n",
            "378 th batch - loss_value: 0.11744258\n",
            "379 th batch - loss_value: 0.29717922\n",
            "# 380 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9733333333333334\n",
            "380 th batch - loss_value: 0.095717564\n",
            "381 th batch - loss_value: 0.10531415\n",
            "382 th batch - loss_value: 0.05486572\n",
            "383 th batch - loss_value: 0.22409277\n",
            "384 th batch - loss_value: 0.22420734\n",
            "385 th batch - loss_value: 0.1622634\n",
            "386 th batch - loss_value: 0.10050006\n",
            "387 th batch - loss_value: 0.16305715\n",
            "388 th batch - loss_value: 0.17288837\n",
            "389 th batch - loss_value: 0.06973076\n",
            "# 390 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "390 th batch - loss_value: 0.09662545\n",
            "391 th batch - loss_value: 0.25272697\n",
            "392 th batch - loss_value: 0.07036645\n",
            "393 th batch - loss_value: 0.08625027\n",
            "394 th batch - loss_value: 0.098446004\n",
            "395 th batch - loss_value: 0.06288683\n",
            "396 th batch - loss_value: 0.15949573\n",
            "397 th batch - loss_value: 0.12284113\n",
            "398 th batch - loss_value: 0.16540584\n",
            "399 th batch - loss_value: 0.11461048\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.94\n",
            "0 th batch - loss_value: 0.033890463\n",
            "1 th batch - loss_value: 0.14976715\n",
            "2 th batch - loss_value: 0.02431783\n",
            "3 th batch - loss_value: 0.048994415\n",
            "4 th batch - loss_value: 0.06447379\n",
            "5 th batch - loss_value: 0.097595744\n",
            "6 th batch - loss_value: 0.05155587\n",
            "7 th batch - loss_value: 0.06757107\n",
            "8 th batch - loss_value: 0.022503119\n",
            "9 th batch - loss_value: 0.04599899\n",
            "# 10 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "10 th batch - loss_value: 0.03240338\n",
            "11 th batch - loss_value: 0.028649164\n",
            "12 th batch - loss_value: 0.026828157\n",
            "13 th batch - loss_value: 0.02682708\n",
            "14 th batch - loss_value: 0.0140828015\n",
            "15 th batch - loss_value: 0.17302608\n",
            "16 th batch - loss_value: 0.058066726\n",
            "17 th batch - loss_value: 0.19963533\n",
            "18 th batch - loss_value: 0.023363315\n",
            "19 th batch - loss_value: 0.038717486\n",
            "# 20 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9533333333333334\n",
            "20 th batch - loss_value: 0.052306045\n",
            "21 th batch - loss_value: 0.009923173\n",
            "22 th batch - loss_value: 0.034401592\n",
            "23 th batch - loss_value: 0.025078867\n",
            "24 th batch - loss_value: 0.035107557\n",
            "25 th batch - loss_value: 0.027931264\n",
            "26 th batch - loss_value: 0.014756231\n",
            "27 th batch - loss_value: 0.11403429\n",
            "28 th batch - loss_value: 0.042496063\n",
            "29 th batch - loss_value: 0.06438068\n",
            "# 30 th batch - train_accuracy: 0.95 , test_accuracy: 0.9466666666666667\n",
            "30 th batch - loss_value: 0.18189025\n",
            "31 th batch - loss_value: 0.091864556\n",
            "32 th batch - loss_value: 0.1998013\n",
            "33 th batch - loss_value: 0.027721643\n",
            "34 th batch - loss_value: 0.025387445\n",
            "35 th batch - loss_value: 0.11650332\n",
            "36 th batch - loss_value: 0.26436183\n",
            "37 th batch - loss_value: 0.06654612\n",
            "38 th batch - loss_value: 0.13607484\n",
            "39 th batch - loss_value: 0.11636858\n",
            "# 40 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.94\n",
            "40 th batch - loss_value: 0.044298973\n",
            "41 th batch - loss_value: 0.09162667\n",
            "42 th batch - loss_value: 0.049242478\n",
            "43 th batch - loss_value: 0.074776694\n",
            "44 th batch - loss_value: 0.0077223447\n",
            "45 th batch - loss_value: 0.11601819\n",
            "46 th batch - loss_value: 0.062766984\n",
            "47 th batch - loss_value: 0.035952393\n",
            "48 th batch - loss_value: 0.037513565\n",
            "49 th batch - loss_value: 0.16275765\n",
            "# 50 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "50 th batch - loss_value: 0.03614511\n",
            "51 th batch - loss_value: 0.15173991\n",
            "52 th batch - loss_value: 0.116189405\n",
            "53 th batch - loss_value: 0.011725335\n",
            "54 th batch - loss_value: 0.04331156\n",
            "55 th batch - loss_value: 0.06606299\n",
            "56 th batch - loss_value: 0.15758413\n",
            "57 th batch - loss_value: 0.049585436\n",
            "58 th batch - loss_value: 0.15959778\n",
            "59 th batch - loss_value: 0.08577343\n",
            "# 60 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.96\n",
            "60 th batch - loss_value: 0.12854223\n",
            "61 th batch - loss_value: 0.015604319\n",
            "62 th batch - loss_value: 0.03336986\n",
            "63 th batch - loss_value: 0.046319485\n",
            "64 th batch - loss_value: 0.024240263\n",
            "65 th batch - loss_value: 0.04383712\n",
            "66 th batch - loss_value: 0.0093199415\n",
            "67 th batch - loss_value: 0.03550255\n",
            "68 th batch - loss_value: 0.09109768\n",
            "69 th batch - loss_value: 0.059896827\n",
            "# 70 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9533333333333334\n",
            "70 th batch - loss_value: 0.059844356\n",
            "71 th batch - loss_value: 0.03204007\n",
            "72 th batch - loss_value: 0.08102813\n",
            "73 th batch - loss_value: 0.22530885\n",
            "74 th batch - loss_value: 0.016061233\n",
            "75 th batch - loss_value: 0.027784174\n",
            "76 th batch - loss_value: 0.03300924\n",
            "77 th batch - loss_value: 0.0941262\n",
            "78 th batch - loss_value: 0.030033277\n",
            "79 th batch - loss_value: 0.049304415\n",
            "# 80 th batch - train_accuracy: 1.0 , test_accuracy: 0.9666666666666667\n",
            "80 th batch - loss_value: 0.019986613\n",
            "81 th batch - loss_value: 0.04012599\n",
            "82 th batch - loss_value: 0.015114535\n",
            "83 th batch - loss_value: 0.035974808\n",
            "84 th batch - loss_value: 0.03716021\n",
            "85 th batch - loss_value: 0.032091018\n",
            "86 th batch - loss_value: 0.008264197\n",
            "87 th batch - loss_value: 0.06070321\n",
            "88 th batch - loss_value: 0.052178893\n",
            "89 th batch - loss_value: 0.10444275\n",
            "# 90 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9733333333333334\n",
            "90 th batch - loss_value: 0.1577288\n",
            "91 th batch - loss_value: 0.018606773\n",
            "92 th batch - loss_value: 0.013779865\n",
            "93 th batch - loss_value: 0.018219104\n",
            "94 th batch - loss_value: 0.1382856\n",
            "95 th batch - loss_value: 0.07666163\n",
            "96 th batch - loss_value: 0.036557015\n",
            "97 th batch - loss_value: 0.06497393\n",
            "98 th batch - loss_value: 0.117348626\n",
            "99 th batch - loss_value: 0.03877474\n",
            "# 100 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9666666666666667\n",
            "100 th batch - loss_value: 0.08357178\n",
            "101 th batch - loss_value: 0.024885999\n",
            "102 th batch - loss_value: 0.013283092\n",
            "103 th batch - loss_value: 0.0033312398\n",
            "104 th batch - loss_value: 0.0456075\n",
            "105 th batch - loss_value: 0.007938232\n",
            "106 th batch - loss_value: 0.02738003\n",
            "107 th batch - loss_value: 0.15734512\n",
            "108 th batch - loss_value: 0.08453765\n",
            "109 th batch - loss_value: 0.09594507\n",
            "# 110 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "110 th batch - loss_value: 0.015139153\n",
            "111 th batch - loss_value: 0.07077556\n",
            "112 th batch - loss_value: 0.012263607\n",
            "113 th batch - loss_value: 0.064886786\n",
            "114 th batch - loss_value: 0.037866972\n",
            "115 th batch - loss_value: 0.016694324\n",
            "116 th batch - loss_value: 0.016846146\n",
            "117 th batch - loss_value: 0.12410501\n",
            "118 th batch - loss_value: 0.071140245\n",
            "119 th batch - loss_value: 0.021555433\n",
            "# 120 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "120 th batch - loss_value: 0.01357199\n",
            "121 th batch - loss_value: 0.018417543\n",
            "122 th batch - loss_value: 0.00911527\n",
            "123 th batch - loss_value: 0.060663648\n",
            "124 th batch - loss_value: 0.085003145\n",
            "125 th batch - loss_value: 0.043741714\n",
            "126 th batch - loss_value: 0.017894758\n",
            "127 th batch - loss_value: 0.031236105\n",
            "128 th batch - loss_value: 0.016022135\n",
            "129 th batch - loss_value: 0.018642316\n",
            "# 130 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9533333333333334\n",
            "130 th batch - loss_value: 0.059888404\n",
            "131 th batch - loss_value: 0.12587456\n",
            "132 th batch - loss_value: 0.032463685\n",
            "133 th batch - loss_value: 0.043447636\n",
            "134 th batch - loss_value: 0.012176681\n",
            "135 th batch - loss_value: 0.03940408\n",
            "136 th batch - loss_value: 0.025315443\n",
            "137 th batch - loss_value: 0.072360314\n",
            "138 th batch - loss_value: 0.022160405\n",
            "139 th batch - loss_value: 0.08144846\n",
            "# 140 th batch - train_accuracy: 0.95 , test_accuracy: 0.94\n",
            "140 th batch - loss_value: 0.12152558\n",
            "141 th batch - loss_value: 0.05360466\n",
            "142 th batch - loss_value: 0.02913907\n",
            "143 th batch - loss_value: 0.006936525\n",
            "144 th batch - loss_value: 0.07595353\n",
            "145 th batch - loss_value: 0.05858829\n",
            "146 th batch - loss_value: 0.0162236\n",
            "147 th batch - loss_value: 0.021061668\n",
            "148 th batch - loss_value: 0.030079849\n",
            "149 th batch - loss_value: 0.04086713\n",
            "# 150 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9466666666666667\n",
            "150 th batch - loss_value: 0.04586618\n",
            "151 th batch - loss_value: 0.0055318335\n",
            "152 th batch - loss_value: 0.017039292\n",
            "153 th batch - loss_value: 0.010934572\n",
            "154 th batch - loss_value: 0.06791141\n",
            "155 th batch - loss_value: 0.0032741432\n",
            "156 th batch - loss_value: 0.012014808\n",
            "157 th batch - loss_value: 0.06360802\n",
            "158 th batch - loss_value: 0.013746024\n",
            "159 th batch - loss_value: 0.033066247\n",
            "# 160 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9266666666666666\n",
            "160 th batch - loss_value: 0.06261962\n",
            "161 th batch - loss_value: 0.028180618\n",
            "162 th batch - loss_value: 0.013286484\n",
            "163 th batch - loss_value: 0.0036027953\n",
            "164 th batch - loss_value: 0.044866156\n",
            "165 th batch - loss_value: 0.017905835\n",
            "166 th batch - loss_value: 0.18216152\n",
            "167 th batch - loss_value: 0.017122786\n",
            "168 th batch - loss_value: 0.04197035\n",
            "169 th batch - loss_value: 0.049017474\n",
            "# 170 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "170 th batch - loss_value: 0.0059772343\n",
            "171 th batch - loss_value: 0.104700476\n",
            "172 th batch - loss_value: 0.01783485\n",
            "173 th batch - loss_value: 0.1366599\n",
            "174 th batch - loss_value: 0.005188459\n",
            "175 th batch - loss_value: 0.011516929\n",
            "176 th batch - loss_value: 0.088414215\n",
            "177 th batch - loss_value: 0.010131849\n",
            "178 th batch - loss_value: 0.06568033\n",
            "179 th batch - loss_value: 0.12764739\n",
            "# 180 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "180 th batch - loss_value: 0.006029942\n",
            "181 th batch - loss_value: 0.027516047\n",
            "182 th batch - loss_value: 0.18380164\n",
            "183 th batch - loss_value: 0.0125318775\n",
            "184 th batch - loss_value: 0.031803224\n",
            "185 th batch - loss_value: 0.042712975\n",
            "186 th batch - loss_value: 0.0040713563\n",
            "187 th batch - loss_value: 0.015792904\n",
            "188 th batch - loss_value: 0.01745441\n",
            "189 th batch - loss_value: 0.049401596\n",
            "# 190 th batch - train_accuracy: 0.95 , test_accuracy: 0.9133333333333333\n",
            "190 th batch - loss_value: 0.06770814\n",
            "191 th batch - loss_value: 0.06364841\n",
            "192 th batch - loss_value: 0.075331084\n",
            "193 th batch - loss_value: 0.025490113\n",
            "194 th batch - loss_value: 0.12816948\n",
            "195 th batch - loss_value: 0.018885959\n",
            "196 th batch - loss_value: 0.073546305\n",
            "197 th batch - loss_value: 0.04241643\n",
            "198 th batch - loss_value: 0.0217715\n",
            "199 th batch - loss_value: 0.057968687\n",
            "# 200 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "200 th batch - loss_value: 0.022909282\n",
            "201 th batch - loss_value: 0.10807367\n",
            "202 th batch - loss_value: 0.068655856\n",
            "203 th batch - loss_value: 0.028476585\n",
            "204 th batch - loss_value: 0.061499853\n",
            "205 th batch - loss_value: 0.026504876\n",
            "206 th batch - loss_value: 0.023584338\n",
            "207 th batch - loss_value: 0.0067509715\n",
            "208 th batch - loss_value: 0.009779277\n",
            "209 th batch - loss_value: 0.124387994\n",
            "# 210 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9733333333333334\n",
            "210 th batch - loss_value: 0.13293016\n",
            "211 th batch - loss_value: 0.09025827\n",
            "212 th batch - loss_value: 0.025954109\n",
            "213 th batch - loss_value: 0.029858904\n",
            "214 th batch - loss_value: 0.006845268\n",
            "215 th batch - loss_value: 0.04933424\n",
            "216 th batch - loss_value: 0.07173965\n",
            "217 th batch - loss_value: 0.07768599\n",
            "218 th batch - loss_value: 0.07940698\n",
            "219 th batch - loss_value: 0.12167053\n",
            "# 220 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.94\n",
            "220 th batch - loss_value: 0.1490212\n",
            "221 th batch - loss_value: 0.08551156\n",
            "222 th batch - loss_value: 0.009889618\n",
            "223 th batch - loss_value: 0.010971746\n",
            "224 th batch - loss_value: 0.055531185\n",
            "225 th batch - loss_value: 0.07747567\n",
            "226 th batch - loss_value: 0.015500089\n",
            "227 th batch - loss_value: 0.066101715\n",
            "228 th batch - loss_value: 0.036002204\n",
            "229 th batch - loss_value: 0.01378698\n",
            "# 230 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "230 th batch - loss_value: 0.09832668\n",
            "231 th batch - loss_value: 0.065216\n",
            "232 th batch - loss_value: 0.026324708\n",
            "233 th batch - loss_value: 0.019510686\n",
            "234 th batch - loss_value: 0.12834264\n",
            "235 th batch - loss_value: 0.009203466\n",
            "236 th batch - loss_value: 0.0016677494\n",
            "237 th batch - loss_value: 0.044278845\n",
            "238 th batch - loss_value: 0.113523774\n",
            "239 th batch - loss_value: 0.03447854\n",
            "# 240 th batch - train_accuracy: 0.95 , test_accuracy: 0.9266666666666666\n",
            "240 th batch - loss_value: 0.100264706\n",
            "241 th batch - loss_value: 0.017833563\n",
            "242 th batch - loss_value: 0.005735436\n",
            "243 th batch - loss_value: 0.033865668\n",
            "244 th batch - loss_value: 0.026420975\n",
            "245 th batch - loss_value: 0.1826053\n",
            "246 th batch - loss_value: 0.048852734\n",
            "247 th batch - loss_value: 0.09976172\n",
            "248 th batch - loss_value: 0.081234075\n",
            "249 th batch - loss_value: 0.05906299\n",
            "# 250 th batch - train_accuracy: 0.95 , test_accuracy: 0.92\n",
            "250 th batch - loss_value: 0.073784105\n",
            "251 th batch - loss_value: 0.25064096\n",
            "252 th batch - loss_value: 0.1324508\n",
            "253 th batch - loss_value: 0.07072102\n",
            "254 th batch - loss_value: 0.082865566\n",
            "255 th batch - loss_value: 0.08571257\n",
            "256 th batch - loss_value: 0.08539935\n",
            "257 th batch - loss_value: 0.026345996\n",
            "258 th batch - loss_value: 0.056882165\n",
            "259 th batch - loss_value: 0.118159175\n",
            "# 260 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9733333333333334\n",
            "260 th batch - loss_value: 0.050750125\n",
            "261 th batch - loss_value: 0.20636486\n",
            "262 th batch - loss_value: 0.14743423\n",
            "263 th batch - loss_value: 0.16216548\n",
            "264 th batch - loss_value: 0.0388849\n",
            "265 th batch - loss_value: 0.1494592\n",
            "266 th batch - loss_value: 0.026400803\n",
            "267 th batch - loss_value: 0.16975221\n",
            "268 th batch - loss_value: 0.12074752\n",
            "269 th batch - loss_value: 0.21636347\n",
            "# 270 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "270 th batch - loss_value: 0.046676517\n",
            "271 th batch - loss_value: 0.091137215\n",
            "272 th batch - loss_value: 0.037308436\n",
            "273 th batch - loss_value: 0.038744073\n",
            "274 th batch - loss_value: 0.020694433\n",
            "275 th batch - loss_value: 0.042464294\n",
            "276 th batch - loss_value: 0.14639285\n",
            "277 th batch - loss_value: 0.09889493\n",
            "278 th batch - loss_value: 0.14059776\n",
            "279 th batch - loss_value: 0.035954103\n",
            "# 280 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "280 th batch - loss_value: 0.027181337\n",
            "281 th batch - loss_value: 0.07505382\n",
            "282 th batch - loss_value: 0.06757573\n",
            "283 th batch - loss_value: 0.04257854\n",
            "284 th batch - loss_value: 0.04619996\n",
            "285 th batch - loss_value: 0.044386514\n",
            "286 th batch - loss_value: 0.11616947\n",
            "287 th batch - loss_value: 0.09111221\n",
            "288 th batch - loss_value: 0.05398356\n",
            "289 th batch - loss_value: 0.027111938\n",
            "# 290 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "290 th batch - loss_value: 0.024379581\n",
            "291 th batch - loss_value: 0.091296464\n",
            "292 th batch - loss_value: 0.049624972\n",
            "293 th batch - loss_value: 0.17768258\n",
            "294 th batch - loss_value: 0.019616146\n",
            "295 th batch - loss_value: 0.1008761\n",
            "296 th batch - loss_value: 0.0313674\n",
            "297 th batch - loss_value: 0.08759494\n",
            "298 th batch - loss_value: 0.053032923\n",
            "299 th batch - loss_value: 0.15093084\n",
            "# 300 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "300 th batch - loss_value: 0.0074119586\n",
            "301 th batch - loss_value: 0.079404645\n",
            "302 th batch - loss_value: 0.07969072\n",
            "303 th batch - loss_value: 0.066796556\n",
            "304 th batch - loss_value: 0.044207014\n",
            "305 th batch - loss_value: 0.058665693\n",
            "306 th batch - loss_value: 0.09290076\n",
            "307 th batch - loss_value: 0.22395913\n",
            "308 th batch - loss_value: 0.021852098\n",
            "309 th batch - loss_value: 0.072801\n",
            "# 310 th batch - train_accuracy: 0.95 , test_accuracy: 0.9266666666666666\n",
            "310 th batch - loss_value: 0.14074728\n",
            "311 th batch - loss_value: 0.04758506\n",
            "312 th batch - loss_value: 0.08007919\n",
            "313 th batch - loss_value: 0.038775932\n",
            "314 th batch - loss_value: 0.12489451\n",
            "315 th batch - loss_value: 0.15095669\n",
            "316 th batch - loss_value: 0.050579127\n",
            "317 th batch - loss_value: 0.028081654\n",
            "318 th batch - loss_value: 0.020118274\n",
            "319 th batch - loss_value: 0.038285576\n",
            "# 320 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9333333333333333\n",
            "320 th batch - loss_value: 0.08343991\n",
            "321 th batch - loss_value: 0.06564699\n",
            "322 th batch - loss_value: 0.09971792\n",
            "323 th batch - loss_value: 0.07009969\n",
            "324 th batch - loss_value: 0.15038571\n",
            "325 th batch - loss_value: 0.12327466\n",
            "326 th batch - loss_value: 0.025534349\n",
            "327 th batch - loss_value: 0.11396725\n",
            "328 th batch - loss_value: 0.09700684\n",
            "329 th batch - loss_value: 0.041651607\n",
            "# 330 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8733333333333333\n",
            "330 th batch - loss_value: 0.042708397\n",
            "331 th batch - loss_value: 0.05126481\n",
            "332 th batch - loss_value: 0.10116938\n",
            "333 th batch - loss_value: 0.013991906\n",
            "334 th batch - loss_value: 0.05722654\n",
            "335 th batch - loss_value: 0.17597243\n",
            "336 th batch - loss_value: 0.05421118\n",
            "337 th batch - loss_value: 0.18922551\n",
            "338 th batch - loss_value: 0.012519793\n",
            "339 th batch - loss_value: 0.016842674\n",
            "# 340 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "340 th batch - loss_value: 0.03901491\n",
            "341 th batch - loss_value: 0.052827932\n",
            "342 th batch - loss_value: 0.057182003\n",
            "343 th batch - loss_value: 0.026395826\n",
            "344 th batch - loss_value: 0.09918217\n",
            "345 th batch - loss_value: 0.068095095\n",
            "346 th batch - loss_value: 0.015807992\n",
            "347 th batch - loss_value: 0.053279717\n",
            "348 th batch - loss_value: 0.031170286\n",
            "349 th batch - loss_value: 0.046584018\n",
            "# 350 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9733333333333334\n",
            "350 th batch - loss_value: 0.06026607\n",
            "351 th batch - loss_value: 0.042717874\n",
            "352 th batch - loss_value: 0.058717202\n",
            "353 th batch - loss_value: 0.03480709\n",
            "354 th batch - loss_value: 0.11869767\n",
            "355 th batch - loss_value: 0.028021658\n",
            "356 th batch - loss_value: 0.007064783\n",
            "357 th batch - loss_value: 0.040472843\n",
            "358 th batch - loss_value: 0.0055064172\n",
            "359 th batch - loss_value: 0.029656267\n",
            "# 360 th batch - train_accuracy: 0.95 , test_accuracy: 0.94\n",
            "360 th batch - loss_value: 0.08172589\n",
            "361 th batch - loss_value: 0.032283824\n",
            "362 th batch - loss_value: 0.06351043\n",
            "363 th batch - loss_value: 0.0694217\n",
            "364 th batch - loss_value: 0.027641518\n",
            "365 th batch - loss_value: 0.026741166\n",
            "366 th batch - loss_value: 0.05877916\n",
            "367 th batch - loss_value: 0.0040746694\n",
            "368 th batch - loss_value: 0.040872753\n",
            "369 th batch - loss_value: 0.13606064\n",
            "# 370 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "370 th batch - loss_value: 0.0075996947\n",
            "371 th batch - loss_value: 0.0052342983\n",
            "372 th batch - loss_value: 0.09832033\n",
            "373 th batch - loss_value: 0.04483389\n",
            "374 th batch - loss_value: 0.03155556\n",
            "375 th batch - loss_value: 0.011030067\n",
            "376 th batch - loss_value: 0.12958953\n",
            "377 th batch - loss_value: 0.09801231\n",
            "378 th batch - loss_value: 0.18372995\n",
            "379 th batch - loss_value: 0.14732543\n",
            "# 380 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.96\n",
            "380 th batch - loss_value: 0.093168244\n",
            "381 th batch - loss_value: 0.065325156\n",
            "382 th batch - loss_value: 0.113202125\n",
            "383 th batch - loss_value: 0.111116625\n",
            "384 th batch - loss_value: 0.050552342\n",
            "385 th batch - loss_value: 0.01802331\n",
            "386 th batch - loss_value: 0.18993889\n",
            "387 th batch - loss_value: 0.10488427\n",
            "388 th batch - loss_value: 0.14269613\n",
            "389 th batch - loss_value: 0.07932804\n",
            "# 390 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "390 th batch - loss_value: 0.15117079\n",
            "391 th batch - loss_value: 0.08961404\n",
            "392 th batch - loss_value: 0.0236963\n",
            "393 th batch - loss_value: 0.006018666\n",
            "394 th batch - loss_value: 0.025259739\n",
            "395 th batch - loss_value: 0.1018205\n",
            "396 th batch - loss_value: 0.14563632\n",
            "397 th batch - loss_value: 0.020286182\n",
            "398 th batch - loss_value: 0.016114254\n",
            "399 th batch - loss_value: 0.0062512537\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "0 th batch - loss_value: 0.0074306293\n",
            "1 th batch - loss_value: 0.020429727\n",
            "2 th batch - loss_value: 0.028384458\n",
            "3 th batch - loss_value: 0.05493276\n",
            "4 th batch - loss_value: 0.13640496\n",
            "5 th batch - loss_value: 0.045409314\n",
            "6 th batch - loss_value: 0.1897475\n",
            "7 th batch - loss_value: 0.040974334\n",
            "8 th batch - loss_value: 0.04402929\n",
            "9 th batch - loss_value: 0.10379175\n",
            "# 10 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "10 th batch - loss_value: 0.04172353\n",
            "11 th batch - loss_value: 0.032551102\n",
            "12 th batch - loss_value: 0.0137128355\n",
            "13 th batch - loss_value: 0.01895469\n",
            "14 th batch - loss_value: 0.11266694\n",
            "15 th batch - loss_value: 0.05304814\n",
            "16 th batch - loss_value: 0.022030614\n",
            "17 th batch - loss_value: 0.0374123\n",
            "18 th batch - loss_value: 0.15727372\n",
            "19 th batch - loss_value: 0.02152825\n",
            "# 20 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "20 th batch - loss_value: 0.02985538\n",
            "21 th batch - loss_value: 0.016016638\n",
            "22 th batch - loss_value: 0.036355432\n",
            "23 th batch - loss_value: 0.02768327\n",
            "24 th batch - loss_value: 0.036835305\n",
            "25 th batch - loss_value: 0.013262839\n",
            "26 th batch - loss_value: 0.049835067\n",
            "27 th batch - loss_value: 0.009205604\n",
            "28 th batch - loss_value: 0.13053392\n",
            "29 th batch - loss_value: 0.007511319\n",
            "# 30 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9666666666666667\n",
            "30 th batch - loss_value: 0.04913063\n",
            "31 th batch - loss_value: 0.011301066\n",
            "32 th batch - loss_value: 0.27255532\n",
            "33 th batch - loss_value: 0.08802507\n",
            "34 th batch - loss_value: 0.08778107\n",
            "35 th batch - loss_value: 0.055561718\n",
            "36 th batch - loss_value: 0.15497866\n",
            "37 th batch - loss_value: 0.09674148\n",
            "38 th batch - loss_value: 0.09896881\n",
            "39 th batch - loss_value: 0.26406422\n",
            "# 40 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "40 th batch - loss_value: 0.0052962867\n",
            "41 th batch - loss_value: 0.16345538\n",
            "42 th batch - loss_value: 0.064050086\n",
            "43 th batch - loss_value: 0.01936494\n",
            "44 th batch - loss_value: 0.13238616\n",
            "45 th batch - loss_value: 0.012997563\n",
            "46 th batch - loss_value: 0.09568462\n",
            "47 th batch - loss_value: 0.024332864\n",
            "48 th batch - loss_value: 0.02302389\n",
            "49 th batch - loss_value: 0.06755767\n",
            "# 50 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "50 th batch - loss_value: 0.044206075\n",
            "51 th batch - loss_value: 0.16428702\n",
            "52 th batch - loss_value: 0.040781874\n",
            "53 th batch - loss_value: 0.052459095\n",
            "54 th batch - loss_value: 0.041396737\n",
            "55 th batch - loss_value: 0.14236654\n",
            "56 th batch - loss_value: 0.12973006\n",
            "57 th batch - loss_value: 0.038318917\n",
            "58 th batch - loss_value: 0.03547248\n",
            "59 th batch - loss_value: 0.06097015\n",
            "# 60 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.94\n",
            "60 th batch - loss_value: 0.08454117\n",
            "61 th batch - loss_value: 0.046946507\n",
            "62 th batch - loss_value: 0.0671573\n",
            "63 th batch - loss_value: 0.061440315\n",
            "64 th batch - loss_value: 0.11050818\n",
            "65 th batch - loss_value: 0.05271077\n",
            "66 th batch - loss_value: 0.013774559\n",
            "67 th batch - loss_value: 0.023873718\n",
            "68 th batch - loss_value: 0.06904409\n",
            "69 th batch - loss_value: 0.019807544\n",
            "# 70 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "70 th batch - loss_value: 0.036074616\n",
            "71 th batch - loss_value: 0.14303207\n",
            "72 th batch - loss_value: 0.02073701\n",
            "73 th batch - loss_value: 0.109885834\n",
            "74 th batch - loss_value: 0.107911274\n",
            "75 th batch - loss_value: 0.048558883\n",
            "76 th batch - loss_value: 0.020718282\n",
            "77 th batch - loss_value: 0.017732594\n",
            "78 th batch - loss_value: 0.081250906\n",
            "79 th batch - loss_value: 0.04955763\n",
            "# 80 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "80 th batch - loss_value: 0.036357734\n",
            "81 th batch - loss_value: 0.030365514\n",
            "82 th batch - loss_value: 0.062100377\n",
            "83 th batch - loss_value: 0.08882378\n",
            "84 th batch - loss_value: 0.04161689\n",
            "85 th batch - loss_value: 0.024534335\n",
            "86 th batch - loss_value: 0.056376718\n",
            "87 th batch - loss_value: 0.014786424\n",
            "88 th batch - loss_value: 0.017967826\n",
            "89 th batch - loss_value: 0.110353604\n",
            "# 90 th batch - train_accuracy: 0.95 , test_accuracy: 0.9866666666666667\n",
            "90 th batch - loss_value: 0.19677798\n",
            "91 th batch - loss_value: 0.012990714\n",
            "92 th batch - loss_value: 0.008794321\n",
            "93 th batch - loss_value: 0.034347713\n",
            "94 th batch - loss_value: 0.061325975\n",
            "95 th batch - loss_value: 0.0010066301\n",
            "96 th batch - loss_value: 0.0030859245\n",
            "97 th batch - loss_value: 0.049611703\n",
            "98 th batch - loss_value: 0.08776775\n",
            "99 th batch - loss_value: 0.11277584\n",
            "# 100 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "100 th batch - loss_value: 0.046706814\n",
            "101 th batch - loss_value: 0.027545365\n",
            "102 th batch - loss_value: 0.0069667627\n",
            "103 th batch - loss_value: 0.0080573065\n",
            "104 th batch - loss_value: 0.062678464\n",
            "105 th batch - loss_value: 0.016592106\n",
            "106 th batch - loss_value: 0.08103477\n",
            "107 th batch - loss_value: 0.045991573\n",
            "108 th batch - loss_value: 0.071228154\n",
            "109 th batch - loss_value: 0.15989445\n",
            "# 110 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9666666666666667\n",
            "110 th batch - loss_value: 0.048203982\n",
            "111 th batch - loss_value: 0.16125628\n",
            "112 th batch - loss_value: 0.08801672\n",
            "113 th batch - loss_value: 0.01456972\n",
            "114 th batch - loss_value: 0.0940375\n",
            "115 th batch - loss_value: 0.063608125\n",
            "116 th batch - loss_value: 0.061520394\n",
            "117 th batch - loss_value: 0.07329475\n",
            "118 th batch - loss_value: 0.06653115\n",
            "119 th batch - loss_value: 0.053908814\n",
            "# 120 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9333333333333333\n",
            "120 th batch - loss_value: 0.117557086\n",
            "121 th batch - loss_value: 0.044327985\n",
            "122 th batch - loss_value: 0.01935319\n",
            "123 th batch - loss_value: 0.0581672\n",
            "124 th batch - loss_value: 0.056661777\n",
            "125 th batch - loss_value: 0.03400975\n",
            "126 th batch - loss_value: 0.10528741\n",
            "127 th batch - loss_value: 0.081456184\n",
            "128 th batch - loss_value: 0.022315359\n",
            "129 th batch - loss_value: 0.07430016\n",
            "# 130 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "130 th batch - loss_value: 0.013007855\n",
            "131 th batch - loss_value: 0.10559233\n",
            "132 th batch - loss_value: 0.018859265\n",
            "133 th batch - loss_value: 0.13706492\n",
            "134 th batch - loss_value: 0.00918517\n",
            "135 th batch - loss_value: 0.090165846\n",
            "136 th batch - loss_value: 0.014719063\n",
            "137 th batch - loss_value: 0.015409753\n",
            "138 th batch - loss_value: 0.01738027\n",
            "139 th batch - loss_value: 0.0018566564\n",
            "# 140 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "140 th batch - loss_value: 0.020040046\n",
            "141 th batch - loss_value: 0.0131738745\n",
            "142 th batch - loss_value: 0.044269565\n",
            "143 th batch - loss_value: 0.036529873\n",
            "144 th batch - loss_value: 0.015036549\n",
            "145 th batch - loss_value: 0.059162393\n",
            "146 th batch - loss_value: 0.0109079825\n",
            "147 th batch - loss_value: 0.025585374\n",
            "148 th batch - loss_value: 0.07458248\n",
            "149 th batch - loss_value: 0.09065353\n",
            "# 150 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9533333333333334\n",
            "150 th batch - loss_value: 0.030665282\n",
            "151 th batch - loss_value: 0.059613295\n",
            "152 th batch - loss_value: 0.008635121\n",
            "153 th batch - loss_value: 0.021085182\n",
            "154 th batch - loss_value: 0.003404659\n",
            "155 th batch - loss_value: 0.0303052\n",
            "156 th batch - loss_value: 0.11454437\n",
            "157 th batch - loss_value: 0.043991726\n",
            "158 th batch - loss_value: 0.04053922\n",
            "159 th batch - loss_value: 0.041281562\n",
            "# 160 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "160 th batch - loss_value: 0.005590596\n",
            "161 th batch - loss_value: 0.0071223257\n",
            "162 th batch - loss_value: 0.012328556\n",
            "163 th batch - loss_value: 0.01705162\n",
            "164 th batch - loss_value: 0.02987003\n",
            "165 th batch - loss_value: 0.017158544\n",
            "166 th batch - loss_value: 0.05274137\n",
            "167 th batch - loss_value: 0.02168173\n",
            "168 th batch - loss_value: 0.05325737\n",
            "169 th batch - loss_value: 0.019529257\n",
            "# 170 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.94\n",
            "170 th batch - loss_value: 0.07622011\n",
            "171 th batch - loss_value: 0.04784269\n",
            "172 th batch - loss_value: 0.060511615\n",
            "173 th batch - loss_value: 0.019827453\n",
            "174 th batch - loss_value: 0.10813193\n",
            "175 th batch - loss_value: 0.07545885\n",
            "176 th batch - loss_value: 0.04265869\n",
            "177 th batch - loss_value: 0.023204437\n",
            "178 th batch - loss_value: 0.06524401\n",
            "179 th batch - loss_value: 0.016279802\n",
            "# 180 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "180 th batch - loss_value: 0.01043453\n",
            "181 th batch - loss_value: 0.058354717\n",
            "182 th batch - loss_value: 0.042247567\n",
            "183 th batch - loss_value: 0.05085607\n",
            "184 th batch - loss_value: 0.057639394\n",
            "185 th batch - loss_value: 0.032615047\n",
            "186 th batch - loss_value: 0.018538004\n",
            "187 th batch - loss_value: 0.04489794\n",
            "188 th batch - loss_value: 0.13626896\n",
            "189 th batch - loss_value: 0.014828298\n",
            "# 190 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "190 th batch - loss_value: 0.03378872\n",
            "191 th batch - loss_value: 0.01074695\n",
            "192 th batch - loss_value: 0.03752986\n",
            "193 th batch - loss_value: 0.011341263\n",
            "194 th batch - loss_value: 0.036620457\n",
            "195 th batch - loss_value: 0.013499411\n",
            "196 th batch - loss_value: 0.3033185\n",
            "197 th batch - loss_value: 0.048360683\n",
            "198 th batch - loss_value: 0.067020416\n",
            "199 th batch - loss_value: 0.112966046\n",
            "# 200 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9066666666666666\n",
            "200 th batch - loss_value: 0.112512864\n",
            "201 th batch - loss_value: 0.04517605\n",
            "202 th batch - loss_value: 0.029605549\n",
            "203 th batch - loss_value: 0.04357797\n",
            "204 th batch - loss_value: 0.035598323\n",
            "205 th batch - loss_value: 0.005106692\n",
            "206 th batch - loss_value: 0.020298839\n",
            "207 th batch - loss_value: 0.10504501\n",
            "208 th batch - loss_value: 0.012773883\n",
            "209 th batch - loss_value: 0.11218173\n",
            "# 210 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.96\n",
            "210 th batch - loss_value: 0.095003694\n",
            "211 th batch - loss_value: 0.00932117\n",
            "212 th batch - loss_value: 0.031167181\n",
            "213 th batch - loss_value: 0.045605015\n",
            "214 th batch - loss_value: 0.01266395\n",
            "215 th batch - loss_value: 0.023596687\n",
            "216 th batch - loss_value: 0.04510177\n",
            "217 th batch - loss_value: 0.0240701\n",
            "218 th batch - loss_value: 0.08942845\n",
            "219 th batch - loss_value: 0.060004313\n",
            "# 220 th batch - train_accuracy: 1.0 , test_accuracy: 0.88\n",
            "220 th batch - loss_value: 0.01823199\n",
            "221 th batch - loss_value: 0.021368323\n",
            "222 th batch - loss_value: 0.0066806646\n",
            "223 th batch - loss_value: 0.124775186\n",
            "224 th batch - loss_value: 0.11055156\n",
            "225 th batch - loss_value: 0.0032846264\n",
            "226 th batch - loss_value: 0.002690725\n",
            "227 th batch - loss_value: 0.015593653\n",
            "228 th batch - loss_value: 0.053038813\n",
            "229 th batch - loss_value: 0.023333738\n",
            "# 230 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "230 th batch - loss_value: 0.0061867456\n",
            "231 th batch - loss_value: 0.07311606\n",
            "232 th batch - loss_value: 0.004582069\n",
            "233 th batch - loss_value: 0.045249537\n",
            "234 th batch - loss_value: 0.091284655\n",
            "235 th batch - loss_value: 0.021765806\n",
            "236 th batch - loss_value: 0.050998162\n",
            "237 th batch - loss_value: 0.1209284\n",
            "238 th batch - loss_value: 0.060363937\n",
            "239 th batch - loss_value: 0.022305673\n",
            "# 240 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.9266666666666666\n",
            "240 th batch - loss_value: 0.13482901\n",
            "241 th batch - loss_value: 0.024927644\n",
            "242 th batch - loss_value: 0.031257894\n",
            "243 th batch - loss_value: 0.022270687\n",
            "244 th batch - loss_value: 0.01897788\n",
            "245 th batch - loss_value: 0.10512338\n",
            "246 th batch - loss_value: 0.09456377\n",
            "247 th batch - loss_value: 0.017439682\n",
            "248 th batch - loss_value: 0.044355314\n",
            "249 th batch - loss_value: 0.026423847\n",
            "# 250 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9266666666666666\n",
            "250 th batch - loss_value: 0.15962791\n",
            "251 th batch - loss_value: 0.11301982\n",
            "252 th batch - loss_value: 0.0061178943\n",
            "253 th batch - loss_value: 0.008591541\n",
            "254 th batch - loss_value: 0.016451525\n",
            "255 th batch - loss_value: 0.15858647\n",
            "256 th batch - loss_value: 0.022839082\n",
            "257 th batch - loss_value: 0.023644798\n",
            "258 th batch - loss_value: 0.0036700587\n",
            "259 th batch - loss_value: 0.12041414\n",
            "# 260 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "260 th batch - loss_value: 0.008100654\n",
            "261 th batch - loss_value: 0.0045701917\n",
            "262 th batch - loss_value: 0.019301975\n",
            "263 th batch - loss_value: 0.10085938\n",
            "264 th batch - loss_value: 0.17248563\n",
            "265 th batch - loss_value: 0.027970137\n",
            "266 th batch - loss_value: 0.054883968\n",
            "267 th batch - loss_value: 0.0070921383\n",
            "268 th batch - loss_value: 0.018034898\n",
            "269 th batch - loss_value: 0.1673851\n",
            "# 270 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "270 th batch - loss_value: 0.02449825\n",
            "271 th batch - loss_value: 0.06331488\n",
            "272 th batch - loss_value: 0.013366968\n",
            "273 th batch - loss_value: 0.025774585\n",
            "274 th batch - loss_value: 0.087600775\n",
            "275 th batch - loss_value: 0.04027883\n",
            "276 th batch - loss_value: 0.08331098\n",
            "277 th batch - loss_value: 0.05434557\n",
            "278 th batch - loss_value: 0.16931877\n",
            "279 th batch - loss_value: 0.033007156\n",
            "# 280 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "280 th batch - loss_value: 0.008727741\n",
            "281 th batch - loss_value: 0.048964273\n",
            "282 th batch - loss_value: 0.1514184\n",
            "283 th batch - loss_value: 0.011297858\n",
            "284 th batch - loss_value: 0.025152514\n",
            "285 th batch - loss_value: 0.10071401\n",
            "286 th batch - loss_value: 0.28970018\n",
            "287 th batch - loss_value: 0.13429536\n",
            "288 th batch - loss_value: 0.061939426\n",
            "289 th batch - loss_value: 0.009885076\n",
            "# 290 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9466666666666667\n",
            "290 th batch - loss_value: 0.06904554\n",
            "291 th batch - loss_value: 0.038650207\n",
            "292 th batch - loss_value: 0.13651755\n",
            "293 th batch - loss_value: 0.033537123\n",
            "294 th batch - loss_value: 0.16704097\n",
            "295 th batch - loss_value: 0.013579592\n",
            "296 th batch - loss_value: 0.05951045\n",
            "297 th batch - loss_value: 0.036658168\n",
            "298 th batch - loss_value: 0.009963784\n",
            "299 th batch - loss_value: 0.054817844\n",
            "# 300 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9666666666666667\n",
            "300 th batch - loss_value: 0.105892174\n",
            "301 th batch - loss_value: 0.026580567\n",
            "302 th batch - loss_value: 0.15725441\n",
            "303 th batch - loss_value: 0.12093984\n",
            "304 th batch - loss_value: 0.03606087\n",
            "305 th batch - loss_value: 0.21940725\n",
            "306 th batch - loss_value: 0.048484087\n",
            "307 th batch - loss_value: 0.017367817\n",
            "308 th batch - loss_value: 0.05286628\n",
            "309 th batch - loss_value: 0.04151807\n",
            "# 310 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "310 th batch - loss_value: 0.047527205\n",
            "311 th batch - loss_value: 0.106911466\n",
            "312 th batch - loss_value: 0.08660658\n",
            "313 th batch - loss_value: 0.09067868\n",
            "314 th batch - loss_value: 0.07064867\n",
            "315 th batch - loss_value: 0.062154725\n",
            "316 th batch - loss_value: 0.049752336\n",
            "317 th batch - loss_value: 0.017474417\n",
            "318 th batch - loss_value: 0.13348731\n",
            "319 th batch - loss_value: 0.024984883\n",
            "# 320 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8933333333333333\n",
            "320 th batch - loss_value: 0.139018\n",
            "321 th batch - loss_value: 0.10958554\n",
            "322 th batch - loss_value: 0.07583664\n",
            "323 th batch - loss_value: 0.05477316\n",
            "324 th batch - loss_value: 0.08857547\n",
            "325 th batch - loss_value: 0.10767948\n",
            "326 th batch - loss_value: 0.012401665\n",
            "327 th batch - loss_value: 0.03992834\n",
            "328 th batch - loss_value: 0.015693495\n",
            "329 th batch - loss_value: 0.038564198\n",
            "# 330 th batch - train_accuracy: 1.0 , test_accuracy: 0.8533333333333334\n",
            "330 th batch - loss_value: 0.017732775\n",
            "331 th batch - loss_value: 0.045873415\n",
            "332 th batch - loss_value: 0.061577413\n",
            "333 th batch - loss_value: 0.03228882\n",
            "334 th batch - loss_value: 0.15994687\n",
            "335 th batch - loss_value: 0.020100141\n",
            "336 th batch - loss_value: 0.007473572\n",
            "337 th batch - loss_value: 0.15347841\n",
            "338 th batch - loss_value: 0.057114005\n",
            "339 th batch - loss_value: 0.00574216\n",
            "# 340 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "340 th batch - loss_value: 0.04912924\n",
            "341 th batch - loss_value: 0.06281494\n",
            "342 th batch - loss_value: 0.0069679003\n",
            "343 th batch - loss_value: 0.018638741\n",
            "344 th batch - loss_value: 0.023616314\n",
            "345 th batch - loss_value: 0.0832806\n",
            "346 th batch - loss_value: 0.007637371\n",
            "347 th batch - loss_value: 0.025004728\n",
            "348 th batch - loss_value: 0.057465825\n",
            "349 th batch - loss_value: 0.030155368\n",
            "# 350 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "350 th batch - loss_value: 0.028185237\n",
            "351 th batch - loss_value: 0.08356903\n",
            "352 th batch - loss_value: 0.15836091\n",
            "353 th batch - loss_value: 0.0038657445\n",
            "354 th batch - loss_value: 0.120891914\n",
            "355 th batch - loss_value: 0.09328913\n",
            "356 th batch - loss_value: 0.07454396\n",
            "357 th batch - loss_value: 0.077952765\n",
            "358 th batch - loss_value: 0.013419342\n",
            "359 th batch - loss_value: 0.16223091\n",
            "# 360 th batch - train_accuracy: 0.95 , test_accuracy: 0.9466666666666667\n",
            "360 th batch - loss_value: 0.2876822\n",
            "361 th batch - loss_value: 0.020875208\n",
            "362 th batch - loss_value: 0.003330144\n",
            "363 th batch - loss_value: 0.1572358\n",
            "364 th batch - loss_value: 0.006235424\n",
            "365 th batch - loss_value: 0.17596807\n",
            "366 th batch - loss_value: 0.066198505\n",
            "367 th batch - loss_value: 0.034998775\n",
            "368 th batch - loss_value: 0.020104548\n",
            "369 th batch - loss_value: 0.00273285\n",
            "# 370 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "370 th batch - loss_value: 0.00937969\n",
            "371 th batch - loss_value: 0.014392073\n",
            "372 th batch - loss_value: 0.09134733\n",
            "373 th batch - loss_value: 0.044966485\n",
            "374 th batch - loss_value: 0.011995435\n",
            "375 th batch - loss_value: 0.0350575\n",
            "376 th batch - loss_value: 0.013309391\n",
            "377 th batch - loss_value: 0.0550139\n",
            "378 th batch - loss_value: 0.024383426\n",
            "379 th batch - loss_value: 0.027423525\n",
            "# 380 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "380 th batch - loss_value: 0.0029611103\n",
            "381 th batch - loss_value: 0.0021119867\n",
            "382 th batch - loss_value: 0.023177512\n",
            "383 th batch - loss_value: 0.06593508\n",
            "384 th batch - loss_value: 0.08870344\n",
            "385 th batch - loss_value: 0.0157793\n",
            "386 th batch - loss_value: 0.15194209\n",
            "387 th batch - loss_value: 0.029345643\n",
            "388 th batch - loss_value: 0.12445147\n",
            "389 th batch - loss_value: 0.009002019\n",
            "# 390 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "390 th batch - loss_value: 0.052233756\n",
            "391 th batch - loss_value: 0.14838009\n",
            "392 th batch - loss_value: 0.20749258\n",
            "393 th batch - loss_value: 0.040450953\n",
            "394 th batch - loss_value: 0.01093919\n",
            "395 th batch - loss_value: 0.041008797\n",
            "396 th batch - loss_value: 0.0537889\n",
            "397 th batch - loss_value: 0.02333824\n",
            "398 th batch - loss_value: 0.08354774\n",
            "399 th batch - loss_value: 0.040201988\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "0 th batch - loss_value: 0.081310645\n",
            "1 th batch - loss_value: 0.04345931\n",
            "2 th batch - loss_value: 0.028755508\n",
            "3 th batch - loss_value: 0.010477869\n",
            "4 th batch - loss_value: 0.01423669\n",
            "5 th batch - loss_value: 0.011389016\n",
            "6 th batch - loss_value: 0.09333154\n",
            "7 th batch - loss_value: 0.018610973\n",
            "8 th batch - loss_value: 0.014079862\n",
            "9 th batch - loss_value: 0.057225704\n",
            "# 10 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "10 th batch - loss_value: 0.068143204\n",
            "11 th batch - loss_value: 0.053024966\n",
            "12 th batch - loss_value: 0.048358265\n",
            "13 th batch - loss_value: 0.0240708\n",
            "14 th batch - loss_value: 0.0066008745\n",
            "15 th batch - loss_value: 0.028972382\n",
            "16 th batch - loss_value: 0.050892804\n",
            "17 th batch - loss_value: 0.020035211\n",
            "18 th batch - loss_value: 0.036420666\n",
            "19 th batch - loss_value: 0.06022102\n",
            "# 20 th batch - train_accuracy: 1.0 , test_accuracy: 0.98\n",
            "20 th batch - loss_value: 0.003898569\n",
            "21 th batch - loss_value: 0.031662602\n",
            "22 th batch - loss_value: 0.015010802\n",
            "23 th batch - loss_value: 0.04432749\n",
            "24 th batch - loss_value: 0.062108573\n",
            "25 th batch - loss_value: 0.015231728\n",
            "26 th batch - loss_value: 0.0940587\n",
            "27 th batch - loss_value: 0.026059555\n",
            "28 th batch - loss_value: 0.009846907\n",
            "29 th batch - loss_value: 0.016525764\n",
            "# 30 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9666666666666667\n",
            "30 th batch - loss_value: 0.09337017\n",
            "31 th batch - loss_value: 0.020935347\n",
            "32 th batch - loss_value: 0.26936576\n",
            "33 th batch - loss_value: 0.039698865\n",
            "34 th batch - loss_value: 0.025697757\n",
            "35 th batch - loss_value: 0.23263723\n",
            "36 th batch - loss_value: 0.072695255\n",
            "37 th batch - loss_value: 0.025326988\n",
            "38 th batch - loss_value: 0.1366097\n",
            "39 th batch - loss_value: 0.07332875\n",
            "# 40 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "40 th batch - loss_value: 0.035094094\n",
            "41 th batch - loss_value: 0.024111973\n",
            "42 th batch - loss_value: 0.007884229\n",
            "43 th batch - loss_value: 0.012691074\n",
            "44 th batch - loss_value: 0.020127675\n",
            "45 th batch - loss_value: 0.039027013\n",
            "46 th batch - loss_value: 0.053581323\n",
            "47 th batch - loss_value: 0.027152115\n",
            "48 th batch - loss_value: 0.0126816435\n",
            "49 th batch - loss_value: 0.012042174\n",
            "# 50 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9266666666666666\n",
            "50 th batch - loss_value: 0.05079103\n",
            "51 th batch - loss_value: 0.04111537\n",
            "52 th batch - loss_value: 0.08927365\n",
            "53 th batch - loss_value: 0.0025119474\n",
            "54 th batch - loss_value: 0.031644195\n",
            "55 th batch - loss_value: 0.059290603\n",
            "56 th batch - loss_value: 0.0098927\n",
            "57 th batch - loss_value: 0.009773151\n",
            "58 th batch - loss_value: 0.045282483\n",
            "59 th batch - loss_value: 0.051071387\n",
            "# 60 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9333333333333333\n",
            "60 th batch - loss_value: 0.051952068\n",
            "61 th batch - loss_value: 0.07896903\n",
            "62 th batch - loss_value: 0.06406478\n",
            "63 th batch - loss_value: 0.06584147\n",
            "64 th batch - loss_value: 0.018632943\n",
            "65 th batch - loss_value: 0.0041107964\n",
            "66 th batch - loss_value: 0.0018212715\n",
            "67 th batch - loss_value: 0.0045583197\n",
            "68 th batch - loss_value: 0.026137572\n",
            "69 th batch - loss_value: 0.023483546\n",
            "# 70 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "70 th batch - loss_value: 0.006895063\n",
            "71 th batch - loss_value: 0.04600519\n",
            "72 th batch - loss_value: 0.10254102\n",
            "73 th batch - loss_value: 0.009537541\n",
            "74 th batch - loss_value: 0.09705986\n",
            "75 th batch - loss_value: 0.042839672\n",
            "76 th batch - loss_value: 0.04202656\n",
            "77 th batch - loss_value: 0.031411223\n",
            "78 th batch - loss_value: 0.064671904\n",
            "79 th batch - loss_value: 0.051682696\n",
            "# 80 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "80 th batch - loss_value: 0.0058718375\n",
            "81 th batch - loss_value: 0.010251034\n",
            "82 th batch - loss_value: 0.009618447\n",
            "83 th batch - loss_value: 0.028727444\n",
            "84 th batch - loss_value: 0.011842237\n",
            "85 th batch - loss_value: 0.050275512\n",
            "86 th batch - loss_value: 0.0030458164\n",
            "87 th batch - loss_value: 0.06652312\n",
            "88 th batch - loss_value: 0.035865795\n",
            "89 th batch - loss_value: 0.014040657\n",
            "# 90 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9666666666666667\n",
            "90 th batch - loss_value: 0.071814574\n",
            "91 th batch - loss_value: 0.067831226\n",
            "92 th batch - loss_value: 0.013315083\n",
            "93 th batch - loss_value: 0.020914026\n",
            "94 th batch - loss_value: 0.0025206052\n",
            "95 th batch - loss_value: 0.026691776\n",
            "96 th batch - loss_value: 0.02559001\n",
            "97 th batch - loss_value: 0.01460608\n",
            "98 th batch - loss_value: 0.10205035\n",
            "99 th batch - loss_value: 0.04440615\n",
            "# 100 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9533333333333334\n",
            "100 th batch - loss_value: 0.029225577\n",
            "101 th batch - loss_value: 0.018569088\n",
            "102 th batch - loss_value: 0.0044521345\n",
            "103 th batch - loss_value: 0.002598198\n",
            "104 th batch - loss_value: 0.01518351\n",
            "105 th batch - loss_value: 0.0015259287\n",
            "106 th batch - loss_value: 0.025043748\n",
            "107 th batch - loss_value: 0.21482293\n",
            "108 th batch - loss_value: 0.004206099\n",
            "109 th batch - loss_value: 0.009940807\n",
            "# 110 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "110 th batch - loss_value: 0.020834468\n",
            "111 th batch - loss_value: 0.08707523\n",
            "112 th batch - loss_value: 0.027411012\n",
            "113 th batch - loss_value: 0.007441648\n",
            "114 th batch - loss_value: 0.1093538\n",
            "115 th batch - loss_value: 0.011726362\n",
            "116 th batch - loss_value: 0.08250267\n",
            "117 th batch - loss_value: 0.019480236\n",
            "118 th batch - loss_value: 0.007849274\n",
            "119 th batch - loss_value: 0.0017900611\n",
            "# 120 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "120 th batch - loss_value: 0.05683213\n",
            "121 th batch - loss_value: 0.09862108\n",
            "122 th batch - loss_value: 0.066555604\n",
            "123 th batch - loss_value: 0.068518475\n",
            "124 th batch - loss_value: 0.011924519\n",
            "125 th batch - loss_value: 0.13114403\n",
            "126 th batch - loss_value: 0.048934095\n",
            "127 th batch - loss_value: 0.0037004657\n",
            "128 th batch - loss_value: 0.13242204\n",
            "129 th batch - loss_value: 0.036370568\n",
            "# 130 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "130 th batch - loss_value: 0.114719614\n",
            "131 th batch - loss_value: 0.025685271\n",
            "132 th batch - loss_value: 0.14852096\n",
            "133 th batch - loss_value: 0.07521904\n",
            "134 th batch - loss_value: 0.05206142\n",
            "135 th batch - loss_value: 0.0057968055\n",
            "136 th batch - loss_value: 0.07413909\n",
            "137 th batch - loss_value: 0.004606415\n",
            "138 th batch - loss_value: 0.020711336\n",
            "139 th batch - loss_value: 0.0038109988\n",
            "# 140 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9\n",
            "140 th batch - loss_value: 0.09842717\n",
            "141 th batch - loss_value: 0.024895227\n",
            "142 th batch - loss_value: 0.013208861\n",
            "143 th batch - loss_value: 0.08297794\n",
            "144 th batch - loss_value: 0.038718224\n",
            "145 th batch - loss_value: 0.15183236\n",
            "146 th batch - loss_value: 0.021120552\n",
            "147 th batch - loss_value: 0.026785458\n",
            "148 th batch - loss_value: 0.08346835\n",
            "149 th batch - loss_value: 0.003470819\n",
            "# 150 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9466666666666667\n",
            "150 th batch - loss_value: 0.051431358\n",
            "151 th batch - loss_value: 0.015608367\n",
            "152 th batch - loss_value: 0.0296511\n",
            "153 th batch - loss_value: 0.09237936\n",
            "154 th batch - loss_value: 0.034178834\n",
            "155 th batch - loss_value: 0.006424591\n",
            "156 th batch - loss_value: 0.112459436\n",
            "157 th batch - loss_value: 0.025160242\n",
            "158 th batch - loss_value: 0.063779384\n",
            "159 th batch - loss_value: 0.04628352\n",
            "# 160 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9533333333333334\n",
            "160 th batch - loss_value: 0.061490137\n",
            "161 th batch - loss_value: 0.0035128207\n",
            "162 th batch - loss_value: 0.049439266\n",
            "163 th batch - loss_value: 0.11475418\n",
            "164 th batch - loss_value: 0.039347555\n",
            "165 th batch - loss_value: 0.016554289\n",
            "166 th batch - loss_value: 0.031289227\n",
            "167 th batch - loss_value: 0.02820011\n",
            "168 th batch - loss_value: 0.06643398\n",
            "169 th batch - loss_value: 0.07103799\n",
            "# 170 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "170 th batch - loss_value: 0.009417717\n",
            "171 th batch - loss_value: 0.10774037\n",
            "172 th batch - loss_value: 0.017521275\n",
            "173 th batch - loss_value: 0.024152238\n",
            "174 th batch - loss_value: 0.056202408\n",
            "175 th batch - loss_value: 0.035418786\n",
            "176 th batch - loss_value: 0.113465495\n",
            "177 th batch - loss_value: 0.007400237\n",
            "178 th batch - loss_value: 0.026508242\n",
            "179 th batch - loss_value: 0.026152117\n",
            "# 180 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "180 th batch - loss_value: 0.00847444\n",
            "181 th batch - loss_value: 0.080361106\n",
            "182 th batch - loss_value: 0.029577263\n",
            "183 th batch - loss_value: 0.011095862\n",
            "184 th batch - loss_value: 0.0003910026\n",
            "185 th batch - loss_value: 0.014133034\n",
            "186 th batch - loss_value: 0.018639166\n",
            "187 th batch - loss_value: 0.026529036\n",
            "188 th batch - loss_value: 0.009860759\n",
            "189 th batch - loss_value: 0.032210242\n",
            "# 190 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "190 th batch - loss_value: 0.0215473\n",
            "191 th batch - loss_value: 0.014104019\n",
            "192 th batch - loss_value: 0.007927914\n",
            "193 th batch - loss_value: 0.2314728\n",
            "194 th batch - loss_value: 0.044142175\n",
            "195 th batch - loss_value: 0.011690031\n",
            "196 th batch - loss_value: 0.0034823567\n",
            "197 th batch - loss_value: 0.03737897\n",
            "198 th batch - loss_value: 0.035394717\n",
            "199 th batch - loss_value: 0.046833385\n",
            "# 200 th batch - train_accuracy: 0.95 , test_accuracy: 0.92\n",
            "200 th batch - loss_value: 0.06544741\n",
            "201 th batch - loss_value: 0.0034467739\n",
            "202 th batch - loss_value: 0.02205802\n",
            "203 th batch - loss_value: 0.0062988014\n",
            "204 th batch - loss_value: 0.03912006\n",
            "205 th batch - loss_value: 0.0077511\n",
            "206 th batch - loss_value: 0.06744143\n",
            "207 th batch - loss_value: 0.0394373\n",
            "208 th batch - loss_value: 0.0050041843\n",
            "209 th batch - loss_value: 0.030802876\n",
            "# 210 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "210 th batch - loss_value: 0.024674814\n",
            "211 th batch - loss_value: 0.059121292\n",
            "212 th batch - loss_value: 0.09474588\n",
            "213 th batch - loss_value: 0.015222153\n",
            "214 th batch - loss_value: 0.04505277\n",
            "215 th batch - loss_value: 0.035699446\n",
            "216 th batch - loss_value: 0.030780824\n",
            "217 th batch - loss_value: 0.055261686\n",
            "218 th batch - loss_value: 0.003467533\n",
            "219 th batch - loss_value: 0.01808114\n",
            "# 220 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9066666666666666\n",
            "220 th batch - loss_value: 0.10434917\n",
            "221 th batch - loss_value: 0.007426739\n",
            "222 th batch - loss_value: 0.0037839178\n",
            "223 th batch - loss_value: 0.019324362\n",
            "224 th batch - loss_value: 0.11334364\n",
            "225 th batch - loss_value: 0.00073947676\n",
            "226 th batch - loss_value: 0.06264551\n",
            "227 th batch - loss_value: 0.07298632\n",
            "228 th batch - loss_value: 0.03193777\n",
            "229 th batch - loss_value: 0.0009300416\n",
            "# 230 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "230 th batch - loss_value: 0.042397045\n",
            "231 th batch - loss_value: 0.0040046494\n",
            "232 th batch - loss_value: 0.0027516303\n",
            "233 th batch - loss_value: 0.010878522\n",
            "234 th batch - loss_value: 0.078099154\n",
            "235 th batch - loss_value: 0.04834666\n",
            "236 th batch - loss_value: 0.10990823\n",
            "237 th batch - loss_value: 0.023064194\n",
            "238 th batch - loss_value: 0.010071714\n",
            "239 th batch - loss_value: 0.018696813\n",
            "# 240 th batch - train_accuracy: 1.0 , test_accuracy: 0.9266666666666666\n",
            "240 th batch - loss_value: 0.012696167\n",
            "241 th batch - loss_value: 0.025527155\n",
            "242 th batch - loss_value: 0.0027021647\n",
            "243 th batch - loss_value: 0.064536065\n",
            "244 th batch - loss_value: 0.0031031151\n",
            "245 th batch - loss_value: 0.021237863\n",
            "246 th batch - loss_value: 0.013082806\n",
            "247 th batch - loss_value: 0.038925573\n",
            "248 th batch - loss_value: 0.014542863\n",
            "249 th batch - loss_value: 0.102626204\n",
            "# 250 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9466666666666667\n",
            "250 th batch - loss_value: 0.105145365\n",
            "251 th batch - loss_value: 0.08565376\n",
            "252 th batch - loss_value: 0.008378919\n",
            "253 th batch - loss_value: 0.013902069\n",
            "254 th batch - loss_value: 0.045754153\n",
            "255 th batch - loss_value: 0.035350237\n",
            "256 th batch - loss_value: 0.020723762\n",
            "257 th batch - loss_value: 0.12649676\n",
            "258 th batch - loss_value: 0.0049255053\n",
            "259 th batch - loss_value: 0.008376613\n",
            "# 260 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "260 th batch - loss_value: 0.0050612665\n",
            "261 th batch - loss_value: 0.014389639\n",
            "262 th batch - loss_value: 0.00087706064\n",
            "263 th batch - loss_value: 0.045804907\n",
            "264 th batch - loss_value: 0.0022259748\n",
            "265 th batch - loss_value: 0.029551966\n",
            "266 th batch - loss_value: 0.004536794\n",
            "267 th batch - loss_value: 0.008343553\n",
            "268 th batch - loss_value: 0.030314285\n",
            "269 th batch - loss_value: 0.040030766\n",
            "# 270 th batch - train_accuracy: 0.95 , test_accuracy: 0.96\n",
            "270 th batch - loss_value: 0.06683563\n",
            "271 th batch - loss_value: 0.026129441\n",
            "272 th batch - loss_value: 0.015640682\n",
            "273 th batch - loss_value: 0.030049693\n",
            "274 th batch - loss_value: 0.008218364\n",
            "275 th batch - loss_value: 0.007535385\n",
            "276 th batch - loss_value: 0.03434151\n",
            "277 th batch - loss_value: 0.02209568\n",
            "278 th batch - loss_value: 0.02104736\n",
            "279 th batch - loss_value: 0.036741115\n",
            "# 280 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8933333333333333\n",
            "280 th batch - loss_value: 0.054786935\n",
            "281 th batch - loss_value: 0.082881406\n",
            "282 th batch - loss_value: 0.0071490016\n",
            "283 th batch - loss_value: 0.10908198\n",
            "284 th batch - loss_value: 0.2003904\n",
            "285 th batch - loss_value: 0.0076568234\n",
            "286 th batch - loss_value: 0.0024681264\n",
            "287 th batch - loss_value: 0.0135598425\n",
            "288 th batch - loss_value: 0.024214622\n",
            "289 th batch - loss_value: 0.012630231\n",
            "# 290 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "290 th batch - loss_value: 0.066623844\n",
            "291 th batch - loss_value: 0.057344064\n",
            "292 th batch - loss_value: 0.13556352\n",
            "293 th batch - loss_value: 0.035466656\n",
            "294 th batch - loss_value: 0.143786\n",
            "295 th batch - loss_value: 0.020031165\n",
            "296 th batch - loss_value: 0.03219118\n",
            "297 th batch - loss_value: 0.009190094\n",
            "298 th batch - loss_value: 0.08679283\n",
            "299 th batch - loss_value: 0.010662631\n",
            "# 300 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "300 th batch - loss_value: 0.007522018\n",
            "301 th batch - loss_value: 0.18428889\n",
            "302 th batch - loss_value: 0.15606187\n",
            "303 th batch - loss_value: 0.10539146\n",
            "304 th batch - loss_value: 0.05715605\n",
            "305 th batch - loss_value: 0.063175924\n",
            "306 th batch - loss_value: 0.008856442\n",
            "307 th batch - loss_value: 0.008505702\n",
            "308 th batch - loss_value: 0.003921919\n",
            "309 th batch - loss_value: 0.038085114\n",
            "# 310 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.88\n",
            "310 th batch - loss_value: 0.026929712\n",
            "311 th batch - loss_value: 0.055976648\n",
            "312 th batch - loss_value: 0.13672869\n",
            "313 th batch - loss_value: 0.13095726\n",
            "314 th batch - loss_value: 0.016038727\n",
            "315 th batch - loss_value: 0.0874824\n",
            "316 th batch - loss_value: 0.008783513\n",
            "317 th batch - loss_value: 0.019947302\n",
            "318 th batch - loss_value: 0.030520266\n",
            "319 th batch - loss_value: 0.031010997\n",
            "# 320 th batch - train_accuracy: 0.95 , test_accuracy: 0.88\n",
            "320 th batch - loss_value: 0.07545719\n",
            "321 th batch - loss_value: 0.15188043\n",
            "322 th batch - loss_value: 0.033433445\n",
            "323 th batch - loss_value: 0.15543301\n",
            "324 th batch - loss_value: 0.23110981\n",
            "325 th batch - loss_value: 0.15757321\n",
            "326 th batch - loss_value: 0.12570548\n",
            "327 th batch - loss_value: 0.08766994\n",
            "328 th batch - loss_value: 0.012530757\n",
            "329 th batch - loss_value: 0.07243504\n",
            "# 330 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9\n",
            "330 th batch - loss_value: 0.04196222\n",
            "331 th batch - loss_value: 0.05044321\n",
            "332 th batch - loss_value: 0.010499933\n",
            "333 th batch - loss_value: 0.020559635\n",
            "334 th batch - loss_value: 0.11095347\n",
            "335 th batch - loss_value: 0.05918606\n",
            "336 th batch - loss_value: 0.014728209\n",
            "337 th batch - loss_value: 0.08472672\n",
            "338 th batch - loss_value: 0.10266279\n",
            "339 th batch - loss_value: 0.0761409\n",
            "# 340 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "340 th batch - loss_value: 0.017450325\n",
            "341 th batch - loss_value: 0.0384492\n",
            "342 th batch - loss_value: 0.034547854\n",
            "343 th batch - loss_value: 0.06861819\n",
            "344 th batch - loss_value: 0.14331712\n",
            "345 th batch - loss_value: 0.048875116\n",
            "346 th batch - loss_value: 0.011014584\n",
            "347 th batch - loss_value: 0.07526506\n",
            "348 th batch - loss_value: 0.06224931\n",
            "349 th batch - loss_value: 0.04333671\n",
            "# 350 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9533333333333334\n",
            "350 th batch - loss_value: 0.22504012\n",
            "351 th batch - loss_value: 0.09022756\n",
            "352 th batch - loss_value: 0.026381468\n",
            "353 th batch - loss_value: 0.04288514\n",
            "354 th batch - loss_value: 0.13526262\n",
            "355 th batch - loss_value: 0.046927102\n",
            "356 th batch - loss_value: 0.11448766\n",
            "357 th batch - loss_value: 0.023903301\n",
            "358 th batch - loss_value: 0.06683458\n",
            "359 th batch - loss_value: 0.01056111\n",
            "# 360 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "360 th batch - loss_value: 0.022026598\n",
            "361 th batch - loss_value: 0.019961987\n",
            "362 th batch - loss_value: 0.08849607\n",
            "363 th batch - loss_value: 0.028629133\n",
            "364 th batch - loss_value: 0.053739704\n",
            "365 th batch - loss_value: 0.018465778\n",
            "366 th batch - loss_value: 0.05503021\n",
            "367 th batch - loss_value: 0.18041734\n",
            "368 th batch - loss_value: 0.05583064\n",
            "369 th batch - loss_value: 0.020645116\n",
            "# 370 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "370 th batch - loss_value: 0.030630808\n",
            "371 th batch - loss_value: 0.020453533\n",
            "372 th batch - loss_value: 0.10875358\n",
            "373 th batch - loss_value: 0.011197563\n",
            "374 th batch - loss_value: 0.08506212\n",
            "375 th batch - loss_value: 0.063072965\n",
            "376 th batch - loss_value: 0.14655907\n",
            "377 th batch - loss_value: 0.03548961\n",
            "378 th batch - loss_value: 0.006508378\n",
            "379 th batch - loss_value: 0.02833879\n",
            "# 380 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "380 th batch - loss_value: 0.0197963\n",
            "381 th batch - loss_value: 0.046809383\n",
            "382 th batch - loss_value: 0.04907185\n",
            "383 th batch - loss_value: 0.05195748\n",
            "384 th batch - loss_value: 0.02068504\n",
            "385 th batch - loss_value: 0.095082745\n",
            "386 th batch - loss_value: 0.048463408\n",
            "387 th batch - loss_value: 0.09263914\n",
            "388 th batch - loss_value: 0.13553731\n",
            "389 th batch - loss_value: 0.069917284\n",
            "# 390 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "390 th batch - loss_value: 0.040024757\n",
            "391 th batch - loss_value: 0.014782569\n",
            "392 th batch - loss_value: 0.040623866\n",
            "393 th batch - loss_value: 0.14956744\n",
            "394 th batch - loss_value: 0.074015126\n",
            "395 th batch - loss_value: 0.05116616\n",
            "396 th batch - loss_value: 0.035779733\n",
            "397 th batch - loss_value: 0.032594506\n",
            "398 th batch - loss_value: 0.06325719\n",
            "399 th batch - loss_value: 0.019967614\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 1.0 , test_accuracy: 0.8866666666666667\n",
            "0 th batch - loss_value: 0.022197925\n",
            "1 th batch - loss_value: 0.07717535\n",
            "2 th batch - loss_value: 0.09432575\n",
            "3 th batch - loss_value: 0.02252064\n",
            "4 th batch - loss_value: 0.013202623\n",
            "5 th batch - loss_value: 0.056756616\n",
            "6 th batch - loss_value: 0.016758086\n",
            "7 th batch - loss_value: 0.077859096\n",
            "8 th batch - loss_value: 0.0043754983\n",
            "9 th batch - loss_value: 0.13226412\n",
            "# 10 th batch - train_accuracy: 0.95 , test_accuracy: 0.9266666666666666\n",
            "10 th batch - loss_value: 0.11610271\n",
            "11 th batch - loss_value: 0.032867663\n",
            "12 th batch - loss_value: 0.012236459\n",
            "13 th batch - loss_value: 0.07524667\n",
            "14 th batch - loss_value: 0.0072113643\n",
            "15 th batch - loss_value: 0.07755396\n",
            "16 th batch - loss_value: 0.022814043\n",
            "17 th batch - loss_value: 0.08708842\n",
            "18 th batch - loss_value: 0.08368526\n",
            "19 th batch - loss_value: 0.079449974\n",
            "# 20 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "20 th batch - loss_value: 0.036484536\n",
            "21 th batch - loss_value: 0.030909367\n",
            "22 th batch - loss_value: 0.10614415\n",
            "23 th batch - loss_value: 0.034637474\n",
            "24 th batch - loss_value: 0.18954341\n",
            "25 th batch - loss_value: 0.03003705\n",
            "26 th batch - loss_value: 0.0051978095\n",
            "27 th batch - loss_value: 0.09473645\n",
            "28 th batch - loss_value: 0.011505327\n",
            "29 th batch - loss_value: 0.03640961\n",
            "# 30 th batch - train_accuracy: 0.95 , test_accuracy: 0.9333333333333333\n",
            "30 th batch - loss_value: 0.084988646\n",
            "31 th batch - loss_value: 0.03796898\n",
            "32 th batch - loss_value: 0.14281482\n",
            "33 th batch - loss_value: 0.20699479\n",
            "34 th batch - loss_value: 0.017961757\n",
            "35 th batch - loss_value: 0.05340782\n",
            "36 th batch - loss_value: 0.10698429\n",
            "37 th batch - loss_value: 0.050554417\n",
            "38 th batch - loss_value: 0.03677906\n",
            "39 th batch - loss_value: 0.3290254\n",
            "# 40 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8733333333333333\n",
            "40 th batch - loss_value: 0.058097895\n",
            "41 th batch - loss_value: 0.04446512\n",
            "42 th batch - loss_value: 0.045565482\n",
            "43 th batch - loss_value: 0.066260554\n",
            "44 th batch - loss_value: 0.0035889905\n",
            "45 th batch - loss_value: 0.23544486\n",
            "46 th batch - loss_value: 0.036044735\n",
            "47 th batch - loss_value: 0.016372154\n",
            "48 th batch - loss_value: 0.004817895\n",
            "49 th batch - loss_value: 0.087407894\n",
            "# 50 th batch - train_accuracy: 1.0 , test_accuracy: 0.8866666666666667\n",
            "50 th batch - loss_value: 0.023267318\n",
            "51 th batch - loss_value: 0.012661806\n",
            "52 th batch - loss_value: 0.11974961\n",
            "53 th batch - loss_value: 0.12755354\n",
            "54 th batch - loss_value: 0.15453784\n",
            "55 th batch - loss_value: 0.07036397\n",
            "56 th batch - loss_value: 0.013934066\n",
            "57 th batch - loss_value: 0.11979655\n",
            "58 th batch - loss_value: 0.17359373\n",
            "59 th batch - loss_value: 0.07061771\n",
            "# 60 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "60 th batch - loss_value: 0.013743581\n",
            "61 th batch - loss_value: 0.062109254\n",
            "62 th batch - loss_value: 0.06048445\n",
            "63 th batch - loss_value: 0.031054344\n",
            "64 th batch - loss_value: 0.12845322\n",
            "65 th batch - loss_value: 0.09412982\n",
            "66 th batch - loss_value: 0.023010334\n",
            "67 th batch - loss_value: 0.1588874\n",
            "68 th batch - loss_value: 0.04104495\n",
            "69 th batch - loss_value: 0.0032038435\n",
            "# 70 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "70 th batch - loss_value: 0.040012967\n",
            "71 th batch - loss_value: 0.032252494\n",
            "72 th batch - loss_value: 0.03940495\n",
            "73 th batch - loss_value: 0.026510688\n",
            "74 th batch - loss_value: 0.03486546\n",
            "75 th batch - loss_value: 0.06231225\n",
            "76 th batch - loss_value: 0.0059188753\n",
            "77 th batch - loss_value: 0.03127038\n",
            "78 th batch - loss_value: 0.014817858\n",
            "79 th batch - loss_value: 0.013591031\n",
            "# 80 th batch - train_accuracy: 1.0 , test_accuracy: 0.9266666666666666\n",
            "80 th batch - loss_value: 0.020522408\n",
            "81 th batch - loss_value: 0.025738563\n",
            "82 th batch - loss_value: 0.046909392\n",
            "83 th batch - loss_value: 0.022412034\n",
            "84 th batch - loss_value: 0.10311214\n",
            "85 th batch - loss_value: 0.023656206\n",
            "86 th batch - loss_value: 0.011323806\n",
            "87 th batch - loss_value: 0.052238423\n",
            "88 th batch - loss_value: 0.047247987\n",
            "89 th batch - loss_value: 0.12316842\n",
            "# 90 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.9533333333333334\n",
            "90 th batch - loss_value: 0.20509185\n",
            "91 th batch - loss_value: 0.06606536\n",
            "92 th batch - loss_value: 0.035936102\n",
            "93 th batch - loss_value: 0.060400236\n",
            "94 th batch - loss_value: 0.113651015\n",
            "95 th batch - loss_value: 0.06463698\n",
            "96 th batch - loss_value: 0.020224271\n",
            "97 th batch - loss_value: 0.0883507\n",
            "98 th batch - loss_value: 0.018658768\n",
            "99 th batch - loss_value: 0.009525029\n",
            "# 100 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "100 th batch - loss_value: 0.050818015\n",
            "101 th batch - loss_value: 0.048830476\n",
            "102 th batch - loss_value: 0.05117266\n",
            "103 th batch - loss_value: 0.15338369\n",
            "104 th batch - loss_value: 0.0835052\n",
            "105 th batch - loss_value: 0.012522984\n",
            "106 th batch - loss_value: 0.030526882\n",
            "107 th batch - loss_value: 0.02773852\n",
            "108 th batch - loss_value: 0.012882726\n",
            "109 th batch - loss_value: 0.028912101\n",
            "# 110 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9266666666666666\n",
            "110 th batch - loss_value: 0.15668239\n",
            "111 th batch - loss_value: 0.07150563\n",
            "112 th batch - loss_value: 0.04287498\n",
            "113 th batch - loss_value: 0.109607086\n",
            "114 th batch - loss_value: 0.01800601\n",
            "115 th batch - loss_value: 0.19721606\n",
            "116 th batch - loss_value: 0.018881738\n",
            "117 th batch - loss_value: 0.010579916\n",
            "118 th batch - loss_value: 0.11380004\n",
            "119 th batch - loss_value: 0.024024103\n",
            "# 120 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9133333333333333\n",
            "120 th batch - loss_value: 0.15211862\n",
            "121 th batch - loss_value: 0.059897058\n",
            "122 th batch - loss_value: 0.0073267785\n",
            "123 th batch - loss_value: 0.0075430763\n",
            "124 th batch - loss_value: 0.041996215\n",
            "125 th batch - loss_value: 0.011104572\n",
            "126 th batch - loss_value: 0.029185181\n",
            "127 th batch - loss_value: 0.08516475\n",
            "128 th batch - loss_value: 0.01949521\n",
            "129 th batch - loss_value: 0.0805451\n",
            "# 130 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "130 th batch - loss_value: 0.0206686\n",
            "131 th batch - loss_value: 0.116817705\n",
            "132 th batch - loss_value: 0.09214453\n",
            "133 th batch - loss_value: 0.045105163\n",
            "134 th batch - loss_value: 0.03487414\n",
            "135 th batch - loss_value: 0.0104076825\n",
            "136 th batch - loss_value: 0.1202195\n",
            "137 th batch - loss_value: 0.06880491\n",
            "138 th batch - loss_value: 0.067890644\n",
            "139 th batch - loss_value: 0.02236403\n",
            "# 140 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "140 th batch - loss_value: 0.009309566\n",
            "141 th batch - loss_value: 0.01127699\n",
            "142 th batch - loss_value: 0.044250008\n",
            "143 th batch - loss_value: 0.05778158\n",
            "144 th batch - loss_value: 0.0459358\n",
            "145 th batch - loss_value: 0.011395791\n",
            "146 th batch - loss_value: 0.0117166005\n",
            "147 th batch - loss_value: 0.0128290495\n",
            "148 th batch - loss_value: 0.06884573\n",
            "149 th batch - loss_value: 0.033896618\n",
            "# 150 th batch - train_accuracy: 0.9166666666666666 , test_accuracy: 0.9133333333333333\n",
            "150 th batch - loss_value: 0.13108495\n",
            "151 th batch - loss_value: 0.023695327\n",
            "152 th batch - loss_value: 0.31754255\n",
            "153 th batch - loss_value: 0.09928311\n",
            "154 th batch - loss_value: 0.023597203\n",
            "155 th batch - loss_value: 0.006051117\n",
            "156 th batch - loss_value: 0.058918793\n",
            "157 th batch - loss_value: 0.10939147\n",
            "158 th batch - loss_value: 0.017327772\n",
            "159 th batch - loss_value: 0.056826893\n",
            "# 160 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "160 th batch - loss_value: 0.027702212\n",
            "161 th batch - loss_value: 0.03682114\n",
            "162 th batch - loss_value: 0.1114334\n",
            "163 th batch - loss_value: 0.003094423\n",
            "164 th batch - loss_value: 0.06832044\n",
            "165 th batch - loss_value: 0.01772088\n",
            "166 th batch - loss_value: 0.020159844\n",
            "167 th batch - loss_value: 0.012683366\n",
            "168 th batch - loss_value: 0.119818665\n",
            "169 th batch - loss_value: 0.04873777\n",
            "# 170 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "170 th batch - loss_value: 0.00919741\n",
            "171 th batch - loss_value: 0.019272767\n",
            "172 th batch - loss_value: 0.1483647\n",
            "173 th batch - loss_value: 0.054377344\n",
            "174 th batch - loss_value: 0.049199063\n",
            "175 th batch - loss_value: 0.059420764\n",
            "176 th batch - loss_value: 0.100523226\n",
            "177 th batch - loss_value: 0.05531421\n",
            "178 th batch - loss_value: 0.020639451\n",
            "179 th batch - loss_value: 0.0037073486\n",
            "# 180 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "180 th batch - loss_value: 0.047317486\n",
            "181 th batch - loss_value: 0.0034124374\n",
            "182 th batch - loss_value: 0.036772452\n",
            "183 th batch - loss_value: 0.016282687\n",
            "184 th batch - loss_value: 0.06336269\n",
            "185 th batch - loss_value: 0.02234905\n",
            "186 th batch - loss_value: 0.03538388\n",
            "187 th batch - loss_value: 0.06337875\n",
            "188 th batch - loss_value: 0.019283447\n",
            "189 th batch - loss_value: 0.14254528\n",
            "# 190 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "190 th batch - loss_value: 0.020175159\n",
            "191 th batch - loss_value: 0.02149391\n",
            "192 th batch - loss_value: 0.025075663\n",
            "193 th batch - loss_value: 0.039650474\n",
            "194 th batch - loss_value: 0.014179783\n",
            "195 th batch - loss_value: 0.051786296\n",
            "196 th batch - loss_value: 0.009099289\n",
            "197 th batch - loss_value: 0.08216361\n",
            "198 th batch - loss_value: 0.01069232\n",
            "199 th batch - loss_value: 0.068886474\n",
            "# 200 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "200 th batch - loss_value: 0.032652926\n",
            "201 th batch - loss_value: 0.026738353\n",
            "202 th batch - loss_value: 0.04228282\n",
            "203 th batch - loss_value: 0.0076209377\n",
            "204 th batch - loss_value: 0.0437232\n",
            "205 th batch - loss_value: 0.005356173\n",
            "206 th batch - loss_value: 0.0074910014\n",
            "207 th batch - loss_value: 0.012070467\n",
            "208 th batch - loss_value: 0.02420889\n",
            "209 th batch - loss_value: 0.035354275\n",
            "# 210 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "210 th batch - loss_value: 0.007288047\n",
            "211 th batch - loss_value: 0.016065594\n",
            "212 th batch - loss_value: 0.0062950077\n",
            "213 th batch - loss_value: 0.093408965\n",
            "214 th batch - loss_value: 0.006841662\n",
            "215 th batch - loss_value: 0.017725058\n",
            "216 th batch - loss_value: 0.056314815\n",
            "217 th batch - loss_value: 0.0022522404\n",
            "218 th batch - loss_value: 0.0024270809\n",
            "219 th batch - loss_value: 0.029980592\n",
            "# 220 th batch - train_accuracy: 1.0 , test_accuracy: 0.9266666666666666\n",
            "220 th batch - loss_value: 0.00064460834\n",
            "221 th batch - loss_value: 0.07540348\n",
            "222 th batch - loss_value: 0.0057460708\n",
            "223 th batch - loss_value: 0.009432544\n",
            "224 th batch - loss_value: 0.016121602\n",
            "225 th batch - loss_value: 0.00580644\n",
            "226 th batch - loss_value: 0.023112457\n",
            "227 th batch - loss_value: 0.09864109\n",
            "228 th batch - loss_value: 0.0029823084\n",
            "229 th batch - loss_value: 0.009685442\n",
            "# 230 th batch - train_accuracy: 1.0 , test_accuracy: 0.9266666666666666\n",
            "230 th batch - loss_value: 0.0025313373\n",
            "231 th batch - loss_value: 0.010569584\n",
            "232 th batch - loss_value: 0.016268292\n",
            "233 th batch - loss_value: 0.09100003\n",
            "234 th batch - loss_value: 0.0038912788\n",
            "235 th batch - loss_value: 0.02869824\n",
            "236 th batch - loss_value: 0.05890124\n",
            "237 th batch - loss_value: 0.011650308\n",
            "238 th batch - loss_value: 0.05346581\n",
            "239 th batch - loss_value: 0.0058146035\n",
            "# 240 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "240 th batch - loss_value: 0.01989622\n",
            "241 th batch - loss_value: 0.019599011\n",
            "242 th batch - loss_value: 0.0047774133\n",
            "243 th batch - loss_value: 0.03528594\n",
            "244 th batch - loss_value: 0.03297881\n",
            "245 th batch - loss_value: 0.18323003\n",
            "246 th batch - loss_value: 0.07345976\n",
            "247 th batch - loss_value: 0.029388016\n",
            "248 th batch - loss_value: 0.0053508054\n",
            "249 th batch - loss_value: 0.02164408\n",
            "# 250 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "250 th batch - loss_value: 0.027037693\n",
            "251 th batch - loss_value: 0.08588509\n",
            "252 th batch - loss_value: 0.17467846\n",
            "253 th batch - loss_value: 0.006840487\n",
            "254 th batch - loss_value: 0.010655699\n",
            "255 th batch - loss_value: 0.0044491207\n",
            "256 th batch - loss_value: 0.06774851\n",
            "257 th batch - loss_value: 0.09629929\n",
            "258 th batch - loss_value: 0.0060462155\n",
            "259 th batch - loss_value: 0.065406054\n",
            "# 260 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9666666666666667\n",
            "260 th batch - loss_value: 0.13483097\n",
            "261 th batch - loss_value: 0.06236624\n",
            "262 th batch - loss_value: 0.005979738\n",
            "263 th batch - loss_value: 0.061459307\n",
            "264 th batch - loss_value: 0.08249711\n",
            "265 th batch - loss_value: 0.03035861\n",
            "266 th batch - loss_value: 0.021995073\n",
            "267 th batch - loss_value: 0.10914338\n",
            "268 th batch - loss_value: 0.08575415\n",
            "269 th batch - loss_value: 0.24434608\n",
            "# 270 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9333333333333333\n",
            "270 th batch - loss_value: 0.07454346\n",
            "271 th batch - loss_value: 0.042152703\n",
            "272 th batch - loss_value: 0.009960606\n",
            "273 th batch - loss_value: 0.021709858\n",
            "274 th batch - loss_value: 0.021127125\n",
            "275 th batch - loss_value: 0.16946666\n",
            "276 th batch - loss_value: 0.014821647\n",
            "277 th batch - loss_value: 0.0888991\n",
            "278 th batch - loss_value: 0.05902624\n",
            "279 th batch - loss_value: 0.06359189\n",
            "# 280 th batch - train_accuracy: 0.95 , test_accuracy: 0.92\n",
            "280 th batch - loss_value: 0.3772774\n",
            "281 th batch - loss_value: 0.047543816\n",
            "282 th batch - loss_value: 0.04607152\n",
            "283 th batch - loss_value: 0.021675449\n",
            "284 th batch - loss_value: 0.14692453\n",
            "285 th batch - loss_value: 0.020772511\n",
            "286 th batch - loss_value: 0.46622616\n",
            "287 th batch - loss_value: 0.1000978\n",
            "288 th batch - loss_value: 0.10788286\n",
            "289 th batch - loss_value: 0.17724076\n",
            "# 290 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "290 th batch - loss_value: 0.06448866\n",
            "291 th batch - loss_value: 0.035797443\n",
            "292 th batch - loss_value: 0.19210052\n",
            "293 th batch - loss_value: 0.033027448\n",
            "294 th batch - loss_value: 0.06740236\n",
            "295 th batch - loss_value: 0.11668337\n",
            "296 th batch - loss_value: 0.10194911\n",
            "297 th batch - loss_value: 0.0038533886\n",
            "298 th batch - loss_value: 0.1185539\n",
            "299 th batch - loss_value: 0.0844645\n",
            "# 300 th batch - train_accuracy: 0.95 , test_accuracy: 0.9266666666666666\n",
            "300 th batch - loss_value: 0.1548526\n",
            "301 th batch - loss_value: 0.08928909\n",
            "302 th batch - loss_value: 0.1724625\n",
            "303 th batch - loss_value: 0.109046355\n",
            "304 th batch - loss_value: 0.027565623\n",
            "305 th batch - loss_value: 0.08912615\n",
            "306 th batch - loss_value: 0.011307948\n",
            "307 th batch - loss_value: 0.08604603\n",
            "308 th batch - loss_value: 0.078044906\n",
            "309 th batch - loss_value: 0.06378073\n",
            "# 310 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.8933333333333333\n",
            "310 th batch - loss_value: 0.057951614\n",
            "311 th batch - loss_value: 0.17949755\n",
            "312 th batch - loss_value: 0.0977718\n",
            "313 th batch - loss_value: 0.18679969\n",
            "314 th batch - loss_value: 0.04443776\n",
            "315 th batch - loss_value: 0.0936663\n",
            "316 th batch - loss_value: 0.01686549\n",
            "317 th batch - loss_value: 0.06243657\n",
            "318 th batch - loss_value: 0.0620492\n",
            "319 th batch - loss_value: 0.014213011\n",
            "# 320 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.88\n",
            "320 th batch - loss_value: 0.08334789\n",
            "321 th batch - loss_value: 0.15709488\n",
            "322 th batch - loss_value: 0.062858105\n",
            "323 th batch - loss_value: 0.104334965\n",
            "324 th batch - loss_value: 0.09481785\n",
            "325 th batch - loss_value: 0.080736086\n",
            "326 th batch - loss_value: 0.12146176\n",
            "327 th batch - loss_value: 0.08541788\n",
            "328 th batch - loss_value: 0.110510334\n",
            "329 th batch - loss_value: 0.06206418\n",
            "# 330 th batch - train_accuracy: 1.0 , test_accuracy: 0.88\n",
            "330 th batch - loss_value: 0.014006571\n",
            "331 th batch - loss_value: 0.014398656\n",
            "332 th batch - loss_value: 0.09658869\n",
            "333 th batch - loss_value: 0.029411934\n",
            "334 th batch - loss_value: 0.038817916\n",
            "335 th batch - loss_value: 0.026126673\n",
            "336 th batch - loss_value: 0.06365178\n",
            "337 th batch - loss_value: 0.08683441\n",
            "338 th batch - loss_value: 0.037833262\n",
            "339 th batch - loss_value: 0.03073516\n",
            "# 340 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "340 th batch - loss_value: 0.06696653\n",
            "341 th batch - loss_value: 0.044994816\n",
            "342 th batch - loss_value: 0.08869348\n",
            "343 th batch - loss_value: 0.0486669\n",
            "344 th batch - loss_value: 0.018228984\n",
            "345 th batch - loss_value: 0.005980564\n",
            "346 th batch - loss_value: 0.01540817\n",
            "347 th batch - loss_value: 0.00908246\n",
            "348 th batch - loss_value: 0.0011610232\n",
            "349 th batch - loss_value: 0.056648467\n",
            "# 350 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9466666666666667\n",
            "350 th batch - loss_value: 0.0933115\n",
            "351 th batch - loss_value: 0.019128907\n",
            "352 th batch - loss_value: 0.0035128286\n",
            "353 th batch - loss_value: 0.04618995\n",
            "354 th batch - loss_value: 0.06102504\n",
            "355 th batch - loss_value: 0.16551322\n",
            "356 th batch - loss_value: 0.024839196\n",
            "357 th batch - loss_value: 0.016439773\n",
            "358 th batch - loss_value: 0.041014507\n",
            "359 th batch - loss_value: 0.022886725\n",
            "# 360 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "360 th batch - loss_value: 0.020156706\n",
            "361 th batch - loss_value: 0.10529915\n",
            "362 th batch - loss_value: 0.21652345\n",
            "363 th batch - loss_value: 0.010415203\n",
            "364 th batch - loss_value: 0.008725848\n",
            "365 th batch - loss_value: 0.023834176\n",
            "366 th batch - loss_value: 0.035686206\n",
            "367 th batch - loss_value: 0.04462344\n",
            "368 th batch - loss_value: 0.0119661605\n",
            "369 th batch - loss_value: 0.07134666\n",
            "# 370 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "370 th batch - loss_value: 0.014081462\n",
            "371 th batch - loss_value: 0.07943891\n",
            "372 th batch - loss_value: 0.05116818\n",
            "373 th batch - loss_value: 0.040830173\n",
            "374 th batch - loss_value: 0.008372178\n",
            "375 th batch - loss_value: 0.035450406\n",
            "376 th batch - loss_value: 0.011167892\n",
            "377 th batch - loss_value: 0.0069558844\n",
            "378 th batch - loss_value: 0.01401378\n",
            "379 th batch - loss_value: 0.016356373\n",
            "# 380 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "380 th batch - loss_value: 0.03216474\n",
            "381 th batch - loss_value: 0.016559996\n",
            "382 th batch - loss_value: 0.07735666\n",
            "383 th batch - loss_value: 0.04158149\n",
            "384 th batch - loss_value: 0.03128031\n",
            "385 th batch - loss_value: 0.009716396\n",
            "386 th batch - loss_value: 0.015819889\n",
            "387 th batch - loss_value: 0.007756955\n",
            "388 th batch - loss_value: 0.06193449\n",
            "389 th batch - loss_value: 0.09270979\n",
            "# 390 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "390 th batch - loss_value: 0.046968613\n",
            "391 th batch - loss_value: 0.0130361505\n",
            "392 th batch - loss_value: 0.004925283\n",
            "393 th batch - loss_value: 0.001802275\n",
            "394 th batch - loss_value: 0.015574836\n",
            "395 th batch - loss_value: 0.07792968\n",
            "396 th batch - loss_value: 0.018310076\n",
            "397 th batch - loss_value: 0.059791856\n",
            "398 th batch - loss_value: 0.0048163254\n",
            "399 th batch - loss_value: 0.0027956679\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "0 th batch - loss_value: 0.00819386\n",
            "1 th batch - loss_value: 0.009837505\n",
            "2 th batch - loss_value: 0.004433247\n",
            "3 th batch - loss_value: 0.09775028\n",
            "4 th batch - loss_value: 0.018875968\n",
            "5 th batch - loss_value: 0.088820115\n",
            "6 th batch - loss_value: 0.16688824\n",
            "7 th batch - loss_value: 0.056391243\n",
            "8 th batch - loss_value: 0.014053445\n",
            "9 th batch - loss_value: 0.015612854\n",
            "# 10 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.94\n",
            "10 th batch - loss_value: 0.033033222\n",
            "11 th batch - loss_value: 0.0026053877\n",
            "12 th batch - loss_value: 0.08343317\n",
            "13 th batch - loss_value: 0.019984636\n",
            "14 th batch - loss_value: 0.021376071\n",
            "15 th batch - loss_value: 0.15193585\n",
            "16 th batch - loss_value: 0.031585336\n",
            "17 th batch - loss_value: 0.033520706\n",
            "18 th batch - loss_value: 0.024555681\n",
            "19 th batch - loss_value: 0.050180867\n",
            "# 20 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9133333333333333\n",
            "20 th batch - loss_value: 0.06522305\n",
            "21 th batch - loss_value: 0.016293531\n",
            "22 th batch - loss_value: 0.04158572\n",
            "23 th batch - loss_value: 0.011313108\n",
            "24 th batch - loss_value: 0.04092764\n",
            "25 th batch - loss_value: 0.053543694\n",
            "26 th batch - loss_value: 0.0012380572\n",
            "27 th batch - loss_value: 0.21559832\n",
            "28 th batch - loss_value: 0.03240598\n",
            "29 th batch - loss_value: 0.06685405\n",
            "# 30 th batch - train_accuracy: 1.0 , test_accuracy: 0.9666666666666667\n",
            "30 th batch - loss_value: 0.0035766615\n",
            "31 th batch - loss_value: 0.07580785\n",
            "32 th batch - loss_value: 0.14845508\n",
            "33 th batch - loss_value: 0.0068988954\n",
            "34 th batch - loss_value: 0.0054031005\n",
            "35 th batch - loss_value: 0.029749488\n",
            "36 th batch - loss_value: 0.11973155\n",
            "37 th batch - loss_value: 0.005188798\n",
            "38 th batch - loss_value: 0.00891636\n",
            "39 th batch - loss_value: 0.015688797\n",
            "# 40 th batch - train_accuracy: 1.0 , test_accuracy: 0.9266666666666666\n",
            "40 th batch - loss_value: 0.0024475812\n",
            "41 th batch - loss_value: 0.036885157\n",
            "42 th batch - loss_value: 0.05768483\n",
            "43 th batch - loss_value: 0.047810815\n",
            "44 th batch - loss_value: 0.07697723\n",
            "45 th batch - loss_value: 0.048207685\n",
            "46 th batch - loss_value: 0.1042123\n",
            "47 th batch - loss_value: 0.05337381\n",
            "48 th batch - loss_value: 0.011940622\n",
            "49 th batch - loss_value: 0.0031416337\n",
            "# 50 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "50 th batch - loss_value: 0.0029996408\n",
            "51 th batch - loss_value: 0.019330287\n",
            "52 th batch - loss_value: 0.031762715\n",
            "53 th batch - loss_value: 0.012560737\n",
            "54 th batch - loss_value: 0.026977127\n",
            "55 th batch - loss_value: 0.023929866\n",
            "56 th batch - loss_value: 0.0052683335\n",
            "57 th batch - loss_value: 0.007907591\n",
            "58 th batch - loss_value: 0.045726158\n",
            "59 th batch - loss_value: 0.013120939\n",
            "# 60 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "60 th batch - loss_value: 0.009398716\n",
            "61 th batch - loss_value: 0.028822713\n",
            "62 th batch - loss_value: 0.002991508\n",
            "63 th batch - loss_value: 0.07269289\n",
            "64 th batch - loss_value: 0.000893796\n",
            "65 th batch - loss_value: 0.013847174\n",
            "66 th batch - loss_value: 0.015866132\n",
            "67 th batch - loss_value: 0.0023841222\n",
            "68 th batch - loss_value: 0.004250558\n",
            "69 th batch - loss_value: 0.082675844\n",
            "# 70 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "70 th batch - loss_value: 0.009526525\n",
            "71 th batch - loss_value: 0.008735448\n",
            "72 th batch - loss_value: 0.013591249\n",
            "73 th batch - loss_value: 0.010690407\n",
            "74 th batch - loss_value: 0.012604172\n",
            "75 th batch - loss_value: 0.009054736\n",
            "76 th batch - loss_value: 0.003851976\n",
            "77 th batch - loss_value: 0.045499362\n",
            "78 th batch - loss_value: 0.017157435\n",
            "79 th batch - loss_value: 0.0878583\n",
            "# 80 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "80 th batch - loss_value: 0.016654821\n",
            "81 th batch - loss_value: 0.00029106374\n",
            "82 th batch - loss_value: 0.014308096\n",
            "83 th batch - loss_value: 0.017177073\n",
            "84 th batch - loss_value: 0.02642012\n",
            "85 th batch - loss_value: 0.005467516\n",
            "86 th batch - loss_value: 0.019401133\n",
            "87 th batch - loss_value: 0.053060707\n",
            "88 th batch - loss_value: 0.04095127\n",
            "89 th batch - loss_value: 0.08350245\n",
            "# 90 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.96\n",
            "90 th batch - loss_value: 0.11558009\n",
            "91 th batch - loss_value: 0.08851122\n",
            "92 th batch - loss_value: 0.006598555\n",
            "93 th batch - loss_value: 0.007751738\n",
            "94 th batch - loss_value: 0.028795723\n",
            "95 th batch - loss_value: 0.12730291\n",
            "96 th batch - loss_value: 0.011721092\n",
            "97 th batch - loss_value: 0.07226905\n",
            "98 th batch - loss_value: 0.092712335\n",
            "99 th batch - loss_value: 0.018177476\n",
            "# 100 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.94\n",
            "100 th batch - loss_value: 0.075602\n",
            "101 th batch - loss_value: 0.08090702\n",
            "102 th batch - loss_value: 0.005859804\n",
            "103 th batch - loss_value: 0.0040950445\n",
            "104 th batch - loss_value: 0.1007116\n",
            "105 th batch - loss_value: 0.008818924\n",
            "106 th batch - loss_value: 0.0042216675\n",
            "107 th batch - loss_value: 0.030085871\n",
            "108 th batch - loss_value: 0.026102966\n",
            "109 th batch - loss_value: 0.11722047\n",
            "# 110 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "110 th batch - loss_value: 0.024274802\n",
            "111 th batch - loss_value: 0.0048249913\n",
            "112 th batch - loss_value: 0.061678827\n",
            "113 th batch - loss_value: 0.32123694\n",
            "114 th batch - loss_value: 0.019623442\n",
            "115 th batch - loss_value: 0.13193612\n",
            "116 th batch - loss_value: 0.09677746\n",
            "117 th batch - loss_value: 0.0068855234\n",
            "118 th batch - loss_value: 0.022649635\n",
            "119 th batch - loss_value: 0.008704778\n",
            "# 120 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "120 th batch - loss_value: 0.010645996\n",
            "121 th batch - loss_value: 0.041529987\n",
            "122 th batch - loss_value: 0.0025120995\n",
            "123 th batch - loss_value: 0.038264528\n",
            "124 th batch - loss_value: 0.051302638\n",
            "125 th batch - loss_value: 0.10488109\n",
            "126 th batch - loss_value: 0.048821483\n",
            "127 th batch - loss_value: 0.043505374\n",
            "128 th batch - loss_value: 0.008417758\n",
            "129 th batch - loss_value: 0.018660264\n",
            "# 130 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "130 th batch - loss_value: 0.0204699\n",
            "131 th batch - loss_value: 0.024274291\n",
            "132 th batch - loss_value: 0.039220866\n",
            "133 th batch - loss_value: 0.009571413\n",
            "134 th batch - loss_value: 0.022649784\n",
            "135 th batch - loss_value: 0.036803495\n",
            "136 th batch - loss_value: 0.078377955\n",
            "137 th batch - loss_value: 0.048791144\n",
            "138 th batch - loss_value: 0.11979478\n",
            "139 th batch - loss_value: 0.024591263\n",
            "# 140 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "140 th batch - loss_value: 0.027396243\n",
            "141 th batch - loss_value: 0.02370571\n",
            "142 th batch - loss_value: 0.01826319\n",
            "143 th batch - loss_value: 0.01215975\n",
            "144 th batch - loss_value: 0.008890404\n",
            "145 th batch - loss_value: 0.005970364\n",
            "146 th batch - loss_value: 0.046747804\n",
            "147 th batch - loss_value: 0.046441663\n",
            "148 th batch - loss_value: 0.044890646\n",
            "149 th batch - loss_value: 0.0092733735\n",
            "# 150 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "150 th batch - loss_value: 0.008682463\n",
            "151 th batch - loss_value: 0.021593945\n",
            "152 th batch - loss_value: 0.0417393\n",
            "153 th batch - loss_value: 0.006651225\n",
            "154 th batch - loss_value: 0.008088342\n",
            "155 th batch - loss_value: 0.026142262\n",
            "156 th batch - loss_value: 0.08333426\n",
            "157 th batch - loss_value: 0.06361521\n",
            "158 th batch - loss_value: 0.007013159\n",
            "159 th batch - loss_value: 0.06860868\n",
            "# 160 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "160 th batch - loss_value: 0.031384792\n",
            "161 th batch - loss_value: 0.0063745445\n",
            "162 th batch - loss_value: 0.007871476\n",
            "163 th batch - loss_value: 0.035557162\n",
            "164 th batch - loss_value: 0.00216933\n",
            "165 th batch - loss_value: 0.0123057375\n",
            "166 th batch - loss_value: 0.018768197\n",
            "167 th batch - loss_value: 0.027417712\n",
            "168 th batch - loss_value: 0.0044153286\n",
            "169 th batch - loss_value: 0.0009111048\n",
            "# 170 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "170 th batch - loss_value: 0.010404927\n",
            "171 th batch - loss_value: 0.021653673\n",
            "172 th batch - loss_value: 0.063367195\n",
            "173 th batch - loss_value: 0.0048003043\n",
            "174 th batch - loss_value: 0.0009071634\n",
            "175 th batch - loss_value: 0.0052869963\n",
            "176 th batch - loss_value: 0.0050620055\n",
            "177 th batch - loss_value: 0.026944201\n",
            "178 th batch - loss_value: 0.05192031\n",
            "179 th batch - loss_value: 0.07504697\n",
            "# 180 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "180 th batch - loss_value: 0.003773335\n",
            "181 th batch - loss_value: 0.0038069533\n",
            "182 th batch - loss_value: 0.038689088\n",
            "183 th batch - loss_value: 0.020771002\n",
            "184 th batch - loss_value: 0.00064444245\n",
            "185 th batch - loss_value: 0.07660881\n",
            "186 th batch - loss_value: 0.016471144\n",
            "187 th batch - loss_value: 0.0037167943\n",
            "188 th batch - loss_value: 0.02419594\n",
            "189 th batch - loss_value: 0.012316553\n",
            "# 190 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "190 th batch - loss_value: 0.029613158\n",
            "191 th batch - loss_value: 0.003345745\n",
            "192 th batch - loss_value: 0.010160021\n",
            "193 th batch - loss_value: 0.0692318\n",
            "194 th batch - loss_value: 0.022534208\n",
            "195 th batch - loss_value: 0.0012402554\n",
            "196 th batch - loss_value: 0.035029355\n",
            "197 th batch - loss_value: 0.11988972\n",
            "198 th batch - loss_value: 0.0049955677\n",
            "199 th batch - loss_value: 0.015045329\n",
            "# 200 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "200 th batch - loss_value: 0.014897939\n",
            "201 th batch - loss_value: 0.038880195\n",
            "202 th batch - loss_value: 0.002651475\n",
            "203 th batch - loss_value: 0.09707681\n",
            "204 th batch - loss_value: 0.0715579\n",
            "205 th batch - loss_value: 0.040930975\n",
            "206 th batch - loss_value: 0.087403335\n",
            "207 th batch - loss_value: 0.034521285\n",
            "208 th batch - loss_value: 0.0027055803\n",
            "209 th batch - loss_value: 0.023346592\n",
            "# 210 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9466666666666667\n",
            "210 th batch - loss_value: 0.05126041\n",
            "211 th batch - loss_value: 0.036059253\n",
            "212 th batch - loss_value: 0.005048155\n",
            "213 th batch - loss_value: 0.024611631\n",
            "214 th batch - loss_value: 0.05008893\n",
            "215 th batch - loss_value: 0.116340004\n",
            "216 th batch - loss_value: 0.064787626\n",
            "217 th batch - loss_value: 0.00815839\n",
            "218 th batch - loss_value: 0.012477369\n",
            "219 th batch - loss_value: 0.060313486\n",
            "# 220 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "220 th batch - loss_value: 0.030688662\n",
            "221 th batch - loss_value: 0.027858146\n",
            "222 th batch - loss_value: 0.009310778\n",
            "223 th batch - loss_value: 0.0508976\n",
            "224 th batch - loss_value: 0.0066322577\n",
            "225 th batch - loss_value: 0.056490384\n",
            "226 th batch - loss_value: 0.039399125\n",
            "227 th batch - loss_value: 0.011393184\n",
            "228 th batch - loss_value: 0.02835637\n",
            "229 th batch - loss_value: 0.1019165\n",
            "# 230 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "230 th batch - loss_value: 0.00468944\n",
            "231 th batch - loss_value: 0.0023378795\n",
            "232 th batch - loss_value: 0.005646667\n",
            "233 th batch - loss_value: 0.049916007\n",
            "234 th batch - loss_value: 0.005448309\n",
            "235 th batch - loss_value: 0.008041949\n",
            "236 th batch - loss_value: 0.010375516\n",
            "237 th batch - loss_value: 0.0005834144\n",
            "238 th batch - loss_value: 0.025340434\n",
            "239 th batch - loss_value: 0.024311846\n",
            "# 240 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "240 th batch - loss_value: 0.04757546\n",
            "241 th batch - loss_value: 0.14047888\n",
            "242 th batch - loss_value: 0.03834143\n",
            "243 th batch - loss_value: 0.041770525\n",
            "244 th batch - loss_value: 0.026596546\n",
            "245 th batch - loss_value: 0.028915025\n",
            "246 th batch - loss_value: 0.049872108\n",
            "247 th batch - loss_value: 0.005639965\n",
            "248 th batch - loss_value: 0.2508797\n",
            "249 th batch - loss_value: 0.026392424\n",
            "# 250 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9333333333333333\n",
            "250 th batch - loss_value: 0.13657515\n",
            "251 th batch - loss_value: 0.034748886\n",
            "252 th batch - loss_value: 0.14813514\n",
            "253 th batch - loss_value: 0.18127874\n",
            "254 th batch - loss_value: 0.0072109797\n",
            "255 th batch - loss_value: 0.059414707\n",
            "256 th batch - loss_value: 0.050828513\n",
            "257 th batch - loss_value: 0.034953393\n",
            "258 th batch - loss_value: 0.008021404\n",
            "259 th batch - loss_value: 0.014948652\n",
            "# 260 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "260 th batch - loss_value: 0.041543085\n",
            "261 th batch - loss_value: 0.092298195\n",
            "262 th batch - loss_value: 0.017047267\n",
            "263 th batch - loss_value: 0.071391724\n",
            "264 th batch - loss_value: 0.066135235\n",
            "265 th batch - loss_value: 0.10774342\n",
            "266 th batch - loss_value: 0.07028269\n",
            "267 th batch - loss_value: 0.0019751622\n",
            "268 th batch - loss_value: 0.010026767\n",
            "269 th batch - loss_value: 0.003858479\n",
            "# 270 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "270 th batch - loss_value: 0.017752262\n",
            "271 th batch - loss_value: 0.006705077\n",
            "272 th batch - loss_value: 0.1543883\n",
            "273 th batch - loss_value: 0.028786384\n",
            "274 th batch - loss_value: 0.008784217\n",
            "275 th batch - loss_value: 0.06414732\n",
            "276 th batch - loss_value: 0.008982834\n",
            "277 th batch - loss_value: 0.057998043\n",
            "278 th batch - loss_value: 0.017317543\n",
            "279 th batch - loss_value: 0.050075304\n",
            "# 280 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "280 th batch - loss_value: 0.026702624\n",
            "281 th batch - loss_value: 0.002982261\n",
            "282 th batch - loss_value: 0.081530534\n",
            "283 th batch - loss_value: 0.0075598517\n",
            "284 th batch - loss_value: 0.08141779\n",
            "285 th batch - loss_value: 0.029948622\n",
            "286 th batch - loss_value: 0.03075481\n",
            "287 th batch - loss_value: 0.16374636\n",
            "288 th batch - loss_value: 0.019656518\n",
            "289 th batch - loss_value: 0.021749463\n",
            "# 290 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.94\n",
            "290 th batch - loss_value: 0.045509588\n",
            "291 th batch - loss_value: 0.02278258\n",
            "292 th batch - loss_value: 0.108652845\n",
            "293 th batch - loss_value: 0.005365705\n",
            "294 th batch - loss_value: 0.05333006\n",
            "295 th batch - loss_value: 0.003900234\n",
            "296 th batch - loss_value: 0.059298914\n",
            "297 th batch - loss_value: 0.002177119\n",
            "298 th batch - loss_value: 0.020644577\n",
            "299 th batch - loss_value: 0.061558492\n",
            "# 300 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "300 th batch - loss_value: 0.0016435899\n",
            "301 th batch - loss_value: 0.0022798888\n",
            "302 th batch - loss_value: 0.070077136\n",
            "303 th batch - loss_value: 0.093071766\n",
            "304 th batch - loss_value: 0.016964419\n",
            "305 th batch - loss_value: 0.029992582\n",
            "306 th batch - loss_value: 0.0906672\n",
            "307 th batch - loss_value: 0.012399566\n",
            "308 th batch - loss_value: 0.014338475\n",
            "309 th batch - loss_value: 0.005517891\n",
            "# 310 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "310 th batch - loss_value: 0.0701647\n",
            "311 th batch - loss_value: 0.003040255\n",
            "312 th batch - loss_value: 0.040778995\n",
            "313 th batch - loss_value: 0.001216261\n",
            "314 th batch - loss_value: 0.008650702\n",
            "315 th batch - loss_value: 0.007849587\n",
            "316 th batch - loss_value: 0.0036651331\n",
            "317 th batch - loss_value: 0.043397017\n",
            "318 th batch - loss_value: 0.08759045\n",
            "319 th batch - loss_value: 0.1122533\n",
            "# 320 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "320 th batch - loss_value: 0.028430754\n",
            "321 th batch - loss_value: 0.024289263\n",
            "322 th batch - loss_value: 0.007417719\n",
            "323 th batch - loss_value: 0.0073546385\n",
            "324 th batch - loss_value: 0.022693068\n",
            "325 th batch - loss_value: 0.0117256045\n",
            "326 th batch - loss_value: 0.01167017\n",
            "327 th batch - loss_value: 0.041372553\n",
            "328 th batch - loss_value: 0.007288541\n",
            "329 th batch - loss_value: 0.012803418\n",
            "# 330 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.88\n",
            "330 th batch - loss_value: 0.065028034\n",
            "331 th batch - loss_value: 0.012718609\n",
            "332 th batch - loss_value: 0.01571213\n",
            "333 th batch - loss_value: 0.0034030606\n",
            "334 th batch - loss_value: 0.002130618\n",
            "335 th batch - loss_value: 0.04729777\n",
            "336 th batch - loss_value: 0.020680271\n",
            "337 th batch - loss_value: 0.12022199\n",
            "338 th batch - loss_value: 0.007628009\n",
            "339 th batch - loss_value: 0.0010893355\n",
            "# 340 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "340 th batch - loss_value: 0.0045091\n",
            "341 th batch - loss_value: 0.008683811\n",
            "342 th batch - loss_value: 0.08430104\n",
            "343 th batch - loss_value: 0.0578182\n",
            "344 th batch - loss_value: 0.06778013\n",
            "345 th batch - loss_value: 0.10964051\n",
            "346 th batch - loss_value: 0.06059263\n",
            "347 th batch - loss_value: 0.12779352\n",
            "348 th batch - loss_value: 0.0088196965\n",
            "349 th batch - loss_value: 0.0021403984\n",
            "# 350 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "350 th batch - loss_value: 0.00583794\n",
            "351 th batch - loss_value: 0.12256028\n",
            "352 th batch - loss_value: 0.09143268\n",
            "353 th batch - loss_value: 0.041040137\n",
            "354 th batch - loss_value: 0.00444645\n",
            "355 th batch - loss_value: 0.017880678\n",
            "356 th batch - loss_value: 0.017629674\n",
            "357 th batch - loss_value: 0.15756364\n",
            "358 th batch - loss_value: 0.015153662\n",
            "359 th batch - loss_value: 0.015520888\n",
            "# 360 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9733333333333334\n",
            "360 th batch - loss_value: 0.05246377\n",
            "361 th batch - loss_value: 0.06654981\n",
            "362 th batch - loss_value: 0.0093230065\n",
            "363 th batch - loss_value: 0.00462111\n",
            "364 th batch - loss_value: 0.029278276\n",
            "365 th batch - loss_value: 0.0659607\n",
            "366 th batch - loss_value: 0.08391377\n",
            "367 th batch - loss_value: 0.083911784\n",
            "368 th batch - loss_value: 0.0030324159\n",
            "369 th batch - loss_value: 0.023387108\n",
            "# 370 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9333333333333333\n",
            "370 th batch - loss_value: 0.06881359\n",
            "371 th batch - loss_value: 0.022274207\n",
            "372 th batch - loss_value: 0.0080392845\n",
            "373 th batch - loss_value: 0.0054759104\n",
            "374 th batch - loss_value: 0.003878984\n",
            "375 th batch - loss_value: 0.013008362\n",
            "376 th batch - loss_value: 0.021425497\n",
            "377 th batch - loss_value: 0.07599418\n",
            "378 th batch - loss_value: 0.04968869\n",
            "379 th batch - loss_value: 0.0054793283\n",
            "# 380 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "380 th batch - loss_value: 0.034040913\n",
            "381 th batch - loss_value: 0.016967144\n",
            "382 th batch - loss_value: 0.007453313\n",
            "383 th batch - loss_value: 0.009291353\n",
            "384 th batch - loss_value: 0.030116495\n",
            "385 th batch - loss_value: 0.023628797\n",
            "386 th batch - loss_value: 0.036175568\n",
            "387 th batch - loss_value: 0.017904695\n",
            "388 th batch - loss_value: 0.041831728\n",
            "389 th batch - loss_value: 0.109956436\n",
            "# 390 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "390 th batch - loss_value: 0.0138317505\n",
            "391 th batch - loss_value: 0.022375353\n",
            "392 th batch - loss_value: 0.003354436\n",
            "393 th batch - loss_value: 0.009895193\n",
            "394 th batch - loss_value: 0.019418214\n",
            "395 th batch - loss_value: 0.0029306833\n",
            "396 th batch - loss_value: 0.035519127\n",
            "397 th batch - loss_value: 0.059391428\n",
            "398 th batch - loss_value: 0.111027285\n",
            "399 th batch - loss_value: 0.017170658\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "0 th batch - loss_value: 0.0041050194\n",
            "1 th batch - loss_value: 0.0029468122\n",
            "2 th batch - loss_value: 0.016732426\n",
            "3 th batch - loss_value: 0.008982763\n",
            "4 th batch - loss_value: 0.0033198053\n",
            "5 th batch - loss_value: 0.040988673\n",
            "6 th batch - loss_value: 0.020142548\n",
            "7 th batch - loss_value: 0.00094965444\n",
            "8 th batch - loss_value: 0.013106049\n",
            "9 th batch - loss_value: 0.0051009804\n",
            "# 10 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "10 th batch - loss_value: 0.0147602\n",
            "11 th batch - loss_value: 0.11602926\n",
            "12 th batch - loss_value: 0.014883686\n",
            "13 th batch - loss_value: 0.042902444\n",
            "14 th batch - loss_value: 0.004785609\n",
            "15 th batch - loss_value: 0.021228114\n",
            "16 th batch - loss_value: 0.005173631\n",
            "17 th batch - loss_value: 0.015233106\n",
            "18 th batch - loss_value: 0.005339578\n",
            "19 th batch - loss_value: 0.020414839\n",
            "# 20 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "20 th batch - loss_value: 0.03381256\n",
            "21 th batch - loss_value: 0.007027145\n",
            "22 th batch - loss_value: 0.0024700873\n",
            "23 th batch - loss_value: 0.013824357\n",
            "24 th batch - loss_value: 0.021221098\n",
            "25 th batch - loss_value: 0.0028629394\n",
            "26 th batch - loss_value: 0.044613857\n",
            "27 th batch - loss_value: 0.0031274285\n",
            "28 th batch - loss_value: 0.0071674404\n",
            "29 th batch - loss_value: 0.014164179\n",
            "# 30 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "30 th batch - loss_value: 0.002256832\n",
            "31 th batch - loss_value: 0.052704968\n",
            "32 th batch - loss_value: 0.13774845\n",
            "33 th batch - loss_value: 0.006261753\n",
            "34 th batch - loss_value: 0.0010567228\n",
            "35 th batch - loss_value: 0.0036703623\n",
            "36 th batch - loss_value: 0.078667164\n",
            "37 th batch - loss_value: 0.034322277\n",
            "38 th batch - loss_value: 0.25263122\n",
            "39 th batch - loss_value: 0.048985206\n",
            "# 40 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "40 th batch - loss_value: 0.0042764777\n",
            "41 th batch - loss_value: 0.001673004\n",
            "42 th batch - loss_value: 0.03831988\n",
            "43 th batch - loss_value: 0.007122095\n",
            "44 th batch - loss_value: 0.03217629\n",
            "45 th batch - loss_value: 0.0051135654\n",
            "46 th batch - loss_value: 0.052565686\n",
            "47 th batch - loss_value: 0.010799258\n",
            "48 th batch - loss_value: 0.0070518884\n",
            "49 th batch - loss_value: 0.028775007\n",
            "# 50 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "50 th batch - loss_value: 0.0014666433\n",
            "51 th batch - loss_value: 0.043694198\n",
            "52 th batch - loss_value: 0.012329883\n",
            "53 th batch - loss_value: 0.09904353\n",
            "54 th batch - loss_value: 0.009947194\n",
            "55 th batch - loss_value: 0.0063802847\n",
            "56 th batch - loss_value: 0.025812916\n",
            "57 th batch - loss_value: 0.010514137\n",
            "58 th batch - loss_value: 0.010252773\n",
            "59 th batch - loss_value: 0.026617033\n",
            "# 60 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9\n",
            "60 th batch - loss_value: 0.055179857\n",
            "61 th batch - loss_value: 0.027714618\n",
            "62 th batch - loss_value: 0.084677204\n",
            "63 th batch - loss_value: 0.011172094\n",
            "64 th batch - loss_value: 0.0018969942\n",
            "65 th batch - loss_value: 0.00814936\n",
            "66 th batch - loss_value: 0.027956529\n",
            "67 th batch - loss_value: 0.0043091225\n",
            "68 th batch - loss_value: 0.030361392\n",
            "69 th batch - loss_value: 0.006064519\n",
            "# 70 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "70 th batch - loss_value: 0.00293751\n",
            "71 th batch - loss_value: 0.0005021241\n",
            "72 th batch - loss_value: 0.0075836754\n",
            "73 th batch - loss_value: 0.04817967\n",
            "74 th batch - loss_value: 0.0008057834\n",
            "75 th batch - loss_value: 0.005842951\n",
            "76 th batch - loss_value: 0.032700352\n",
            "77 th batch - loss_value: 0.028957985\n",
            "78 th batch - loss_value: 0.11375616\n",
            "79 th batch - loss_value: 0.24153797\n",
            "# 80 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "80 th batch - loss_value: 0.0037677956\n",
            "81 th batch - loss_value: 0.0037356156\n",
            "82 th batch - loss_value: 0.028038055\n",
            "83 th batch - loss_value: 0.11989375\n",
            "84 th batch - loss_value: 0.00043360982\n",
            "85 th batch - loss_value: 0.06979177\n",
            "86 th batch - loss_value: 0.001477642\n",
            "87 th batch - loss_value: 0.003324416\n",
            "88 th batch - loss_value: 0.06249539\n",
            "89 th batch - loss_value: 0.04405826\n",
            "# 90 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.94\n",
            "90 th batch - loss_value: 0.038641848\n",
            "91 th batch - loss_value: 0.045359753\n",
            "92 th batch - loss_value: 0.062269118\n",
            "93 th batch - loss_value: 0.0054632914\n",
            "94 th batch - loss_value: 0.07583365\n",
            "95 th batch - loss_value: 0.0123792095\n",
            "96 th batch - loss_value: 0.104684874\n",
            "97 th batch - loss_value: 0.014263294\n",
            "98 th batch - loss_value: 0.039971534\n",
            "99 th batch - loss_value: 0.031996347\n",
            "# 100 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "100 th batch - loss_value: 0.04214609\n",
            "101 th batch - loss_value: 0.042556945\n",
            "102 th batch - loss_value: 0.031656627\n",
            "103 th batch - loss_value: 0.0354617\n",
            "104 th batch - loss_value: 0.0054324535\n",
            "105 th batch - loss_value: 0.05433935\n",
            "106 th batch - loss_value: 0.0469983\n",
            "107 th batch - loss_value: 0.05621296\n",
            "108 th batch - loss_value: 0.011399884\n",
            "109 th batch - loss_value: 0.08841219\n",
            "# 110 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "110 th batch - loss_value: 0.01649411\n",
            "111 th batch - loss_value: 0.23813501\n",
            "112 th batch - loss_value: 0.0487035\n",
            "113 th batch - loss_value: 0.0015637783\n",
            "114 th batch - loss_value: 0.039453458\n",
            "115 th batch - loss_value: 0.020741798\n",
            "116 th batch - loss_value: 0.15494369\n",
            "117 th batch - loss_value: 0.10105295\n",
            "118 th batch - loss_value: 0.11659961\n",
            "119 th batch - loss_value: 0.023550514\n",
            "# 120 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "120 th batch - loss_value: 0.016604828\n",
            "121 th batch - loss_value: 0.011180892\n",
            "122 th batch - loss_value: 0.020207727\n",
            "123 th batch - loss_value: 0.07049092\n",
            "124 th batch - loss_value: 0.03182666\n",
            "125 th batch - loss_value: 0.0033014233\n",
            "126 th batch - loss_value: 0.025658282\n",
            "127 th batch - loss_value: 0.0074129472\n",
            "128 th batch - loss_value: 0.013938387\n",
            "129 th batch - loss_value: 0.0076315408\n",
            "# 130 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "130 th batch - loss_value: 0.019152336\n",
            "131 th batch - loss_value: 0.025142409\n",
            "132 th batch - loss_value: 0.022321837\n",
            "133 th batch - loss_value: 0.14100985\n",
            "134 th batch - loss_value: 0.005329711\n",
            "135 th batch - loss_value: 0.045323614\n",
            "136 th batch - loss_value: 0.019056747\n",
            "137 th batch - loss_value: 0.032762237\n",
            "138 th batch - loss_value: 0.009205622\n",
            "139 th batch - loss_value: 0.019389275\n",
            "# 140 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "140 th batch - loss_value: 0.0032981155\n",
            "141 th batch - loss_value: 0.021793822\n",
            "142 th batch - loss_value: 0.0699347\n",
            "143 th batch - loss_value: 0.051382367\n",
            "144 th batch - loss_value: 0.009936657\n",
            "145 th batch - loss_value: 0.024020445\n",
            "146 th batch - loss_value: 0.009680862\n",
            "147 th batch - loss_value: 0.023332689\n",
            "148 th batch - loss_value: 0.004134233\n",
            "149 th batch - loss_value: 0.027571132\n",
            "# 150 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "150 th batch - loss_value: 0.0037690334\n",
            "151 th batch - loss_value: 0.011863167\n",
            "152 th batch - loss_value: 0.046981227\n",
            "153 th batch - loss_value: 0.008948028\n",
            "154 th batch - loss_value: 0.04557636\n",
            "155 th batch - loss_value: 0.003656184\n",
            "156 th batch - loss_value: 0.0015209214\n",
            "157 th batch - loss_value: 0.011171195\n",
            "158 th batch - loss_value: 0.02812764\n",
            "159 th batch - loss_value: 0.052737392\n",
            "# 160 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "160 th batch - loss_value: 0.0630205\n",
            "161 th batch - loss_value: 0.0059710806\n",
            "162 th batch - loss_value: 0.023796013\n",
            "163 th batch - loss_value: 0.011767174\n",
            "164 th batch - loss_value: 0.0015833002\n",
            "165 th batch - loss_value: 0.008285036\n",
            "166 th batch - loss_value: 0.016467012\n",
            "167 th batch - loss_value: 0.08644343\n",
            "168 th batch - loss_value: 0.023513528\n",
            "169 th batch - loss_value: 0.00829481\n",
            "# 170 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9266666666666666\n",
            "170 th batch - loss_value: 0.042597305\n",
            "171 th batch - loss_value: 0.0123335505\n",
            "172 th batch - loss_value: 0.0062378473\n",
            "173 th batch - loss_value: 0.0021934754\n",
            "174 th batch - loss_value: 0.004466998\n",
            "175 th batch - loss_value: 0.05099874\n",
            "176 th batch - loss_value: 0.0062508155\n",
            "177 th batch - loss_value: 0.027911736\n",
            "178 th batch - loss_value: 0.007544819\n",
            "179 th batch - loss_value: 0.011010094\n",
            "# 180 th batch - train_accuracy: 1.0 , test_accuracy: 0.9666666666666667\n",
            "180 th batch - loss_value: 0.01001072\n",
            "181 th batch - loss_value: 0.005433718\n",
            "182 th batch - loss_value: 0.006030026\n",
            "183 th batch - loss_value: 0.008448723\n",
            "184 th batch - loss_value: 0.01644793\n",
            "185 th batch - loss_value: 0.17596824\n",
            "186 th batch - loss_value: 0.027542578\n",
            "187 th batch - loss_value: 0.002058837\n",
            "188 th batch - loss_value: 0.008196391\n",
            "189 th batch - loss_value: 0.006970023\n",
            "# 190 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.94\n",
            "190 th batch - loss_value: 0.025352828\n",
            "191 th batch - loss_value: 0.0050005903\n",
            "192 th batch - loss_value: 0.01606142\n",
            "193 th batch - loss_value: 0.0055228933\n",
            "194 th batch - loss_value: 0.032411937\n",
            "195 th batch - loss_value: 0.011777808\n",
            "196 th batch - loss_value: 0.012157161\n",
            "197 th batch - loss_value: 0.005300829\n",
            "198 th batch - loss_value: 0.09847446\n",
            "199 th batch - loss_value: 0.04135402\n",
            "# 200 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "200 th batch - loss_value: 0.0018417467\n",
            "201 th batch - loss_value: 0.002641472\n",
            "202 th batch - loss_value: 0.0067529883\n",
            "203 th batch - loss_value: 0.019043263\n",
            "204 th batch - loss_value: 0.008732891\n",
            "205 th batch - loss_value: 0.028703142\n",
            "206 th batch - loss_value: 0.049975727\n",
            "207 th batch - loss_value: 0.0008857269\n",
            "208 th batch - loss_value: 0.023654308\n",
            "209 th batch - loss_value: 0.06263515\n",
            "# 210 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9533333333333334\n",
            "210 th batch - loss_value: 0.049480774\n",
            "211 th batch - loss_value: 0.055447675\n",
            "212 th batch - loss_value: 0.06953116\n",
            "213 th batch - loss_value: 0.10548528\n",
            "214 th batch - loss_value: 0.004162962\n",
            "215 th batch - loss_value: 0.0033293674\n",
            "216 th batch - loss_value: 0.096878156\n",
            "217 th batch - loss_value: 0.021543933\n",
            "218 th batch - loss_value: 0.004237322\n",
            "219 th batch - loss_value: 0.017062774\n",
            "# 220 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "220 th batch - loss_value: 0.02082147\n",
            "221 th batch - loss_value: 0.020740155\n",
            "222 th batch - loss_value: 0.01525068\n",
            "223 th batch - loss_value: 0.06534912\n",
            "224 th batch - loss_value: 0.06653572\n",
            "225 th batch - loss_value: 0.018907579\n",
            "226 th batch - loss_value: 0.025228143\n",
            "227 th batch - loss_value: 0.07078453\n",
            "228 th batch - loss_value: 0.014267148\n",
            "229 th batch - loss_value: 0.039418377\n",
            "# 230 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8866666666666667\n",
            "230 th batch - loss_value: 0.10160525\n",
            "231 th batch - loss_value: 0.009793459\n",
            "232 th batch - loss_value: 0.15022212\n",
            "233 th batch - loss_value: 0.031061618\n",
            "234 th batch - loss_value: 0.0036212392\n",
            "235 th batch - loss_value: 0.0063905944\n",
            "236 th batch - loss_value: 0.004522351\n",
            "237 th batch - loss_value: 0.0411341\n",
            "238 th batch - loss_value: 0.023513572\n",
            "239 th batch - loss_value: 0.008140629\n",
            "# 240 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "240 th batch - loss_value: 0.040526617\n",
            "241 th batch - loss_value: 0.019184269\n",
            "242 th batch - loss_value: 0.0003638862\n",
            "243 th batch - loss_value: 0.028483242\n",
            "244 th batch - loss_value: 0.016336653\n",
            "245 th batch - loss_value: 0.010524622\n",
            "246 th batch - loss_value: 0.009166685\n",
            "247 th batch - loss_value: 0.00071291544\n",
            "248 th batch - loss_value: 0.0041618813\n",
            "249 th batch - loss_value: 0.0028502159\n",
            "# 250 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "250 th batch - loss_value: 0.00084880687\n",
            "251 th batch - loss_value: 0.036940712\n",
            "252 th batch - loss_value: 0.06411035\n",
            "253 th batch - loss_value: 0.014329534\n",
            "254 th batch - loss_value: 0.0040925993\n",
            "255 th batch - loss_value: 0.020514227\n",
            "256 th batch - loss_value: 0.10183347\n",
            "257 th batch - loss_value: 0.00077864033\n",
            "258 th batch - loss_value: 0.08211026\n",
            "259 th batch - loss_value: 0.008483369\n",
            "# 260 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.9466666666666667\n",
            "260 th batch - loss_value: 0.20021224\n",
            "261 th batch - loss_value: 0.045133624\n",
            "262 th batch - loss_value: 0.04486778\n",
            "263 th batch - loss_value: 0.04138053\n",
            "264 th batch - loss_value: 0.021757375\n",
            "265 th batch - loss_value: 0.15343615\n",
            "266 th batch - loss_value: 0.025194373\n",
            "267 th batch - loss_value: 0.0507372\n",
            "268 th batch - loss_value: 0.010804213\n",
            "269 th batch - loss_value: 0.13416237\n",
            "# 270 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "270 th batch - loss_value: 0.0237079\n",
            "271 th batch - loss_value: 0.15556864\n",
            "272 th batch - loss_value: 0.010288128\n",
            "273 th batch - loss_value: 0.028733164\n",
            "274 th batch - loss_value: 0.095700294\n",
            "275 th batch - loss_value: 0.006490358\n",
            "276 th batch - loss_value: 0.020203501\n",
            "277 th batch - loss_value: 0.088561974\n",
            "278 th batch - loss_value: 0.025128076\n",
            "279 th batch - loss_value: 0.045606\n",
            "# 280 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "280 th batch - loss_value: 0.0069627226\n",
            "281 th batch - loss_value: 0.0063615325\n",
            "282 th batch - loss_value: 0.017147856\n",
            "283 th batch - loss_value: 0.020454872\n",
            "284 th batch - loss_value: 0.026596624\n",
            "285 th batch - loss_value: 0.043361187\n",
            "286 th batch - loss_value: 0.04468304\n",
            "287 th batch - loss_value: 0.07803252\n",
            "288 th batch - loss_value: 0.031115348\n",
            "289 th batch - loss_value: 0.014487308\n",
            "# 290 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "290 th batch - loss_value: 0.037688527\n",
            "291 th batch - loss_value: 0.21997537\n",
            "292 th batch - loss_value: 0.0014992042\n",
            "293 th batch - loss_value: 0.0036969972\n",
            "294 th batch - loss_value: 0.010695402\n",
            "295 th batch - loss_value: 0.032827143\n",
            "296 th batch - loss_value: 0.02215716\n",
            "297 th batch - loss_value: 0.06927788\n",
            "298 th batch - loss_value: 0.17008911\n",
            "299 th batch - loss_value: 0.07977062\n",
            "# 300 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "300 th batch - loss_value: 0.009507324\n",
            "301 th batch - loss_value: 0.032624047\n",
            "302 th batch - loss_value: 0.047959685\n",
            "303 th batch - loss_value: 0.0054526688\n",
            "304 th batch - loss_value: 0.009669279\n",
            "305 th batch - loss_value: 0.17383014\n",
            "306 th batch - loss_value: 0.17518257\n",
            "307 th batch - loss_value: 0.018591639\n",
            "308 th batch - loss_value: 0.14291324\n",
            "309 th batch - loss_value: 0.08951812\n",
            "# 310 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "310 th batch - loss_value: 0.040224086\n",
            "311 th batch - loss_value: 0.049441937\n",
            "312 th batch - loss_value: 0.044675406\n",
            "313 th batch - loss_value: 0.13623662\n",
            "314 th batch - loss_value: 0.062035084\n",
            "315 th batch - loss_value: 0.03836649\n",
            "316 th batch - loss_value: 0.04935235\n",
            "317 th batch - loss_value: 0.043428257\n",
            "318 th batch - loss_value: 0.02521104\n",
            "319 th batch - loss_value: 0.02665811\n",
            "# 320 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8733333333333333\n",
            "320 th batch - loss_value: 0.025692284\n",
            "321 th batch - loss_value: 0.062103413\n",
            "322 th batch - loss_value: 0.028725218\n",
            "323 th batch - loss_value: 0.01824472\n",
            "324 th batch - loss_value: 0.028802749\n",
            "325 th batch - loss_value: 0.12100445\n",
            "326 th batch - loss_value: 0.03073471\n",
            "327 th batch - loss_value: 0.06289443\n",
            "328 th batch - loss_value: 0.060191516\n",
            "329 th batch - loss_value: 0.021208351\n",
            "# 330 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "330 th batch - loss_value: 0.007944772\n",
            "331 th batch - loss_value: 0.046431314\n",
            "332 th batch - loss_value: 0.034658987\n",
            "333 th batch - loss_value: 0.0107180625\n",
            "334 th batch - loss_value: 0.021843793\n",
            "335 th batch - loss_value: 0.06004034\n",
            "336 th batch - loss_value: 0.0444592\n",
            "337 th batch - loss_value: 0.012661022\n",
            "338 th batch - loss_value: 0.00087063067\n",
            "339 th batch - loss_value: 0.003744517\n",
            "# 340 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "340 th batch - loss_value: 0.027070308\n",
            "341 th batch - loss_value: 0.033321716\n",
            "342 th batch - loss_value: 0.001541311\n",
            "343 th batch - loss_value: 0.075230144\n",
            "344 th batch - loss_value: 0.08097318\n",
            "345 th batch - loss_value: 0.009066238\n",
            "346 th batch - loss_value: 0.039542858\n",
            "347 th batch - loss_value: 0.0012927221\n",
            "348 th batch - loss_value: 0.00704953\n",
            "349 th batch - loss_value: 0.034916308\n",
            "# 350 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "350 th batch - loss_value: 0.016506871\n",
            "351 th batch - loss_value: 0.024344318\n",
            "352 th batch - loss_value: 0.011143802\n",
            "353 th batch - loss_value: 0.0025773307\n",
            "354 th batch - loss_value: 0.0013261101\n",
            "355 th batch - loss_value: 0.0063032336\n",
            "356 th batch - loss_value: 0.004790802\n",
            "357 th batch - loss_value: 0.006357529\n",
            "358 th batch - loss_value: 0.0136498045\n",
            "359 th batch - loss_value: 0.010318962\n",
            "# 360 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9466666666666667\n",
            "360 th batch - loss_value: 0.041503143\n",
            "361 th batch - loss_value: 0.02744888\n",
            "362 th batch - loss_value: 0.1020646\n",
            "363 th batch - loss_value: 0.0077740094\n",
            "364 th batch - loss_value: 0.006817272\n",
            "365 th batch - loss_value: 0.021775207\n",
            "366 th batch - loss_value: 0.032066274\n",
            "367 th batch - loss_value: 0.008577311\n",
            "368 th batch - loss_value: 0.02204811\n",
            "369 th batch - loss_value: 0.10162928\n",
            "# 370 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "370 th batch - loss_value: 0.03855736\n",
            "371 th batch - loss_value: 0.02012404\n",
            "372 th batch - loss_value: 0.0062433695\n",
            "373 th batch - loss_value: 0.002127993\n",
            "374 th batch - loss_value: 0.0003537022\n",
            "375 th batch - loss_value: 0.006386311\n",
            "376 th batch - loss_value: 0.01937895\n",
            "377 th batch - loss_value: 0.16227463\n",
            "378 th batch - loss_value: 0.02392862\n",
            "379 th batch - loss_value: 0.004678646\n",
            "# 380 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "380 th batch - loss_value: 0.0049650683\n",
            "381 th batch - loss_value: 0.004699438\n",
            "382 th batch - loss_value: 0.0023803192\n",
            "383 th batch - loss_value: 0.0032804175\n",
            "384 th batch - loss_value: 0.013271967\n",
            "385 th batch - loss_value: 0.006760678\n",
            "386 th batch - loss_value: 0.006414087\n",
            "387 th batch - loss_value: 0.0026276773\n",
            "388 th batch - loss_value: 0.038915686\n",
            "389 th batch - loss_value: 0.0014307917\n",
            "# 390 th batch - train_accuracy: 1.0 , test_accuracy: 0.9\n",
            "390 th batch - loss_value: 0.0019784134\n",
            "391 th batch - loss_value: 0.010558376\n",
            "392 th batch - loss_value: 0.003875461\n",
            "393 th batch - loss_value: 0.0018627486\n",
            "394 th batch - loss_value: 0.006752555\n",
            "395 th batch - loss_value: 0.05090557\n",
            "396 th batch - loss_value: 0.07560985\n",
            "397 th batch - loss_value: 0.032537717\n",
            "398 th batch - loss_value: 0.0038725366\n",
            "399 th batch - loss_value: 0.09109628\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 1.0 , test_accuracy: 0.9\n",
            "0 th batch - loss_value: 0.0072848373\n",
            "1 th batch - loss_value: 0.006814361\n",
            "2 th batch - loss_value: 0.0022600277\n",
            "3 th batch - loss_value: 0.031216882\n",
            "4 th batch - loss_value: 0.038132273\n",
            "5 th batch - loss_value: 0.017199235\n",
            "6 th batch - loss_value: 0.0032601901\n",
            "7 th batch - loss_value: 0.060812887\n",
            "8 th batch - loss_value: 0.0013524375\n",
            "9 th batch - loss_value: 0.011855498\n",
            "# 10 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9466666666666667\n",
            "10 th batch - loss_value: 0.039056495\n",
            "11 th batch - loss_value: 0.056526065\n",
            "12 th batch - loss_value: 0.06284115\n",
            "13 th batch - loss_value: 0.0007776159\n",
            "14 th batch - loss_value: 0.00096567295\n",
            "15 th batch - loss_value: 0.03193774\n",
            "16 th batch - loss_value: 0.009662637\n",
            "17 th batch - loss_value: 0.061616022\n",
            "18 th batch - loss_value: 0.0329145\n",
            "19 th batch - loss_value: 0.10278663\n",
            "# 20 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "20 th batch - loss_value: 0.00639384\n",
            "21 th batch - loss_value: 0.0051555885\n",
            "22 th batch - loss_value: 0.032647546\n",
            "23 th batch - loss_value: 0.011479149\n",
            "24 th batch - loss_value: 0.0070222826\n",
            "25 th batch - loss_value: 0.014897808\n",
            "26 th batch - loss_value: 0.019563992\n",
            "27 th batch - loss_value: 0.014373243\n",
            "28 th batch - loss_value: 0.02652106\n",
            "29 th batch - loss_value: 0.01526675\n",
            "# 30 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "30 th batch - loss_value: 0.0018531125\n",
            "31 th batch - loss_value: 0.0022163624\n",
            "32 th batch - loss_value: 0.15271586\n",
            "33 th batch - loss_value: 0.01652154\n",
            "34 th batch - loss_value: 0.0040631774\n",
            "35 th batch - loss_value: 0.0034317032\n",
            "36 th batch - loss_value: 0.027555943\n",
            "37 th batch - loss_value: 0.044244595\n",
            "38 th batch - loss_value: 0.009512621\n",
            "39 th batch - loss_value: 0.0013120859\n",
            "# 40 th batch - train_accuracy: 1.0 , test_accuracy: 0.9266666666666666\n",
            "40 th batch - loss_value: 0.0024972002\n",
            "41 th batch - loss_value: 0.04821081\n",
            "42 th batch - loss_value: 0.0029627818\n",
            "43 th batch - loss_value: 0.00532621\n",
            "44 th batch - loss_value: 0.003949919\n",
            "45 th batch - loss_value: 0.01252331\n",
            "46 th batch - loss_value: 0.06918156\n",
            "47 th batch - loss_value: 0.008533935\n",
            "48 th batch - loss_value: 0.006402217\n",
            "49 th batch - loss_value: 0.010534926\n",
            "# 50 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "50 th batch - loss_value: 0.009566052\n",
            "51 th batch - loss_value: 0.000467775\n",
            "52 th batch - loss_value: 0.03617912\n",
            "53 th batch - loss_value: 0.08761893\n",
            "54 th batch - loss_value: 0.033570375\n",
            "55 th batch - loss_value: 0.018642073\n",
            "56 th batch - loss_value: 0.004711002\n",
            "57 th batch - loss_value: 0.012406025\n",
            "58 th batch - loss_value: 0.07889015\n",
            "59 th batch - loss_value: 0.011302476\n",
            "# 60 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "60 th batch - loss_value: 0.005779317\n",
            "61 th batch - loss_value: 0.043744072\n",
            "62 th batch - loss_value: 0.0034516065\n",
            "63 th batch - loss_value: 0.01190185\n",
            "64 th batch - loss_value: 0.03500222\n",
            "65 th batch - loss_value: 0.0021270209\n",
            "66 th batch - loss_value: 0.051454548\n",
            "67 th batch - loss_value: 0.012473499\n",
            "68 th batch - loss_value: 0.0017323431\n",
            "69 th batch - loss_value: 0.046388313\n",
            "# 70 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "70 th batch - loss_value: 0.0035950735\n",
            "71 th batch - loss_value: 0.03594065\n",
            "72 th batch - loss_value: 0.009743381\n",
            "73 th batch - loss_value: 0.11855073\n",
            "74 th batch - loss_value: 0.0015055494\n",
            "75 th batch - loss_value: 0.061094005\n",
            "76 th batch - loss_value: 0.19561073\n",
            "77 th batch - loss_value: 0.003741341\n",
            "78 th batch - loss_value: 0.07844945\n",
            "79 th batch - loss_value: 0.013137247\n",
            "# 80 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "80 th batch - loss_value: 0.0043787286\n",
            "81 th batch - loss_value: 0.0156009365\n",
            "82 th batch - loss_value: 0.12729463\n",
            "83 th batch - loss_value: 0.033193488\n",
            "84 th batch - loss_value: 0.15042502\n",
            "85 th batch - loss_value: 0.010329099\n",
            "86 th batch - loss_value: 0.0054695588\n",
            "87 th batch - loss_value: 0.0077623944\n",
            "88 th batch - loss_value: 0.11212405\n",
            "89 th batch - loss_value: 0.022743998\n",
            "# 90 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "90 th batch - loss_value: 0.047632467\n",
            "91 th batch - loss_value: 0.0029140466\n",
            "92 th batch - loss_value: 0.12355967\n",
            "93 th batch - loss_value: 0.048278645\n",
            "94 th batch - loss_value: 0.020068375\n",
            "95 th batch - loss_value: 0.024477901\n",
            "96 th batch - loss_value: 0.03509832\n",
            "97 th batch - loss_value: 0.041358408\n",
            "98 th batch - loss_value: 0.03531238\n",
            "99 th batch - loss_value: 0.019797819\n",
            "# 100 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9466666666666667\n",
            "100 th batch - loss_value: 0.07676587\n",
            "101 th batch - loss_value: 0.020370958\n",
            "102 th batch - loss_value: 0.01001208\n",
            "103 th batch - loss_value: 0.012827834\n",
            "104 th batch - loss_value: 0.031913888\n",
            "105 th batch - loss_value: 0.060062133\n",
            "106 th batch - loss_value: 0.018616468\n",
            "107 th batch - loss_value: 0.006398174\n",
            "108 th batch - loss_value: 0.039337076\n",
            "109 th batch - loss_value: 0.01660374\n",
            "# 110 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "110 th batch - loss_value: 0.041995052\n",
            "111 th batch - loss_value: 0.009589125\n",
            "112 th batch - loss_value: 0.018694136\n",
            "113 th batch - loss_value: 0.02436886\n",
            "114 th batch - loss_value: 0.048753586\n",
            "115 th batch - loss_value: 0.01289922\n",
            "116 th batch - loss_value: 0.0022498737\n",
            "117 th batch - loss_value: 0.09165365\n",
            "118 th batch - loss_value: 0.12092229\n",
            "119 th batch - loss_value: 0.0113090705\n",
            "# 120 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.94\n",
            "120 th batch - loss_value: 0.022117544\n",
            "121 th batch - loss_value: 0.005688395\n",
            "122 th batch - loss_value: 0.016690968\n",
            "123 th batch - loss_value: 0.071093135\n",
            "124 th batch - loss_value: 0.07629583\n",
            "125 th batch - loss_value: 0.09351231\n",
            "126 th batch - loss_value: 0.029524213\n",
            "127 th batch - loss_value: 0.018331744\n",
            "128 th batch - loss_value: 0.005444065\n",
            "129 th batch - loss_value: 0.14118263\n",
            "# 130 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9466666666666667\n",
            "130 th batch - loss_value: 0.11962331\n",
            "131 th batch - loss_value: 0.012596909\n",
            "132 th batch - loss_value: 0.04708954\n",
            "133 th batch - loss_value: 0.011668564\n",
            "134 th batch - loss_value: 0.087455526\n",
            "135 th batch - loss_value: 0.01478164\n",
            "136 th batch - loss_value: 0.023830933\n",
            "137 th batch - loss_value: 0.02800727\n",
            "138 th batch - loss_value: 0.022014024\n",
            "139 th batch - loss_value: 0.0022706944\n",
            "# 140 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "140 th batch - loss_value: 0.005285173\n",
            "141 th batch - loss_value: 0.04146024\n",
            "142 th batch - loss_value: 0.029865328\n",
            "143 th batch - loss_value: 0.0046565575\n",
            "144 th batch - loss_value: 0.0035851554\n",
            "145 th batch - loss_value: 0.043742742\n",
            "146 th batch - loss_value: 0.027748415\n",
            "147 th batch - loss_value: 0.033242125\n",
            "148 th batch - loss_value: 0.003964806\n",
            "149 th batch - loss_value: 0.016991837\n",
            "# 150 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "150 th batch - loss_value: 0.0058044563\n",
            "151 th batch - loss_value: 0.17758843\n",
            "152 th batch - loss_value: 0.004278704\n",
            "153 th batch - loss_value: 0.012974987\n",
            "154 th batch - loss_value: 0.020188041\n",
            "155 th batch - loss_value: 0.0050927503\n",
            "156 th batch - loss_value: 0.015730374\n",
            "157 th batch - loss_value: 0.038983557\n",
            "158 th batch - loss_value: 0.0083758375\n",
            "159 th batch - loss_value: 0.0018936082\n",
            "# 160 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "160 th batch - loss_value: 0.05122497\n",
            "161 th batch - loss_value: 0.06369884\n",
            "162 th batch - loss_value: 0.0037292598\n",
            "163 th batch - loss_value: 0.007572341\n",
            "164 th batch - loss_value: 0.0027116213\n",
            "165 th batch - loss_value: 0.009223117\n",
            "166 th batch - loss_value: 0.2195216\n",
            "167 th batch - loss_value: 0.08555464\n",
            "168 th batch - loss_value: 0.01541741\n",
            "169 th batch - loss_value: 0.120310105\n",
            "# 170 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.96\n",
            "170 th batch - loss_value: 0.040463183\n",
            "171 th batch - loss_value: 0.0077912016\n",
            "172 th batch - loss_value: 0.0921653\n",
            "173 th batch - loss_value: 0.051725376\n",
            "174 th batch - loss_value: 0.0011356123\n",
            "175 th batch - loss_value: 0.055208076\n",
            "176 th batch - loss_value: 0.012313883\n",
            "177 th batch - loss_value: 0.027367331\n",
            "178 th batch - loss_value: 0.00081029383\n",
            "179 th batch - loss_value: 0.009698575\n",
            "# 180 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "180 th batch - loss_value: 0.015068848\n",
            "181 th batch - loss_value: 0.0032608754\n",
            "182 th batch - loss_value: 0.05722858\n",
            "183 th batch - loss_value: 0.0070055476\n",
            "184 th batch - loss_value: 0.014794801\n",
            "185 th batch - loss_value: 0.015877936\n",
            "186 th batch - loss_value: 0.0027089342\n",
            "187 th batch - loss_value: 0.0044414815\n",
            "188 th batch - loss_value: 0.030786596\n",
            "189 th batch - loss_value: 0.038438577\n",
            "# 190 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "190 th batch - loss_value: 0.07355177\n",
            "191 th batch - loss_value: 0.15216552\n",
            "192 th batch - loss_value: 0.012355597\n",
            "193 th batch - loss_value: 0.013926113\n",
            "194 th batch - loss_value: 0.009091093\n",
            "195 th batch - loss_value: 0.10380941\n",
            "196 th batch - loss_value: 0.045407243\n",
            "197 th batch - loss_value: 0.19173138\n",
            "198 th batch - loss_value: 0.18869378\n",
            "199 th batch - loss_value: 0.039477207\n",
            "# 200 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "200 th batch - loss_value: 0.002412294\n",
            "201 th batch - loss_value: 0.111140564\n",
            "202 th batch - loss_value: 0.04150801\n",
            "203 th batch - loss_value: 0.03774483\n",
            "204 th batch - loss_value: 0.024603905\n",
            "205 th batch - loss_value: 0.037564453\n",
            "206 th batch - loss_value: 0.044414427\n",
            "207 th batch - loss_value: 0.0010347158\n",
            "208 th batch - loss_value: 0.014002909\n",
            "209 th batch - loss_value: 0.0347926\n",
            "# 210 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "210 th batch - loss_value: 0.007465689\n",
            "211 th batch - loss_value: 0.046894927\n",
            "212 th batch - loss_value: 0.0346551\n",
            "213 th batch - loss_value: 0.13569166\n",
            "214 th batch - loss_value: 0.031363595\n",
            "215 th batch - loss_value: 0.068865225\n",
            "216 th batch - loss_value: 0.03190603\n",
            "217 th batch - loss_value: 0.039746244\n",
            "218 th batch - loss_value: 0.0026029053\n",
            "219 th batch - loss_value: 0.117404774\n",
            "# 220 th batch - train_accuracy: 1.0 , test_accuracy: 0.8866666666666667\n",
            "220 th batch - loss_value: 0.006071883\n",
            "221 th batch - loss_value: 0.079786144\n",
            "222 th batch - loss_value: 0.0029365919\n",
            "223 th batch - loss_value: 0.060428087\n",
            "224 th batch - loss_value: 0.010209367\n",
            "225 th batch - loss_value: 0.035685822\n",
            "226 th batch - loss_value: 0.0022281795\n",
            "227 th batch - loss_value: 0.024180286\n",
            "228 th batch - loss_value: 0.029018622\n",
            "229 th batch - loss_value: 0.0028871745\n",
            "# 230 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9133333333333333\n",
            "230 th batch - loss_value: 0.10227377\n",
            "231 th batch - loss_value: 0.08379983\n",
            "232 th batch - loss_value: 0.036785584\n",
            "233 th batch - loss_value: 0.011598771\n",
            "234 th batch - loss_value: 0.01126354\n",
            "235 th batch - loss_value: 0.007960978\n",
            "236 th batch - loss_value: 0.057894025\n",
            "237 th batch - loss_value: 0.13951804\n",
            "238 th batch - loss_value: 0.008808906\n",
            "239 th batch - loss_value: 0.0040836562\n",
            "# 240 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "240 th batch - loss_value: 0.0111045\n",
            "241 th batch - loss_value: 0.0172568\n",
            "242 th batch - loss_value: 0.007271576\n",
            "243 th batch - loss_value: 0.050175246\n",
            "244 th batch - loss_value: 0.016934428\n",
            "245 th batch - loss_value: 0.060101226\n",
            "246 th batch - loss_value: 0.017868763\n",
            "247 th batch - loss_value: 0.0077524283\n",
            "248 th batch - loss_value: 0.025478909\n",
            "249 th batch - loss_value: 0.023698103\n",
            "# 250 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "250 th batch - loss_value: 0.0596325\n",
            "251 th batch - loss_value: 0.029811766\n",
            "252 th batch - loss_value: 0.070198424\n",
            "253 th batch - loss_value: 0.15328713\n",
            "254 th batch - loss_value: 0.02945446\n",
            "255 th batch - loss_value: 0.04433434\n",
            "256 th batch - loss_value: 0.009200493\n",
            "257 th batch - loss_value: 0.005314404\n",
            "258 th batch - loss_value: 0.031187778\n",
            "259 th batch - loss_value: 0.057489786\n",
            "# 260 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9466666666666667\n",
            "260 th batch - loss_value: 0.10666045\n",
            "261 th batch - loss_value: 0.09466121\n",
            "262 th batch - loss_value: 0.028734121\n",
            "263 th batch - loss_value: 0.014099142\n",
            "264 th batch - loss_value: 0.0033904642\n",
            "265 th batch - loss_value: 0.047387827\n",
            "266 th batch - loss_value: 0.0400655\n",
            "267 th batch - loss_value: 0.0064936876\n",
            "268 th batch - loss_value: 0.08450777\n",
            "269 th batch - loss_value: 0.01266074\n",
            "# 270 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "270 th batch - loss_value: 0.006971771\n",
            "271 th batch - loss_value: 0.040432505\n",
            "272 th batch - loss_value: 0.010992421\n",
            "273 th batch - loss_value: 0.07744753\n",
            "274 th batch - loss_value: 0.0095158005\n",
            "275 th batch - loss_value: 0.040582243\n",
            "276 th batch - loss_value: 0.008462926\n",
            "277 th batch - loss_value: 0.012760967\n",
            "278 th batch - loss_value: 0.029693034\n",
            "279 th batch - loss_value: 0.054040395\n",
            "# 280 th batch - train_accuracy: 1.0 , test_accuracy: 0.88\n",
            "280 th batch - loss_value: 0.02205206\n",
            "281 th batch - loss_value: 0.1458365\n",
            "282 th batch - loss_value: 0.011972761\n",
            "283 th batch - loss_value: 0.022656668\n",
            "284 th batch - loss_value: 0.024144763\n",
            "285 th batch - loss_value: 0.033778917\n",
            "286 th batch - loss_value: 0.04403737\n",
            "287 th batch - loss_value: 0.0035530196\n",
            "288 th batch - loss_value: 0.023020092\n",
            "289 th batch - loss_value: 0.008187198\n",
            "# 290 th batch - train_accuracy: 0.95 , test_accuracy: 0.9333333333333333\n",
            "290 th batch - loss_value: 0.07243137\n",
            "291 th batch - loss_value: 0.024532394\n",
            "292 th batch - loss_value: 0.0039326334\n",
            "293 th batch - loss_value: 0.0022715484\n",
            "294 th batch - loss_value: 0.01713386\n",
            "295 th batch - loss_value: 0.00982349\n",
            "296 th batch - loss_value: 0.02740767\n",
            "297 th batch - loss_value: 0.01641253\n",
            "298 th batch - loss_value: 0.01905313\n",
            "299 th batch - loss_value: 0.0020856315\n",
            "# 300 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9466666666666667\n",
            "300 th batch - loss_value: 0.03472585\n",
            "301 th batch - loss_value: 0.060360875\n",
            "302 th batch - loss_value: 0.031151166\n",
            "303 th batch - loss_value: 0.0036035841\n",
            "304 th batch - loss_value: 0.038238395\n",
            "305 th batch - loss_value: 0.0030411268\n",
            "306 th batch - loss_value: 0.118723996\n",
            "307 th batch - loss_value: 0.0037479266\n",
            "308 th batch - loss_value: 0.0060530063\n",
            "309 th batch - loss_value: 0.021696763\n",
            "# 310 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "310 th batch - loss_value: 0.023262711\n",
            "311 th batch - loss_value: 0.0037986734\n",
            "312 th batch - loss_value: 0.034940913\n",
            "313 th batch - loss_value: 0.0088850595\n",
            "314 th batch - loss_value: 0.014982481\n",
            "315 th batch - loss_value: 0.022668628\n",
            "316 th batch - loss_value: 0.043899726\n",
            "317 th batch - loss_value: 0.0009189897\n",
            "318 th batch - loss_value: 0.0006264712\n",
            "319 th batch - loss_value: 0.007224347\n",
            "# 320 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "320 th batch - loss_value: 0.004796667\n",
            "321 th batch - loss_value: 0.00943946\n",
            "322 th batch - loss_value: 0.0062249056\n",
            "323 th batch - loss_value: 0.0045455536\n",
            "324 th batch - loss_value: 0.06556095\n",
            "325 th batch - loss_value: 0.005224086\n",
            "326 th batch - loss_value: 0.015066403\n",
            "327 th batch - loss_value: 0.04104119\n",
            "328 th batch - loss_value: 0.0044752634\n",
            "329 th batch - loss_value: 0.064707614\n",
            "# 330 th batch - train_accuracy: 1.0 , test_accuracy: 0.9\n",
            "330 th batch - loss_value: 0.0032443588\n",
            "331 th batch - loss_value: 0.0058715795\n",
            "332 th batch - loss_value: 0.05338407\n",
            "333 th batch - loss_value: 0.010755689\n",
            "334 th batch - loss_value: 0.0005842071\n",
            "335 th batch - loss_value: 0.005800603\n",
            "336 th batch - loss_value: 0.023693798\n",
            "337 th batch - loss_value: 0.005497447\n",
            "338 th batch - loss_value: 0.004400603\n",
            "339 th batch - loss_value: 0.0495047\n",
            "# 340 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "340 th batch - loss_value: 0.00048970047\n",
            "341 th batch - loss_value: 0.0015508704\n",
            "342 th batch - loss_value: 0.01567211\n",
            "343 th batch - loss_value: 0.003384904\n",
            "344 th batch - loss_value: 0.009345305\n",
            "345 th batch - loss_value: 0.006631685\n",
            "346 th batch - loss_value: 0.0036914211\n",
            "347 th batch - loss_value: 0.0032980933\n",
            "348 th batch - loss_value: 0.0027757601\n",
            "349 th batch - loss_value: 0.0044311136\n",
            "# 350 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "350 th batch - loss_value: 0.025478892\n",
            "351 th batch - loss_value: 0.00078638166\n",
            "352 th batch - loss_value: 0.0029304598\n",
            "353 th batch - loss_value: 0.0010533152\n",
            "354 th batch - loss_value: 0.0016917035\n",
            "355 th batch - loss_value: 0.020991173\n",
            "356 th batch - loss_value: 0.018294748\n",
            "357 th batch - loss_value: 0.0044912803\n",
            "358 th batch - loss_value: 0.017565709\n",
            "359 th batch - loss_value: 0.11925183\n",
            "# 360 th batch - train_accuracy: 1.0 , test_accuracy: 0.9266666666666666\n",
            "360 th batch - loss_value: 0.012729228\n",
            "361 th batch - loss_value: 0.0014020636\n",
            "362 th batch - loss_value: 0.0029952289\n",
            "363 th batch - loss_value: 0.015061961\n",
            "364 th batch - loss_value: 0.026215127\n",
            "365 th batch - loss_value: 0.14928614\n",
            "366 th batch - loss_value: 0.18638249\n",
            "367 th batch - loss_value: 0.035528447\n",
            "368 th batch - loss_value: 0.049660094\n",
            "369 th batch - loss_value: 0.01416438\n",
            "# 370 th batch - train_accuracy: 0.95 , test_accuracy: 0.8866666666666667\n",
            "370 th batch - loss_value: 0.10165618\n",
            "371 th batch - loss_value: 0.0018860559\n",
            "372 th batch - loss_value: 0.030631276\n",
            "373 th batch - loss_value: 0.25889465\n",
            "374 th batch - loss_value: 0.045326337\n",
            "375 th batch - loss_value: 0.06447158\n",
            "376 th batch - loss_value: 0.10029693\n",
            "377 th batch - loss_value: 0.035343364\n",
            "378 th batch - loss_value: 0.009366633\n",
            "379 th batch - loss_value: 0.001358436\n",
            "# 380 th batch - train_accuracy: 1.0 , test_accuracy: 0.9266666666666666\n",
            "380 th batch - loss_value: 0.010816628\n",
            "381 th batch - loss_value: 0.071133405\n",
            "382 th batch - loss_value: 0.016899284\n",
            "383 th batch - loss_value: 0.045529358\n",
            "384 th batch - loss_value: 0.0068150335\n",
            "385 th batch - loss_value: 0.047306124\n",
            "386 th batch - loss_value: 0.056310847\n",
            "387 th batch - loss_value: 0.018911056\n",
            "388 th batch - loss_value: 0.0055655143\n",
            "389 th batch - loss_value: 0.031190842\n",
            "# 390 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "390 th batch - loss_value: 0.0012711544\n",
            "391 th batch - loss_value: 0.031931385\n",
            "392 th batch - loss_value: 0.0029609352\n",
            "393 th batch - loss_value: 0.0011581768\n",
            "394 th batch - loss_value: 0.009619105\n",
            "395 th batch - loss_value: 0.016639743\n",
            "396 th batch - loss_value: 0.0263923\n",
            "397 th batch - loss_value: 0.0084863035\n",
            "398 th batch - loss_value: 0.016135184\n",
            "399 th batch - loss_value: 0.016228493\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 0.95 , test_accuracy: 0.9066666666666666\n",
            "0 th batch - loss_value: 0.12757169\n",
            "1 th batch - loss_value: 0.040358\n",
            "2 th batch - loss_value: 0.03690428\n",
            "3 th batch - loss_value: 0.02598796\n",
            "4 th batch - loss_value: 0.019783711\n",
            "5 th batch - loss_value: 0.0027836887\n",
            "6 th batch - loss_value: 0.047468677\n",
            "7 th batch - loss_value: 0.049783424\n",
            "8 th batch - loss_value: 0.034393538\n",
            "9 th batch - loss_value: 0.06318502\n",
            "# 10 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9333333333333333\n",
            "10 th batch - loss_value: 0.13514857\n",
            "11 th batch - loss_value: 0.091379814\n",
            "12 th batch - loss_value: 0.0126598235\n",
            "13 th batch - loss_value: 0.004707816\n",
            "14 th batch - loss_value: 0.062269174\n",
            "15 th batch - loss_value: 0.07484673\n",
            "16 th batch - loss_value: 0.014864052\n",
            "17 th batch - loss_value: 0.018790195\n",
            "18 th batch - loss_value: 0.009205318\n",
            "19 th batch - loss_value: 0.046722732\n",
            "# 20 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9\n",
            "20 th batch - loss_value: 0.082410984\n",
            "21 th batch - loss_value: 0.007437577\n",
            "22 th batch - loss_value: 0.04338588\n",
            "23 th batch - loss_value: 0.014587213\n",
            "24 th batch - loss_value: 0.04757112\n",
            "25 th batch - loss_value: 0.0040555457\n",
            "26 th batch - loss_value: 0.02279562\n",
            "27 th batch - loss_value: 0.005832438\n",
            "28 th batch - loss_value: 0.0007351175\n",
            "29 th batch - loss_value: 0.0677729\n",
            "# 30 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9333333333333333\n",
            "30 th batch - loss_value: 0.09987807\n",
            "31 th batch - loss_value: 0.048148032\n",
            "32 th batch - loss_value: 0.16932902\n",
            "33 th batch - loss_value: 0.008968353\n",
            "34 th batch - loss_value: 0.034702323\n",
            "35 th batch - loss_value: 0.02899711\n",
            "36 th batch - loss_value: 0.038522307\n",
            "37 th batch - loss_value: 0.014922675\n",
            "38 th batch - loss_value: 0.0038140663\n",
            "39 th batch - loss_value: 0.01025021\n",
            "# 40 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "40 th batch - loss_value: 0.013801849\n",
            "41 th batch - loss_value: 0.009627676\n",
            "42 th batch - loss_value: 0.02750764\n",
            "43 th batch - loss_value: 0.029920552\n",
            "44 th batch - loss_value: 0.00044818234\n",
            "45 th batch - loss_value: 0.04598192\n",
            "46 th batch - loss_value: 0.0035544548\n",
            "47 th batch - loss_value: 0.036961745\n",
            "48 th batch - loss_value: 0.062304594\n",
            "49 th batch - loss_value: 0.0053073927\n",
            "# 50 th batch - train_accuracy: 1.0 , test_accuracy: 0.9266666666666666\n",
            "50 th batch - loss_value: 0.0036908002\n",
            "51 th batch - loss_value: 0.005486789\n",
            "52 th batch - loss_value: 0.08039466\n",
            "53 th batch - loss_value: 0.03555556\n",
            "54 th batch - loss_value: 0.013865171\n",
            "55 th batch - loss_value: 0.015381566\n",
            "56 th batch - loss_value: 0.005360236\n",
            "57 th batch - loss_value: 0.08385269\n",
            "58 th batch - loss_value: 0.00026788923\n",
            "59 th batch - loss_value: 0.008867818\n",
            "# 60 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9266666666666666\n",
            "60 th batch - loss_value: 0.06824705\n",
            "61 th batch - loss_value: 0.002837361\n",
            "62 th batch - loss_value: 0.016603908\n",
            "63 th batch - loss_value: 0.033459377\n",
            "64 th batch - loss_value: 0.0037295367\n",
            "65 th batch - loss_value: 0.0057457373\n",
            "66 th batch - loss_value: 0.0039582057\n",
            "67 th batch - loss_value: 0.04305893\n",
            "68 th batch - loss_value: 0.072753415\n",
            "69 th batch - loss_value: 0.0029433756\n",
            "# 70 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "70 th batch - loss_value: 0.0063577816\n",
            "71 th batch - loss_value: 0.012974638\n",
            "72 th batch - loss_value: 0.022386191\n",
            "73 th batch - loss_value: 0.007039189\n",
            "74 th batch - loss_value: 0.003087701\n",
            "75 th batch - loss_value: 0.009025039\n",
            "76 th batch - loss_value: 0.0078398045\n",
            "77 th batch - loss_value: 0.0034596694\n",
            "78 th batch - loss_value: 0.043679606\n",
            "79 th batch - loss_value: 0.0035568376\n",
            "# 80 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.94\n",
            "80 th batch - loss_value: 0.07389409\n",
            "81 th batch - loss_value: 0.15752885\n",
            "82 th batch - loss_value: 0.051530425\n",
            "83 th batch - loss_value: 0.009788413\n",
            "84 th batch - loss_value: 0.038625892\n",
            "85 th batch - loss_value: 0.044585545\n",
            "86 th batch - loss_value: 0.044918608\n",
            "87 th batch - loss_value: 0.05260623\n",
            "88 th batch - loss_value: 0.010051433\n",
            "89 th batch - loss_value: 0.005885219\n",
            "# 90 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "90 th batch - loss_value: 0.09785982\n",
            "91 th batch - loss_value: 0.002728694\n",
            "92 th batch - loss_value: 0.00080466014\n",
            "93 th batch - loss_value: 0.113493405\n",
            "94 th batch - loss_value: 0.04752539\n",
            "95 th batch - loss_value: 0.09951024\n",
            "96 th batch - loss_value: 0.0107468655\n",
            "97 th batch - loss_value: 0.0051012626\n",
            "98 th batch - loss_value: 0.060991038\n",
            "99 th batch - loss_value: 0.02960614\n",
            "# 100 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "100 th batch - loss_value: 0.016219469\n",
            "101 th batch - loss_value: 0.0023062446\n",
            "102 th batch - loss_value: 0.0033189564\n",
            "103 th batch - loss_value: 0.0019903153\n",
            "104 th batch - loss_value: 0.004350701\n",
            "105 th batch - loss_value: 0.0042433357\n",
            "106 th batch - loss_value: 0.003943027\n",
            "107 th batch - loss_value: 0.029493937\n",
            "108 th batch - loss_value: 0.0066741197\n",
            "109 th batch - loss_value: 0.046362106\n",
            "# 110 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9466666666666667\n",
            "110 th batch - loss_value: 0.04828299\n",
            "111 th batch - loss_value: 0.0053919\n",
            "112 th batch - loss_value: 0.0029736497\n",
            "113 th batch - loss_value: 0.084841184\n",
            "114 th batch - loss_value: 0.01056028\n",
            "115 th batch - loss_value: 0.060143206\n",
            "116 th batch - loss_value: 0.040263318\n",
            "117 th batch - loss_value: 0.00035409376\n",
            "118 th batch - loss_value: 0.0076717255\n",
            "119 th batch - loss_value: 0.028206462\n",
            "# 120 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "120 th batch - loss_value: 0.040059403\n",
            "121 th batch - loss_value: 0.005712144\n",
            "122 th batch - loss_value: 0.004545178\n",
            "123 th batch - loss_value: 0.16285051\n",
            "124 th batch - loss_value: 0.008486805\n",
            "125 th batch - loss_value: 0.009478704\n",
            "126 th batch - loss_value: 0.002606175\n",
            "127 th batch - loss_value: 0.0027631468\n",
            "128 th batch - loss_value: 0.037193824\n",
            "129 th batch - loss_value: 0.004830742\n",
            "# 130 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "130 th batch - loss_value: 0.003790442\n",
            "131 th batch - loss_value: 0.0038511422\n",
            "132 th batch - loss_value: 0.005344332\n",
            "133 th batch - loss_value: 0.0034164337\n",
            "134 th batch - loss_value: 0.01452866\n",
            "135 th batch - loss_value: 0.07927741\n",
            "136 th batch - loss_value: 0.086427115\n",
            "137 th batch - loss_value: 0.0033778602\n",
            "138 th batch - loss_value: 0.058559723\n",
            "139 th batch - loss_value: 0.00029050725\n",
            "# 140 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "140 th batch - loss_value: 0.026062166\n",
            "141 th batch - loss_value: 0.054041218\n",
            "142 th batch - loss_value: 0.026578577\n",
            "143 th batch - loss_value: 0.008250211\n",
            "144 th batch - loss_value: 0.14050958\n",
            "145 th batch - loss_value: 0.08524808\n",
            "146 th batch - loss_value: 0.03445713\n",
            "147 th batch - loss_value: 0.004450219\n",
            "148 th batch - loss_value: 0.0026006466\n",
            "149 th batch - loss_value: 0.035392933\n",
            "# 150 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "150 th batch - loss_value: 0.023805894\n",
            "151 th batch - loss_value: 0.024803216\n",
            "152 th batch - loss_value: 0.014685888\n",
            "153 th batch - loss_value: 0.039785314\n",
            "154 th batch - loss_value: 0.028324597\n",
            "155 th batch - loss_value: 0.16142236\n",
            "156 th batch - loss_value: 0.010468052\n",
            "157 th batch - loss_value: 0.06399961\n",
            "158 th batch - loss_value: 0.0010408341\n",
            "159 th batch - loss_value: 0.04560359\n",
            "# 160 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.94\n",
            "160 th batch - loss_value: 0.026737075\n",
            "161 th batch - loss_value: 0.002361721\n",
            "162 th batch - loss_value: 0.0024584986\n",
            "163 th batch - loss_value: 0.0017328516\n",
            "164 th batch - loss_value: 0.010749459\n",
            "165 th batch - loss_value: 0.09745279\n",
            "166 th batch - loss_value: 0.0100224465\n",
            "167 th batch - loss_value: 0.0062434035\n",
            "168 th batch - loss_value: 0.12619571\n",
            "169 th batch - loss_value: 0.018999195\n",
            "# 170 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9266666666666666\n",
            "170 th batch - loss_value: 0.04605506\n",
            "171 th batch - loss_value: 0.019699287\n",
            "172 th batch - loss_value: 0.0016169633\n",
            "173 th batch - loss_value: 0.15502676\n",
            "174 th batch - loss_value: 0.015931256\n",
            "175 th batch - loss_value: 0.041336007\n",
            "176 th batch - loss_value: 0.04095507\n",
            "177 th batch - loss_value: 0.017577292\n",
            "178 th batch - loss_value: 0.007338606\n",
            "179 th batch - loss_value: 0.03138347\n",
            "# 180 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "180 th batch - loss_value: 0.055173896\n",
            "181 th batch - loss_value: 0.13000849\n",
            "182 th batch - loss_value: 0.049941257\n",
            "183 th batch - loss_value: 0.030234644\n",
            "184 th batch - loss_value: 0.09753552\n",
            "185 th batch - loss_value: 0.003152016\n",
            "186 th batch - loss_value: 0.11903831\n",
            "187 th batch - loss_value: 0.036563523\n",
            "188 th batch - loss_value: 0.0066462904\n",
            "189 th batch - loss_value: 0.18239266\n",
            "# 190 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "190 th batch - loss_value: 0.06818226\n",
            "191 th batch - loss_value: 0.14017339\n",
            "192 th batch - loss_value: 0.006491857\n",
            "193 th batch - loss_value: 0.10893453\n",
            "194 th batch - loss_value: 0.012081537\n",
            "195 th batch - loss_value: 0.04750382\n",
            "196 th batch - loss_value: 0.060803488\n",
            "197 th batch - loss_value: 0.016071338\n",
            "198 th batch - loss_value: 0.12645005\n",
            "199 th batch - loss_value: 0.07412823\n",
            "# 200 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9\n",
            "200 th batch - loss_value: 0.11700388\n",
            "201 th batch - loss_value: 0.009903601\n",
            "202 th batch - loss_value: 0.043772798\n",
            "203 th batch - loss_value: 0.008337131\n",
            "204 th batch - loss_value: 0.038824078\n",
            "205 th batch - loss_value: 0.0056637353\n",
            "206 th batch - loss_value: 0.013400569\n",
            "207 th batch - loss_value: 0.071699955\n",
            "208 th batch - loss_value: 0.049312558\n",
            "209 th batch - loss_value: 0.10312549\n",
            "# 210 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "210 th batch - loss_value: 0.034996074\n",
            "211 th batch - loss_value: 0.004466346\n",
            "212 th batch - loss_value: 0.0040443884\n",
            "213 th batch - loss_value: 0.002427554\n",
            "214 th batch - loss_value: 0.022996515\n",
            "215 th batch - loss_value: 0.07551901\n",
            "216 th batch - loss_value: 0.06673267\n",
            "217 th batch - loss_value: 0.0046874224\n",
            "218 th batch - loss_value: 0.10140361\n",
            "219 th batch - loss_value: 0.09771316\n",
            "# 220 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "220 th batch - loss_value: 0.0563814\n",
            "221 th batch - loss_value: 0.007628438\n",
            "222 th batch - loss_value: 0.0131391585\n",
            "223 th batch - loss_value: 0.0026254116\n",
            "224 th batch - loss_value: 0.03434463\n",
            "225 th batch - loss_value: 0.10664083\n",
            "226 th batch - loss_value: 0.0047524394\n",
            "227 th batch - loss_value: 0.23295236\n",
            "228 th batch - loss_value: 0.030964183\n",
            "229 th batch - loss_value: 0.11636708\n",
            "# 230 th batch - train_accuracy: 0.95 , test_accuracy: 0.9\n",
            "230 th batch - loss_value: 0.07987534\n",
            "231 th batch - loss_value: 0.0039034553\n",
            "232 th batch - loss_value: 0.03310676\n",
            "233 th batch - loss_value: 0.086513884\n",
            "234 th batch - loss_value: 0.15205596\n",
            "235 th batch - loss_value: 0.04384678\n",
            "236 th batch - loss_value: 0.017080633\n",
            "237 th batch - loss_value: 0.039974704\n",
            "238 th batch - loss_value: 0.17375287\n",
            "239 th batch - loss_value: 0.042356588\n",
            "# 240 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8933333333333333\n",
            "240 th batch - loss_value: 0.029369896\n",
            "241 th batch - loss_value: 0.13330814\n",
            "242 th batch - loss_value: 0.007145752\n",
            "243 th batch - loss_value: 0.099418186\n",
            "244 th batch - loss_value: 0.046496842\n",
            "245 th batch - loss_value: 0.06924205\n",
            "246 th batch - loss_value: 0.06394508\n",
            "247 th batch - loss_value: 0.113717325\n",
            "248 th batch - loss_value: 0.03725248\n",
            "249 th batch - loss_value: 0.19676508\n",
            "# 250 th batch - train_accuracy: 0.9333333333333333 , test_accuracy: 0.8733333333333333\n",
            "250 th batch - loss_value: 0.20181814\n",
            "251 th batch - loss_value: 0.07686928\n",
            "252 th batch - loss_value: 0.11272868\n",
            "253 th batch - loss_value: 0.034183636\n",
            "254 th batch - loss_value: 0.05734718\n",
            "255 th batch - loss_value: 0.09412531\n",
            "256 th batch - loss_value: 0.0247604\n",
            "257 th batch - loss_value: 0.0901748\n",
            "258 th batch - loss_value: 0.07627149\n",
            "259 th batch - loss_value: 0.12423639\n",
            "# 260 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9466666666666667\n",
            "260 th batch - loss_value: 0.053955667\n",
            "261 th batch - loss_value: 0.011923632\n",
            "262 th batch - loss_value: 0.012006396\n",
            "263 th batch - loss_value: 0.01658449\n",
            "264 th batch - loss_value: 0.017873159\n",
            "265 th batch - loss_value: 0.007878696\n",
            "266 th batch - loss_value: 0.12428076\n",
            "267 th batch - loss_value: 0.02396768\n",
            "268 th batch - loss_value: 0.075807\n",
            "269 th batch - loss_value: 0.15008005\n",
            "# 270 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.88\n",
            "270 th batch - loss_value: 0.018905394\n",
            "271 th batch - loss_value: 0.014968582\n",
            "272 th batch - loss_value: 0.01357426\n",
            "273 th batch - loss_value: 0.023262683\n",
            "274 th batch - loss_value: 0.11837577\n",
            "275 th batch - loss_value: 0.022996183\n",
            "276 th batch - loss_value: 0.084761865\n",
            "277 th batch - loss_value: 0.10905122\n",
            "278 th batch - loss_value: 0.011595098\n",
            "279 th batch - loss_value: 0.01745377\n",
            "# 280 th batch - train_accuracy: 1.0 , test_accuracy: 0.9\n",
            "280 th batch - loss_value: 0.0050401925\n",
            "281 th batch - loss_value: 0.02439784\n",
            "282 th batch - loss_value: 0.011785489\n",
            "283 th batch - loss_value: 0.0043383166\n",
            "284 th batch - loss_value: 0.01603264\n",
            "285 th batch - loss_value: 0.038743973\n",
            "286 th batch - loss_value: 0.036601868\n",
            "287 th batch - loss_value: 0.0010316337\n",
            "288 th batch - loss_value: 0.0075957878\n",
            "289 th batch - loss_value: 0.030933257\n",
            "# 290 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "290 th batch - loss_value: 0.009024406\n",
            "291 th batch - loss_value: 0.032410957\n",
            "292 th batch - loss_value: 0.015788367\n",
            "293 th batch - loss_value: 0.011654132\n",
            "294 th batch - loss_value: 0.07279267\n",
            "295 th batch - loss_value: 0.034989797\n",
            "296 th batch - loss_value: 0.08352701\n",
            "297 th batch - loss_value: 0.008682515\n",
            "298 th batch - loss_value: 0.02764209\n",
            "299 th batch - loss_value: 0.0039653354\n",
            "# 300 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "300 th batch - loss_value: 0.5222395\n",
            "301 th batch - loss_value: 0.04528888\n",
            "302 th batch - loss_value: 0.097625166\n",
            "303 th batch - loss_value: 0.12520443\n",
            "304 th batch - loss_value: 0.007879438\n",
            "305 th batch - loss_value: 0.0012151616\n",
            "306 th batch - loss_value: 0.01651526\n",
            "307 th batch - loss_value: 0.06306847\n",
            "308 th batch - loss_value: 0.24292792\n",
            "309 th batch - loss_value: 0.06391211\n",
            "# 310 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9066666666666666\n",
            "310 th batch - loss_value: 0.20009425\n",
            "311 th batch - loss_value: 0.02964908\n",
            "312 th batch - loss_value: 0.015219775\n",
            "313 th batch - loss_value: 0.03176298\n",
            "314 th batch - loss_value: 0.0031031726\n",
            "315 th batch - loss_value: 0.12182328\n",
            "316 th batch - loss_value: 0.11007118\n",
            "317 th batch - loss_value: 0.051348515\n",
            "318 th batch - loss_value: 0.010142756\n",
            "319 th batch - loss_value: 0.03618278\n",
            "# 320 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "320 th batch - loss_value: 0.033683114\n",
            "321 th batch - loss_value: 0.035392057\n",
            "322 th batch - loss_value: 0.01884951\n",
            "323 th batch - loss_value: 0.005562328\n",
            "324 th batch - loss_value: 0.07029106\n",
            "325 th batch - loss_value: 0.025790341\n",
            "326 th batch - loss_value: 0.017005412\n",
            "327 th batch - loss_value: 0.063062444\n",
            "328 th batch - loss_value: 0.012299956\n",
            "329 th batch - loss_value: 0.018556083\n",
            "# 330 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9266666666666666\n",
            "330 th batch - loss_value: 0.052207123\n",
            "331 th batch - loss_value: 0.008551297\n",
            "332 th batch - loss_value: 0.014783078\n",
            "333 th batch - loss_value: 0.010046709\n",
            "334 th batch - loss_value: 0.07402542\n",
            "335 th batch - loss_value: 0.009206791\n",
            "336 th batch - loss_value: 0.0042522782\n",
            "337 th batch - loss_value: 0.040459454\n",
            "338 th batch - loss_value: 0.1100046\n",
            "339 th batch - loss_value: 0.0032832797\n",
            "# 340 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8933333333333333\n",
            "340 th batch - loss_value: 0.044005282\n",
            "341 th batch - loss_value: 0.008791269\n",
            "342 th batch - loss_value: 0.073337644\n",
            "343 th batch - loss_value: 0.05947386\n",
            "344 th batch - loss_value: 0.020370295\n",
            "345 th batch - loss_value: 0.0074711917\n",
            "346 th batch - loss_value: 0.008084753\n",
            "347 th batch - loss_value: 0.033619467\n",
            "348 th batch - loss_value: 0.0057261907\n",
            "349 th batch - loss_value: 0.020405842\n",
            "# 350 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "350 th batch - loss_value: 0.008266008\n",
            "351 th batch - loss_value: 0.05635825\n",
            "352 th batch - loss_value: 0.030854855\n",
            "353 th batch - loss_value: 0.013520824\n",
            "354 th batch - loss_value: 0.0049958783\n",
            "355 th batch - loss_value: 0.038211428\n",
            "356 th batch - loss_value: 0.0045336527\n",
            "357 th batch - loss_value: 0.016268147\n",
            "358 th batch - loss_value: 0.0024332388\n",
            "359 th batch - loss_value: 0.02126807\n",
            "# 360 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "360 th batch - loss_value: 0.01604124\n",
            "361 th batch - loss_value: 0.07718077\n",
            "362 th batch - loss_value: 0.0006576352\n",
            "363 th batch - loss_value: 0.008400177\n",
            "364 th batch - loss_value: 0.0066007306\n",
            "365 th batch - loss_value: 0.0062931944\n",
            "366 th batch - loss_value: 0.07235675\n",
            "367 th batch - loss_value: 0.003178394\n",
            "368 th batch - loss_value: 0.0029828097\n",
            "369 th batch - loss_value: 0.014636163\n",
            "# 370 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "370 th batch - loss_value: 0.00369808\n",
            "371 th batch - loss_value: 0.045857366\n",
            "372 th batch - loss_value: 0.0055561275\n",
            "373 th batch - loss_value: 0.004803223\n",
            "374 th batch - loss_value: 0.01953483\n",
            "375 th batch - loss_value: 0.002472099\n",
            "376 th batch - loss_value: 0.0005352012\n",
            "377 th batch - loss_value: 0.057981487\n",
            "378 th batch - loss_value: 0.0035996095\n",
            "379 th batch - loss_value: 0.037616048\n",
            "# 380 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "380 th batch - loss_value: 0.03589693\n",
            "381 th batch - loss_value: 0.0067004906\n",
            "382 th batch - loss_value: 0.0044902596\n",
            "383 th batch - loss_value: 0.0816768\n",
            "384 th batch - loss_value: 0.19287199\n",
            "385 th batch - loss_value: 0.124480024\n",
            "386 th batch - loss_value: 0.003392708\n",
            "387 th batch - loss_value: 0.009393175\n",
            "388 th batch - loss_value: 0.0026693405\n",
            "389 th batch - loss_value: 0.0063776495\n",
            "# 390 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "390 th batch - loss_value: 0.0033115754\n",
            "391 th batch - loss_value: 0.019384699\n",
            "392 th batch - loss_value: 0.06203793\n",
            "393 th batch - loss_value: 0.22605895\n",
            "394 th batch - loss_value: 0.015038411\n",
            "395 th batch - loss_value: 0.010121516\n",
            "396 th batch - loss_value: 0.09381461\n",
            "397 th batch - loss_value: 0.020240566\n",
            "398 th batch - loss_value: 0.003285498\n",
            "399 th batch - loss_value: 0.005330795\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9\n",
            "0 th batch - loss_value: 0.14044958\n",
            "1 th batch - loss_value: 0.011878327\n",
            "2 th batch - loss_value: 0.040842492\n",
            "3 th batch - loss_value: 0.037124578\n",
            "4 th batch - loss_value: 0.0078743175\n",
            "5 th batch - loss_value: 0.010416177\n",
            "6 th batch - loss_value: 0.03799306\n",
            "7 th batch - loss_value: 0.06777627\n",
            "8 th batch - loss_value: 0.0045678564\n",
            "9 th batch - loss_value: 0.020754645\n",
            "# 10 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "10 th batch - loss_value: 0.05803003\n",
            "11 th batch - loss_value: 0.008919061\n",
            "12 th batch - loss_value: 0.020239182\n",
            "13 th batch - loss_value: 0.035256527\n",
            "14 th batch - loss_value: 0.013904329\n",
            "15 th batch - loss_value: 0.071632266\n",
            "16 th batch - loss_value: 0.019830015\n",
            "17 th batch - loss_value: 0.0077907545\n",
            "18 th batch - loss_value: 0.05060549\n",
            "19 th batch - loss_value: 0.06356194\n",
            "# 20 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "20 th batch - loss_value: 0.001190025\n",
            "21 th batch - loss_value: 0.0127888825\n",
            "22 th batch - loss_value: 0.050601996\n",
            "23 th batch - loss_value: 0.0020543125\n",
            "24 th batch - loss_value: 0.0059329704\n",
            "25 th batch - loss_value: 0.0041337707\n",
            "26 th batch - loss_value: 0.051682632\n",
            "27 th batch - loss_value: 0.07645867\n",
            "28 th batch - loss_value: 0.0059689707\n",
            "29 th batch - loss_value: 0.005017139\n",
            "# 30 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.96\n",
            "30 th batch - loss_value: 0.07125141\n",
            "31 th batch - loss_value: 0.19913068\n",
            "32 th batch - loss_value: 0.16658679\n",
            "33 th batch - loss_value: 0.00832873\n",
            "34 th batch - loss_value: 0.0030449065\n",
            "35 th batch - loss_value: 0.0040805447\n",
            "36 th batch - loss_value: 0.16940384\n",
            "37 th batch - loss_value: 0.045789216\n",
            "38 th batch - loss_value: 0.021417717\n",
            "39 th batch - loss_value: 0.01932578\n",
            "# 40 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "40 th batch - loss_value: 0.0066666584\n",
            "41 th batch - loss_value: 0.045008257\n",
            "42 th batch - loss_value: 0.0058177956\n",
            "43 th batch - loss_value: 0.024294648\n",
            "44 th batch - loss_value: 0.00073001185\n",
            "45 th batch - loss_value: 0.014492525\n",
            "46 th batch - loss_value: 0.020028602\n",
            "47 th batch - loss_value: 0.030150834\n",
            "48 th batch - loss_value: 0.0072691995\n",
            "49 th batch - loss_value: 0.029781247\n",
            "# 50 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "50 th batch - loss_value: 0.011192497\n",
            "51 th batch - loss_value: 0.02366456\n",
            "52 th batch - loss_value: 0.0274724\n",
            "53 th batch - loss_value: 0.0026964063\n",
            "54 th batch - loss_value: 0.025494684\n",
            "55 th batch - loss_value: 0.088633865\n",
            "56 th batch - loss_value: 0.012398864\n",
            "57 th batch - loss_value: 0.0025766508\n",
            "58 th batch - loss_value: 0.00588739\n",
            "59 th batch - loss_value: 0.029986223\n",
            "# 60 th batch - train_accuracy: 1.0 , test_accuracy: 0.9\n",
            "60 th batch - loss_value: 0.019232368\n",
            "61 th batch - loss_value: 0.02018446\n",
            "62 th batch - loss_value: 0.0067550624\n",
            "63 th batch - loss_value: 0.018298183\n",
            "64 th batch - loss_value: 0.007820101\n",
            "65 th batch - loss_value: 0.018678026\n",
            "66 th batch - loss_value: 0.07715687\n",
            "67 th batch - loss_value: 0.0023960765\n",
            "68 th batch - loss_value: 0.036328904\n",
            "69 th batch - loss_value: 0.0029912249\n",
            "# 70 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "70 th batch - loss_value: 0.04949995\n",
            "71 th batch - loss_value: 0.040782936\n",
            "72 th batch - loss_value: 0.039063502\n",
            "73 th batch - loss_value: 0.046456143\n",
            "74 th batch - loss_value: 0.019187469\n",
            "75 th batch - loss_value: 0.0012832466\n",
            "76 th batch - loss_value: 0.016010556\n",
            "77 th batch - loss_value: 0.013105138\n",
            "78 th batch - loss_value: 0.009778558\n",
            "79 th batch - loss_value: 0.012028666\n",
            "# 80 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "80 th batch - loss_value: 0.024489973\n",
            "81 th batch - loss_value: 0.010590756\n",
            "82 th batch - loss_value: 0.05895453\n",
            "83 th batch - loss_value: 0.06898192\n",
            "84 th batch - loss_value: 0.004691416\n",
            "85 th batch - loss_value: 0.01569166\n",
            "86 th batch - loss_value: 0.025319528\n",
            "87 th batch - loss_value: 0.0016908905\n",
            "88 th batch - loss_value: 0.085019186\n",
            "89 th batch - loss_value: 0.009814014\n",
            "# 90 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9266666666666666\n",
            "90 th batch - loss_value: 0.03570974\n",
            "91 th batch - loss_value: 0.0023012178\n",
            "92 th batch - loss_value: 0.0064509725\n",
            "93 th batch - loss_value: 0.0334311\n",
            "94 th batch - loss_value: 0.11022594\n",
            "95 th batch - loss_value: 0.048051395\n",
            "96 th batch - loss_value: 0.033448435\n",
            "97 th batch - loss_value: 0.014575081\n",
            "98 th batch - loss_value: 0.0742345\n",
            "99 th batch - loss_value: 0.0025955667\n",
            "# 100 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "100 th batch - loss_value: 0.009375183\n",
            "101 th batch - loss_value: 0.0036912442\n",
            "102 th batch - loss_value: 0.038053393\n",
            "103 th batch - loss_value: 0.0038617442\n",
            "104 th batch - loss_value: 0.047962733\n",
            "105 th batch - loss_value: 0.0045576277\n",
            "106 th batch - loss_value: 0.03725333\n",
            "107 th batch - loss_value: 0.0019517797\n",
            "108 th batch - loss_value: 0.043668937\n",
            "109 th batch - loss_value: 0.01212932\n",
            "# 110 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9\n",
            "110 th batch - loss_value: 0.032039046\n",
            "111 th batch - loss_value: 0.012102085\n",
            "112 th batch - loss_value: 0.020867221\n",
            "113 th batch - loss_value: 0.0062430273\n",
            "114 th batch - loss_value: 0.001637887\n",
            "115 th batch - loss_value: 0.00027109584\n",
            "116 th batch - loss_value: 0.012457847\n",
            "117 th batch - loss_value: 0.029638017\n",
            "118 th batch - loss_value: 0.07257137\n",
            "119 th batch - loss_value: 0.012289967\n",
            "# 120 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "120 th batch - loss_value: 0.020307096\n",
            "121 th batch - loss_value: 0.048903003\n",
            "122 th batch - loss_value: 0.04791701\n",
            "123 th batch - loss_value: 0.060659945\n",
            "124 th batch - loss_value: 0.012261942\n",
            "125 th batch - loss_value: 0.0007396465\n",
            "126 th batch - loss_value: 0.010976172\n",
            "127 th batch - loss_value: 0.0058600637\n",
            "128 th batch - loss_value: 0.00013483483\n",
            "129 th batch - loss_value: 0.0019915865\n",
            "# 130 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9466666666666667\n",
            "130 th batch - loss_value: 0.077428475\n",
            "131 th batch - loss_value: 0.027408669\n",
            "132 th batch - loss_value: 0.044114057\n",
            "133 th batch - loss_value: 0.0033385106\n",
            "134 th batch - loss_value: 0.122856766\n",
            "135 th batch - loss_value: 0.04722918\n",
            "136 th batch - loss_value: 0.17581293\n",
            "137 th batch - loss_value: 0.00784803\n",
            "138 th batch - loss_value: 0.012979729\n",
            "139 th batch - loss_value: 0.020143757\n",
            "# 140 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "140 th batch - loss_value: 0.09030994\n",
            "141 th batch - loss_value: 0.02704715\n",
            "142 th batch - loss_value: 0.006929748\n",
            "143 th batch - loss_value: 0.008196103\n",
            "144 th batch - loss_value: 0.0024588066\n",
            "145 th batch - loss_value: 0.008072777\n",
            "146 th batch - loss_value: 0.05029429\n",
            "147 th batch - loss_value: 0.056400876\n",
            "148 th batch - loss_value: 0.0756286\n",
            "149 th batch - loss_value: 0.043807507\n",
            "# 150 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "150 th batch - loss_value: 0.030537015\n",
            "151 th batch - loss_value: 0.023323812\n",
            "152 th batch - loss_value: 0.007803015\n",
            "153 th batch - loss_value: 0.19079375\n",
            "154 th batch - loss_value: 0.033490825\n",
            "155 th batch - loss_value: 0.0022441945\n",
            "156 th batch - loss_value: 0.026781654\n",
            "157 th batch - loss_value: 0.0047415453\n",
            "158 th batch - loss_value: 0.058388513\n",
            "159 th batch - loss_value: 0.12599768\n",
            "# 160 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "160 th batch - loss_value: 0.028139256\n",
            "161 th batch - loss_value: 0.022932231\n",
            "162 th batch - loss_value: 0.034428477\n",
            "163 th batch - loss_value: 0.022387965\n",
            "164 th batch - loss_value: 0.048211433\n",
            "165 th batch - loss_value: 0.029062597\n",
            "166 th batch - loss_value: 0.01290543\n",
            "167 th batch - loss_value: 0.010982215\n",
            "168 th batch - loss_value: 0.0070058107\n",
            "169 th batch - loss_value: 0.007869515\n",
            "# 170 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8866666666666667\n",
            "170 th batch - loss_value: 0.060056295\n",
            "171 th batch - loss_value: 0.017270373\n",
            "172 th batch - loss_value: 0.0066666217\n",
            "173 th batch - loss_value: 0.020748058\n",
            "174 th batch - loss_value: 0.007576973\n",
            "175 th batch - loss_value: 0.011139364\n",
            "176 th batch - loss_value: 0.026236337\n",
            "177 th batch - loss_value: 0.0023925442\n",
            "178 th batch - loss_value: 0.005402534\n",
            "179 th batch - loss_value: 0.0059378943\n",
            "# 180 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "180 th batch - loss_value: 0.008020585\n",
            "181 th batch - loss_value: 0.007159138\n",
            "182 th batch - loss_value: 0.006060988\n",
            "183 th batch - loss_value: 0.05734446\n",
            "184 th batch - loss_value: 0.13223448\n",
            "185 th batch - loss_value: 0.04474527\n",
            "186 th batch - loss_value: 0.025886199\n",
            "187 th batch - loss_value: 0.0040208944\n",
            "188 th batch - loss_value: 0.08583644\n",
            "189 th batch - loss_value: 0.011026664\n",
            "# 190 th batch - train_accuracy: 1.0 , test_accuracy: 0.9\n",
            "190 th batch - loss_value: 0.015732447\n",
            "191 th batch - loss_value: 0.038928285\n",
            "192 th batch - loss_value: 0.061676845\n",
            "193 th batch - loss_value: 0.10899275\n",
            "194 th batch - loss_value: 0.016381465\n",
            "195 th batch - loss_value: 0.016526384\n",
            "196 th batch - loss_value: 0.07310345\n",
            "197 th batch - loss_value: 0.0114296125\n",
            "198 th batch - loss_value: 0.004388481\n",
            "199 th batch - loss_value: 0.015257631\n",
            "# 200 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.8733333333333333\n",
            "200 th batch - loss_value: 0.12588963\n",
            "201 th batch - loss_value: 0.06230084\n",
            "202 th batch - loss_value: 0.116397575\n",
            "203 th batch - loss_value: 0.06931185\n",
            "204 th batch - loss_value: 0.0044492316\n",
            "205 th batch - loss_value: 0.043002095\n",
            "206 th batch - loss_value: 0.016222699\n",
            "207 th batch - loss_value: 0.014763758\n",
            "208 th batch - loss_value: 0.010740072\n",
            "209 th batch - loss_value: 0.058759052\n",
            "# 210 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "210 th batch - loss_value: 0.03620315\n",
            "211 th batch - loss_value: 0.036041234\n",
            "212 th batch - loss_value: 0.032779388\n",
            "213 th batch - loss_value: 0.018636655\n",
            "214 th batch - loss_value: 0.029366484\n",
            "215 th batch - loss_value: 0.0008665484\n",
            "216 th batch - loss_value: 0.02750382\n",
            "217 th batch - loss_value: 0.023433669\n",
            "218 th batch - loss_value: 0.029261798\n",
            "219 th batch - loss_value: 0.00965945\n",
            "# 220 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8933333333333333\n",
            "220 th batch - loss_value: 0.09334866\n",
            "221 th batch - loss_value: 0.04190737\n",
            "222 th batch - loss_value: 0.022612454\n",
            "223 th batch - loss_value: 0.008363238\n",
            "224 th batch - loss_value: 0.09227339\n",
            "225 th batch - loss_value: 0.021263607\n",
            "226 th batch - loss_value: 0.066499546\n",
            "227 th batch - loss_value: 0.013685698\n",
            "228 th batch - loss_value: 0.0007798986\n",
            "229 th batch - loss_value: 0.027715487\n",
            "# 230 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "230 th batch - loss_value: 0.008044827\n",
            "231 th batch - loss_value: 0.006331378\n",
            "232 th batch - loss_value: 0.0031110963\n",
            "233 th batch - loss_value: 0.049944445\n",
            "234 th batch - loss_value: 0.0036645054\n",
            "235 th batch - loss_value: 0.004225372\n",
            "236 th batch - loss_value: 0.06461025\n",
            "237 th batch - loss_value: 0.017933328\n",
            "238 th batch - loss_value: 0.004832159\n",
            "239 th batch - loss_value: 0.028214686\n",
            "# 240 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "240 th batch - loss_value: 0.04019309\n",
            "241 th batch - loss_value: 0.0036129048\n",
            "242 th batch - loss_value: 0.0026943444\n",
            "243 th batch - loss_value: 0.0046787537\n",
            "244 th batch - loss_value: 0.006125052\n",
            "245 th batch - loss_value: 0.25812972\n",
            "246 th batch - loss_value: 0.026980447\n",
            "247 th batch - loss_value: 0.031906586\n",
            "248 th batch - loss_value: 0.0014520341\n",
            "249 th batch - loss_value: 0.01959258\n",
            "# 250 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "250 th batch - loss_value: 0.0051111113\n",
            "251 th batch - loss_value: 0.002903103\n",
            "252 th batch - loss_value: 0.0016674396\n",
            "253 th batch - loss_value: 0.008975834\n",
            "254 th batch - loss_value: 0.040430956\n",
            "255 th batch - loss_value: 0.022773342\n",
            "256 th batch - loss_value: 0.02032867\n",
            "257 th batch - loss_value: 0.01708197\n",
            "258 th batch - loss_value: 0.06025395\n",
            "259 th batch - loss_value: 0.045655783\n",
            "# 260 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9533333333333334\n",
            "260 th batch - loss_value: 0.026066538\n",
            "261 th batch - loss_value: 0.024402\n",
            "262 th batch - loss_value: 0.0043287505\n",
            "263 th batch - loss_value: 0.05886549\n",
            "264 th batch - loss_value: 0.003094049\n",
            "265 th batch - loss_value: 0.034692943\n",
            "266 th batch - loss_value: 0.003631304\n",
            "267 th batch - loss_value: 0.12134336\n",
            "268 th batch - loss_value: 0.0013486508\n",
            "269 th batch - loss_value: 0.02947388\n",
            "# 270 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8733333333333333\n",
            "270 th batch - loss_value: 0.1793797\n",
            "271 th batch - loss_value: 0.020754991\n",
            "272 th batch - loss_value: 0.047717046\n",
            "273 th batch - loss_value: 0.0073050912\n",
            "274 th batch - loss_value: 0.013094578\n",
            "275 th batch - loss_value: 0.0005358385\n",
            "276 th batch - loss_value: 0.12354896\n",
            "277 th batch - loss_value: 0.01616631\n",
            "278 th batch - loss_value: 0.027726175\n",
            "279 th batch - loss_value: 0.009196257\n",
            "# 280 th batch - train_accuracy: 1.0 , test_accuracy: 0.9\n",
            "280 th batch - loss_value: 0.017468294\n",
            "281 th batch - loss_value: 0.025810454\n",
            "282 th batch - loss_value: 0.03627479\n",
            "283 th batch - loss_value: 0.055308294\n",
            "284 th batch - loss_value: 0.046189353\n",
            "285 th batch - loss_value: 0.01849802\n",
            "286 th batch - loss_value: 0.0042993156\n",
            "287 th batch - loss_value: 0.048134845\n",
            "288 th batch - loss_value: 0.00576721\n",
            "289 th batch - loss_value: 0.0028466887\n",
            "# 290 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "290 th batch - loss_value: 0.013440254\n",
            "291 th batch - loss_value: 0.03760999\n",
            "292 th batch - loss_value: 0.0052064084\n",
            "293 th batch - loss_value: 0.00048593755\n",
            "294 th batch - loss_value: 0.0021943336\n",
            "295 th batch - loss_value: 0.030527284\n",
            "296 th batch - loss_value: 0.00037322709\n",
            "297 th batch - loss_value: 0.016500141\n",
            "298 th batch - loss_value: 0.044237807\n",
            "299 th batch - loss_value: 0.02396863\n",
            "# 300 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "300 th batch - loss_value: 0.0073443344\n",
            "301 th batch - loss_value: 0.0036926249\n",
            "302 th batch - loss_value: 0.14669746\n",
            "303 th batch - loss_value: 0.015993856\n",
            "304 th batch - loss_value: 0.011619348\n",
            "305 th batch - loss_value: 0.005560628\n",
            "306 th batch - loss_value: 0.008567\n",
            "307 th batch - loss_value: 0.0035697373\n",
            "308 th batch - loss_value: 0.03870888\n",
            "309 th batch - loss_value: 0.057925943\n",
            "# 310 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "310 th batch - loss_value: 0.0012614914\n",
            "311 th batch - loss_value: 0.0019716222\n",
            "312 th batch - loss_value: 0.010683119\n",
            "313 th batch - loss_value: 0.0030342771\n",
            "314 th batch - loss_value: 0.016777156\n",
            "315 th batch - loss_value: 0.0023080693\n",
            "316 th batch - loss_value: 0.0026440679\n",
            "317 th batch - loss_value: 0.000884283\n",
            "318 th batch - loss_value: 0.017971579\n",
            "319 th batch - loss_value: 0.005375923\n",
            "# 320 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "320 th batch - loss_value: 0.0878483\n",
            "321 th batch - loss_value: 0.044885274\n",
            "322 th batch - loss_value: 0.014439136\n",
            "323 th batch - loss_value: 0.004580544\n",
            "324 th batch - loss_value: 0.17989278\n",
            "325 th batch - loss_value: 0.02696276\n",
            "326 th batch - loss_value: 0.011720374\n",
            "327 th batch - loss_value: 0.04678921\n",
            "328 th batch - loss_value: 0.0026522106\n",
            "329 th batch - loss_value: 0.0027686127\n",
            "# 330 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "330 th batch - loss_value: 0.017090691\n",
            "331 th batch - loss_value: 0.0054457686\n",
            "332 th batch - loss_value: 0.004278602\n",
            "333 th batch - loss_value: 0.07992773\n",
            "334 th batch - loss_value: 0.123632155\n",
            "335 th batch - loss_value: 0.018153502\n",
            "336 th batch - loss_value: 0.019597413\n",
            "337 th batch - loss_value: 0.02694414\n",
            "338 th batch - loss_value: 0.0063851336\n",
            "339 th batch - loss_value: 0.012780945\n",
            "# 340 th batch - train_accuracy: 1.0 , test_accuracy: 0.8866666666666667\n",
            "340 th batch - loss_value: 0.004056538\n",
            "341 th batch - loss_value: 0.0094223535\n",
            "342 th batch - loss_value: 0.015273468\n",
            "343 th batch - loss_value: 0.0715525\n",
            "344 th batch - loss_value: 0.12462156\n",
            "345 th batch - loss_value: 0.028437534\n",
            "346 th batch - loss_value: 0.16995735\n",
            "347 th batch - loss_value: 0.102505006\n",
            "348 th batch - loss_value: 0.029477246\n",
            "349 th batch - loss_value: 0.0018877238\n",
            "# 350 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "350 th batch - loss_value: 0.019704554\n",
            "351 th batch - loss_value: 0.05344359\n",
            "352 th batch - loss_value: 0.0169859\n",
            "353 th batch - loss_value: 0.1555197\n",
            "354 th batch - loss_value: 0.049410854\n",
            "355 th batch - loss_value: 0.06411706\n",
            "356 th batch - loss_value: 0.02763054\n",
            "357 th batch - loss_value: 0.006044389\n",
            "358 th batch - loss_value: 0.14563887\n",
            "359 th batch - loss_value: 0.02973799\n",
            "# 360 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "360 th batch - loss_value: 0.073351346\n",
            "361 th batch - loss_value: 0.019998556\n",
            "362 th batch - loss_value: 0.0074418117\n",
            "363 th batch - loss_value: 0.02731477\n",
            "364 th batch - loss_value: 0.12932132\n",
            "365 th batch - loss_value: 0.011485744\n",
            "366 th batch - loss_value: 0.060489666\n",
            "367 th batch - loss_value: 0.033211686\n",
            "368 th batch - loss_value: 0.0324335\n",
            "369 th batch - loss_value: 0.0297193\n",
            "# 370 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9\n",
            "370 th batch - loss_value: 0.016701205\n",
            "371 th batch - loss_value: 0.0019568042\n",
            "372 th batch - loss_value: 0.026399938\n",
            "373 th batch - loss_value: 0.015813852\n",
            "374 th batch - loss_value: 0.12971658\n",
            "375 th batch - loss_value: 0.026045267\n",
            "376 th batch - loss_value: 0.097028665\n",
            "377 th batch - loss_value: 0.03866298\n",
            "378 th batch - loss_value: 0.038498033\n",
            "379 th batch - loss_value: 0.01418307\n",
            "# 380 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "380 th batch - loss_value: 0.0023354427\n",
            "381 th batch - loss_value: 0.0047020693\n",
            "382 th batch - loss_value: 0.0322121\n",
            "383 th batch - loss_value: 0.043687496\n",
            "384 th batch - loss_value: 0.0034606904\n",
            "385 th batch - loss_value: 0.0058735106\n",
            "386 th batch - loss_value: 0.07251363\n",
            "387 th batch - loss_value: 0.0022854959\n",
            "388 th batch - loss_value: 0.000883182\n",
            "389 th batch - loss_value: 0.012742594\n",
            "# 390 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "390 th batch - loss_value: 0.015478101\n",
            "391 th batch - loss_value: 0.050925877\n",
            "392 th batch - loss_value: 0.05882915\n",
            "393 th batch - loss_value: 0.011352684\n",
            "394 th batch - loss_value: 0.002486232\n",
            "395 th batch - loss_value: 0.0079966225\n",
            "396 th batch - loss_value: 0.014225814\n",
            "397 th batch - loss_value: 0.007856606\n",
            "398 th batch - loss_value: 0.0047830935\n",
            "399 th batch - loss_value: 0.01664396\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "0 th batch - loss_value: 0.0008688456\n",
            "1 th batch - loss_value: 0.008494495\n",
            "2 th batch - loss_value: 0.00066862465\n",
            "3 th batch - loss_value: 0.07360001\n",
            "4 th batch - loss_value: 0.0029392005\n",
            "5 th batch - loss_value: 0.0039636362\n",
            "6 th batch - loss_value: 0.07355486\n",
            "7 th batch - loss_value: 0.0037731107\n",
            "8 th batch - loss_value: 0.003939152\n",
            "9 th batch - loss_value: 0.0005381759\n",
            "# 10 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "10 th batch - loss_value: 0.0135463895\n",
            "11 th batch - loss_value: 0.013778303\n",
            "12 th batch - loss_value: 0.010438027\n",
            "13 th batch - loss_value: 0.010507845\n",
            "14 th batch - loss_value: 0.04112368\n",
            "15 th batch - loss_value: 0.020630093\n",
            "16 th batch - loss_value: 0.046527073\n",
            "17 th batch - loss_value: 0.0012081527\n",
            "18 th batch - loss_value: 0.023301458\n",
            "19 th batch - loss_value: 0.0041325367\n",
            "# 20 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9\n",
            "20 th batch - loss_value: 0.055960204\n",
            "21 th batch - loss_value: 0.0007950522\n",
            "22 th batch - loss_value: 0.0015526784\n",
            "23 th batch - loss_value: 0.14383933\n",
            "24 th batch - loss_value: 0.024028469\n",
            "25 th batch - loss_value: 0.059814733\n",
            "26 th batch - loss_value: 0.033110064\n",
            "27 th batch - loss_value: 0.086932965\n",
            "28 th batch - loss_value: 0.04687194\n",
            "29 th batch - loss_value: 0.01959055\n",
            "# 30 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "30 th batch - loss_value: 0.009588196\n",
            "31 th batch - loss_value: 0.021429386\n",
            "32 th batch - loss_value: 0.25217152\n",
            "33 th batch - loss_value: 0.011024384\n",
            "34 th batch - loss_value: 0.030196017\n",
            "35 th batch - loss_value: 0.0077840895\n",
            "36 th batch - loss_value: 0.06932543\n",
            "37 th batch - loss_value: 0.032898303\n",
            "38 th batch - loss_value: 0.101333566\n",
            "39 th batch - loss_value: 0.14600922\n",
            "# 40 th batch - train_accuracy: 1.0 , test_accuracy: 0.9\n",
            "40 th batch - loss_value: 0.0023733897\n",
            "41 th batch - loss_value: 0.017302949\n",
            "42 th batch - loss_value: 0.005506994\n",
            "43 th batch - loss_value: 0.0141169615\n",
            "44 th batch - loss_value: 0.016690413\n",
            "45 th batch - loss_value: 0.01307984\n",
            "46 th batch - loss_value: 0.033493623\n",
            "47 th batch - loss_value: 0.06381315\n",
            "48 th batch - loss_value: 0.044523075\n",
            "49 th batch - loss_value: 0.010882038\n",
            "# 50 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.8933333333333333\n",
            "50 th batch - loss_value: 0.06399148\n",
            "51 th batch - loss_value: 0.004561232\n",
            "52 th batch - loss_value: 0.020895693\n",
            "53 th batch - loss_value: 0.045406345\n",
            "54 th batch - loss_value: 0.0022331574\n",
            "55 th batch - loss_value: 0.011730564\n",
            "56 th batch - loss_value: 0.015731113\n",
            "57 th batch - loss_value: 0.008764119\n",
            "58 th batch - loss_value: 0.03257952\n",
            "59 th batch - loss_value: 0.0046328213\n",
            "# 60 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "60 th batch - loss_value: 0.04136985\n",
            "61 th batch - loss_value: 0.07019938\n",
            "62 th batch - loss_value: 0.0026039744\n",
            "63 th batch - loss_value: 0.0019629635\n",
            "64 th batch - loss_value: 0.028719282\n",
            "65 th batch - loss_value: 0.120319456\n",
            "66 th batch - loss_value: 0.0030607402\n",
            "67 th batch - loss_value: 0.051369336\n",
            "68 th batch - loss_value: 0.01418787\n",
            "69 th batch - loss_value: 0.007792126\n",
            "# 70 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "70 th batch - loss_value: 0.0033125803\n",
            "71 th batch - loss_value: 0.036079414\n",
            "72 th batch - loss_value: 0.012008348\n",
            "73 th batch - loss_value: 0.0046333075\n",
            "74 th batch - loss_value: 0.055847917\n",
            "75 th batch - loss_value: 0.004095203\n",
            "76 th batch - loss_value: 0.054604564\n",
            "77 th batch - loss_value: 0.0018602897\n",
            "78 th batch - loss_value: 0.01956243\n",
            "79 th batch - loss_value: 0.008307383\n",
            "# 80 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "80 th batch - loss_value: 0.05723141\n",
            "81 th batch - loss_value: 0.011238779\n",
            "82 th batch - loss_value: 0.021363089\n",
            "83 th batch - loss_value: 0.031311397\n",
            "84 th batch - loss_value: 0.062110916\n",
            "85 th batch - loss_value: 0.005554608\n",
            "86 th batch - loss_value: 0.02356588\n",
            "87 th batch - loss_value: 0.002140636\n",
            "88 th batch - loss_value: 0.015331322\n",
            "89 th batch - loss_value: 0.009179663\n",
            "# 90 th batch - train_accuracy: 0.95 , test_accuracy: 0.94\n",
            "90 th batch - loss_value: 0.18401763\n",
            "91 th batch - loss_value: 0.011783951\n",
            "92 th batch - loss_value: 0.08110034\n",
            "93 th batch - loss_value: 0.009875704\n",
            "94 th batch - loss_value: 0.0011915392\n",
            "95 th batch - loss_value: 0.013603074\n",
            "96 th batch - loss_value: 0.0011784951\n",
            "97 th batch - loss_value: 0.0027307337\n",
            "98 th batch - loss_value: 0.15855724\n",
            "99 th batch - loss_value: 0.0021396566\n",
            "# 100 th batch - train_accuracy: 1.0 , test_accuracy: 0.9466666666666667\n",
            "100 th batch - loss_value: 0.00950409\n",
            "101 th batch - loss_value: 0.057784796\n",
            "102 th batch - loss_value: 0.02444187\n",
            "103 th batch - loss_value: 0.18397926\n",
            "104 th batch - loss_value: 0.004902531\n",
            "105 th batch - loss_value: 0.016188702\n",
            "106 th batch - loss_value: 0.13819452\n",
            "107 th batch - loss_value: 0.027889451\n",
            "108 th batch - loss_value: 0.1121455\n",
            "109 th batch - loss_value: 0.12218406\n",
            "# 110 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "110 th batch - loss_value: 0.021416439\n",
            "111 th batch - loss_value: 0.011040487\n",
            "112 th batch - loss_value: 0.0067217085\n",
            "113 th batch - loss_value: 0.014700205\n",
            "114 th batch - loss_value: 0.1643098\n",
            "115 th batch - loss_value: 0.008722481\n",
            "116 th batch - loss_value: 0.065569624\n",
            "117 th batch - loss_value: 0.017185206\n",
            "118 th batch - loss_value: 0.059022058\n",
            "119 th batch - loss_value: 0.018361667\n",
            "# 120 th batch - train_accuracy: 0.95 , test_accuracy: 0.92\n",
            "120 th batch - loss_value: 0.12322399\n",
            "121 th batch - loss_value: 0.011115426\n",
            "122 th batch - loss_value: 0.065310016\n",
            "123 th batch - loss_value: 0.0068405797\n",
            "124 th batch - loss_value: 0.07328202\n",
            "125 th batch - loss_value: 0.06153055\n",
            "126 th batch - loss_value: 0.075301886\n",
            "127 th batch - loss_value: 0.005013545\n",
            "128 th batch - loss_value: 0.004738797\n",
            "129 th batch - loss_value: 0.082133524\n",
            "# 130 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "130 th batch - loss_value: 0.03065941\n",
            "131 th batch - loss_value: 0.0953784\n",
            "132 th batch - loss_value: 0.15636332\n",
            "133 th batch - loss_value: 0.014297709\n",
            "134 th batch - loss_value: 0.015364998\n",
            "135 th batch - loss_value: 0.024805961\n",
            "136 th batch - loss_value: 0.059043963\n",
            "137 th batch - loss_value: 0.01295756\n",
            "138 th batch - loss_value: 0.014919786\n",
            "139 th batch - loss_value: 0.009298072\n",
            "# 140 th batch - train_accuracy: 1.0 , test_accuracy: 0.8666666666666667\n",
            "140 th batch - loss_value: 0.00823249\n",
            "141 th batch - loss_value: 0.02358885\n",
            "142 th batch - loss_value: 0.01442558\n",
            "143 th batch - loss_value: 0.013240397\n",
            "144 th batch - loss_value: 0.008395157\n",
            "145 th batch - loss_value: 0.0032591342\n",
            "146 th batch - loss_value: 0.0044016195\n",
            "147 th batch - loss_value: 0.0042588953\n",
            "148 th batch - loss_value: 0.012679671\n",
            "149 th batch - loss_value: 0.00096278713\n",
            "# 150 th batch - train_accuracy: 1.0 , test_accuracy: 0.9\n",
            "150 th batch - loss_value: 0.010915713\n",
            "151 th batch - loss_value: 0.041175637\n",
            "152 th batch - loss_value: 0.14196819\n",
            "153 th batch - loss_value: 0.002033878\n",
            "154 th batch - loss_value: 0.028823819\n",
            "155 th batch - loss_value: 0.04558323\n",
            "156 th batch - loss_value: 0.0060877027\n",
            "157 th batch - loss_value: 0.22475447\n",
            "158 th batch - loss_value: 0.0072018537\n",
            "159 th batch - loss_value: 0.039204698\n",
            "# 160 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "160 th batch - loss_value: 0.005353248\n",
            "161 th batch - loss_value: 0.00044205316\n",
            "162 th batch - loss_value: 0.004602677\n",
            "163 th batch - loss_value: 0.054990303\n",
            "164 th batch - loss_value: 0.004432357\n",
            "165 th batch - loss_value: 0.045171913\n",
            "166 th batch - loss_value: 0.005420257\n",
            "167 th batch - loss_value: 0.12529814\n",
            "168 th batch - loss_value: 0.0071019535\n",
            "169 th batch - loss_value: 0.012635439\n",
            "# 170 th batch - train_accuracy: 1.0 , test_accuracy: 0.9266666666666666\n",
            "170 th batch - loss_value: 0.00978674\n",
            "171 th batch - loss_value: 0.027442891\n",
            "172 th batch - loss_value: 0.14475626\n",
            "173 th batch - loss_value: 0.0635743\n",
            "174 th batch - loss_value: 0.065703005\n",
            "175 th batch - loss_value: 0.00918222\n",
            "176 th batch - loss_value: 0.009242867\n",
            "177 th batch - loss_value: 0.009746061\n",
            "178 th batch - loss_value: 0.076858364\n",
            "179 th batch - loss_value: 0.010238073\n",
            "# 180 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "180 th batch - loss_value: 0.0054051154\n",
            "181 th batch - loss_value: 0.027388748\n",
            "182 th batch - loss_value: 0.01333874\n",
            "183 th batch - loss_value: 0.039357137\n",
            "184 th batch - loss_value: 0.01978935\n",
            "185 th batch - loss_value: 0.00352023\n",
            "186 th batch - loss_value: 0.104403384\n",
            "187 th batch - loss_value: 0.05674776\n",
            "188 th batch - loss_value: 0.002777834\n",
            "189 th batch - loss_value: 0.029317085\n",
            "# 190 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "190 th batch - loss_value: 0.0011561259\n",
            "191 th batch - loss_value: 0.020949295\n",
            "192 th batch - loss_value: 0.028941374\n",
            "193 th batch - loss_value: 0.12887186\n",
            "194 th batch - loss_value: 0.071590856\n",
            "195 th batch - loss_value: 0.017776279\n",
            "196 th batch - loss_value: 0.036210764\n",
            "197 th batch - loss_value: 0.084529676\n",
            "198 th batch - loss_value: 0.0017022223\n",
            "199 th batch - loss_value: 0.12343575\n",
            "# 200 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "200 th batch - loss_value: 0.005393503\n",
            "201 th batch - loss_value: 0.0057501355\n",
            "202 th batch - loss_value: 0.0113985045\n",
            "203 th batch - loss_value: 0.040068146\n",
            "204 th batch - loss_value: 0.021678763\n",
            "205 th batch - loss_value: 0.024059081\n",
            "206 th batch - loss_value: 0.1763418\n",
            "207 th batch - loss_value: 0.037058756\n",
            "208 th batch - loss_value: 0.06426401\n",
            "209 th batch - loss_value: 0.04170604\n",
            "# 210 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "210 th batch - loss_value: 0.017858991\n",
            "211 th batch - loss_value: 0.005737956\n",
            "212 th batch - loss_value: 0.01392201\n",
            "213 th batch - loss_value: 0.00880875\n",
            "214 th batch - loss_value: 0.00401549\n",
            "215 th batch - loss_value: 0.004379486\n",
            "216 th batch - loss_value: 0.005952953\n",
            "217 th batch - loss_value: 0.009029891\n",
            "218 th batch - loss_value: 0.049241636\n",
            "219 th batch - loss_value: 0.12895118\n",
            "# 220 th batch - train_accuracy: 1.0 , test_accuracy: 0.8733333333333333\n",
            "220 th batch - loss_value: 0.016661687\n",
            "221 th batch - loss_value: 0.059612796\n",
            "222 th batch - loss_value: 0.009476171\n",
            "223 th batch - loss_value: 0.025247842\n",
            "224 th batch - loss_value: 0.00636477\n",
            "225 th batch - loss_value: 0.0026003472\n",
            "226 th batch - loss_value: 0.022289129\n",
            "227 th batch - loss_value: 0.013166433\n",
            "228 th batch - loss_value: 0.0009647193\n",
            "229 th batch - loss_value: 0.014289393\n",
            "# 230 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8866666666666667\n",
            "230 th batch - loss_value: 0.03321437\n",
            "231 th batch - loss_value: 0.0020920804\n",
            "232 th batch - loss_value: 0.0005869598\n",
            "233 th batch - loss_value: 0.016312921\n",
            "234 th batch - loss_value: 0.00572038\n",
            "235 th batch - loss_value: 0.0037801575\n",
            "236 th batch - loss_value: 0.0070119477\n",
            "237 th batch - loss_value: 0.0172085\n",
            "238 th batch - loss_value: 0.026927343\n",
            "239 th batch - loss_value: 0.09465186\n",
            "# 240 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.88\n",
            "240 th batch - loss_value: 0.019569583\n",
            "241 th batch - loss_value: 0.001504427\n",
            "242 th batch - loss_value: 0.0077621546\n",
            "243 th batch - loss_value: 0.0049607214\n",
            "244 th batch - loss_value: 0.004620117\n",
            "245 th batch - loss_value: 0.05946243\n",
            "246 th batch - loss_value: 0.0024888485\n",
            "247 th batch - loss_value: 0.0064504542\n",
            "248 th batch - loss_value: 0.017554583\n",
            "249 th batch - loss_value: 0.033416286\n",
            "# 250 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "250 th batch - loss_value: 0.004562808\n",
            "251 th batch - loss_value: 0.032073878\n",
            "252 th batch - loss_value: 0.0012481047\n",
            "253 th batch - loss_value: 0.002877157\n",
            "254 th batch - loss_value: 0.013626241\n",
            "255 th batch - loss_value: 0.009423168\n",
            "256 th batch - loss_value: 0.0075330334\n",
            "257 th batch - loss_value: 0.020147104\n",
            "258 th batch - loss_value: 0.13390891\n",
            "259 th batch - loss_value: 0.0013056023\n",
            "# 260 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "260 th batch - loss_value: 0.0011601745\n",
            "261 th batch - loss_value: 0.0016629427\n",
            "262 th batch - loss_value: 0.008126454\n",
            "263 th batch - loss_value: 0.0023738886\n",
            "264 th batch - loss_value: 0.0020516932\n",
            "265 th batch - loss_value: 0.010648817\n",
            "266 th batch - loss_value: 0.00035668287\n",
            "267 th batch - loss_value: 0.0014491972\n",
            "268 th batch - loss_value: 0.00016831681\n",
            "269 th batch - loss_value: 0.039193396\n",
            "# 270 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9333333333333333\n",
            "270 th batch - loss_value: 0.021710118\n",
            "271 th batch - loss_value: 0.0071173324\n",
            "272 th batch - loss_value: 0.029730221\n",
            "273 th batch - loss_value: 0.003326473\n",
            "274 th batch - loss_value: 0.0038719848\n",
            "275 th batch - loss_value: 0.012571806\n",
            "276 th batch - loss_value: 0.08775929\n",
            "277 th batch - loss_value: 0.0012145985\n",
            "278 th batch - loss_value: 0.00912167\n",
            "279 th batch - loss_value: 0.088585116\n",
            "# 280 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "280 th batch - loss_value: 0.0074948836\n",
            "281 th batch - loss_value: 0.020986354\n",
            "282 th batch - loss_value: 0.0027478116\n",
            "283 th batch - loss_value: 0.00031626786\n",
            "284 th batch - loss_value: 0.0030928382\n",
            "285 th batch - loss_value: 0.034205396\n",
            "286 th batch - loss_value: 0.0019341406\n",
            "287 th batch - loss_value: 0.01912999\n",
            "288 th batch - loss_value: 0.057675615\n",
            "289 th batch - loss_value: 0.001820848\n",
            "# 290 th batch - train_accuracy: 1.0 , test_accuracy: 0.8933333333333333\n",
            "290 th batch - loss_value: 0.0056426646\n",
            "291 th batch - loss_value: 0.012595091\n",
            "292 th batch - loss_value: 0.035543088\n",
            "293 th batch - loss_value: 0.0043667117\n",
            "294 th batch - loss_value: 0.05612924\n",
            "295 th batch - loss_value: 0.00054421625\n",
            "296 th batch - loss_value: 0.04084735\n",
            "297 th batch - loss_value: 0.003861491\n",
            "298 th batch - loss_value: 0.0120915\n",
            "299 th batch - loss_value: 0.0034285556\n",
            "# 300 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9266666666666666\n",
            "300 th batch - loss_value: 0.04671491\n",
            "301 th batch - loss_value: 0.0037788625\n",
            "302 th batch - loss_value: 0.03283004\n",
            "303 th batch - loss_value: 0.0014390902\n",
            "304 th batch - loss_value: 0.031174101\n",
            "305 th batch - loss_value: 0.0013870505\n",
            "306 th batch - loss_value: 0.007897671\n",
            "307 th batch - loss_value: 0.0050275354\n",
            "308 th batch - loss_value: 0.0061033694\n",
            "309 th batch - loss_value: 0.021386871\n",
            "# 310 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "310 th batch - loss_value: 0.044900306\n",
            "311 th batch - loss_value: 0.02912786\n",
            "312 th batch - loss_value: 0.017679006\n",
            "313 th batch - loss_value: 0.00021344014\n",
            "314 th batch - loss_value: 0.013752895\n",
            "315 th batch - loss_value: 0.0008873116\n",
            "316 th batch - loss_value: 0.008291268\n",
            "317 th batch - loss_value: 0.011467244\n",
            "318 th batch - loss_value: 0.0030808358\n",
            "319 th batch - loss_value: 0.0018924593\n",
            "# 320 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "320 th batch - loss_value: 0.0010453024\n",
            "321 th batch - loss_value: 0.0060704653\n",
            "322 th batch - loss_value: 0.0031201912\n",
            "323 th batch - loss_value: 0.015537693\n",
            "324 th batch - loss_value: 0.012474499\n",
            "325 th batch - loss_value: 0.0008080526\n",
            "326 th batch - loss_value: 0.040826995\n",
            "327 th batch - loss_value: 0.10259298\n",
            "328 th batch - loss_value: 0.0006281624\n",
            "329 th batch - loss_value: 0.0059729824\n",
            "# 330 th batch - train_accuracy: 1.0 , test_accuracy: 0.88\n",
            "330 th batch - loss_value: 0.0016738022\n",
            "331 th batch - loss_value: 0.00055149687\n",
            "332 th batch - loss_value: 0.013443055\n",
            "333 th batch - loss_value: 0.12816736\n",
            "334 th batch - loss_value: 0.0051544975\n",
            "335 th batch - loss_value: 0.022075312\n",
            "336 th batch - loss_value: 0.008139114\n",
            "337 th batch - loss_value: 0.040845867\n",
            "338 th batch - loss_value: 0.0007508799\n",
            "339 th batch - loss_value: 0.11583225\n",
            "# 340 th batch - train_accuracy: 1.0 , test_accuracy: 0.8733333333333333\n",
            "340 th batch - loss_value: 0.0035092677\n",
            "341 th batch - loss_value: 0.0008002035\n",
            "342 th batch - loss_value: 0.052430674\n",
            "343 th batch - loss_value: 0.0790904\n",
            "344 th batch - loss_value: 0.004851103\n",
            "345 th batch - loss_value: 0.002091182\n",
            "346 th batch - loss_value: 0.0019441124\n",
            "347 th batch - loss_value: 0.0009893468\n",
            "348 th batch - loss_value: 0.07126313\n",
            "349 th batch - loss_value: 0.13341236\n",
            "# 350 th batch - train_accuracy: 1.0 , test_accuracy: 0.9\n",
            "350 th batch - loss_value: 0.010510875\n",
            "351 th batch - loss_value: 0.23677927\n",
            "352 th batch - loss_value: 0.16505724\n",
            "353 th batch - loss_value: 0.0046246257\n",
            "354 th batch - loss_value: 0.0060747936\n",
            "355 th batch - loss_value: 0.13099596\n",
            "356 th batch - loss_value: 0.0097735245\n",
            "357 th batch - loss_value: 0.031227993\n",
            "358 th batch - loss_value: 0.26661044\n",
            "359 th batch - loss_value: 0.31506404\n",
            "# 360 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "360 th batch - loss_value: 0.044238262\n",
            "361 th batch - loss_value: 0.021956615\n",
            "362 th batch - loss_value: 0.0009707078\n",
            "363 th batch - loss_value: 0.015251678\n",
            "364 th batch - loss_value: 0.010862578\n",
            "365 th batch - loss_value: 0.017314462\n",
            "366 th batch - loss_value: 0.0108814975\n",
            "367 th batch - loss_value: 0.015384092\n",
            "368 th batch - loss_value: 0.046629086\n",
            "369 th batch - loss_value: 0.070355244\n",
            "# 370 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8866666666666667\n",
            "370 th batch - loss_value: 0.013265628\n",
            "371 th batch - loss_value: 0.004401471\n",
            "372 th batch - loss_value: 0.13228665\n",
            "373 th batch - loss_value: 0.06669939\n",
            "374 th batch - loss_value: 0.003651072\n",
            "375 th batch - loss_value: 0.13327362\n",
            "376 th batch - loss_value: 0.002617762\n",
            "377 th batch - loss_value: 0.09545756\n",
            "378 th batch - loss_value: 0.051744293\n",
            "379 th batch - loss_value: 0.29810333\n",
            "# 380 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.92\n",
            "380 th batch - loss_value: 0.12016497\n",
            "381 th batch - loss_value: 0.13823318\n",
            "382 th batch - loss_value: 0.0046157455\n",
            "383 th batch - loss_value: 0.020499349\n",
            "384 th batch - loss_value: 0.022644317\n",
            "385 th batch - loss_value: 0.122915745\n",
            "386 th batch - loss_value: 0.032137003\n",
            "387 th batch - loss_value: 0.046839304\n",
            "388 th batch - loss_value: 0.2315958\n",
            "389 th batch - loss_value: 0.20424834\n",
            "# 390 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.8933333333333333\n",
            "390 th batch - loss_value: 0.15257746\n",
            "391 th batch - loss_value: 0.07057727\n",
            "392 th batch - loss_value: 0.004481943\n",
            "393 th batch - loss_value: 0.015423388\n",
            "394 th batch - loss_value: 0.012912881\n",
            "395 th batch - loss_value: 0.035669006\n",
            "396 th batch - loss_value: 0.059585426\n",
            "397 th batch - loss_value: 0.08523167\n",
            "398 th batch - loss_value: 0.025107218\n",
            "399 th batch - loss_value: 0.03796854\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 0 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9\n",
            "0 th batch - loss_value: 0.018649485\n",
            "1 th batch - loss_value: 0.019962037\n",
            "2 th batch - loss_value: 0.03350825\n",
            "3 th batch - loss_value: 0.07298097\n",
            "4 th batch - loss_value: 0.048098866\n",
            "5 th batch - loss_value: 0.18388078\n",
            "6 th batch - loss_value: 0.07165577\n",
            "7 th batch - loss_value: 0.088316664\n",
            "8 th batch - loss_value: 0.099674836\n",
            "9 th batch - loss_value: 0.030363003\n",
            "# 10 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9466666666666667\n",
            "10 th batch - loss_value: 0.033790942\n",
            "11 th batch - loss_value: 0.0073582404\n",
            "12 th batch - loss_value: 0.0048233345\n",
            "13 th batch - loss_value: 0.08175768\n",
            "14 th batch - loss_value: 0.03598939\n",
            "15 th batch - loss_value: 0.015223692\n",
            "16 th batch - loss_value: 0.06785418\n",
            "17 th batch - loss_value: 0.10428632\n",
            "18 th batch - loss_value: 0.009165928\n",
            "19 th batch - loss_value: 0.083676115\n",
            "# 20 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "20 th batch - loss_value: 0.0058763684\n",
            "21 th batch - loss_value: 0.031163335\n",
            "22 th batch - loss_value: 0.026992895\n",
            "23 th batch - loss_value: 0.018210497\n",
            "24 th batch - loss_value: 0.035651516\n",
            "25 th batch - loss_value: 0.011099691\n",
            "26 th batch - loss_value: 0.016204646\n",
            "27 th batch - loss_value: 0.1679832\n",
            "28 th batch - loss_value: 0.022362582\n",
            "29 th batch - loss_value: 0.029431354\n",
            "# 30 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "30 th batch - loss_value: 0.007897321\n",
            "31 th batch - loss_value: 0.37480032\n",
            "32 th batch - loss_value: 0.14444342\n",
            "33 th batch - loss_value: 0.03773686\n",
            "34 th batch - loss_value: 0.00876397\n",
            "35 th batch - loss_value: 0.045646064\n",
            "36 th batch - loss_value: 0.033608112\n",
            "37 th batch - loss_value: 0.007803137\n",
            "38 th batch - loss_value: 0.0062215673\n",
            "39 th batch - loss_value: 0.044997398\n",
            "# 40 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "40 th batch - loss_value: 0.04074253\n",
            "41 th batch - loss_value: 0.04016804\n",
            "42 th batch - loss_value: 0.124362424\n",
            "43 th batch - loss_value: 0.037524853\n",
            "44 th batch - loss_value: 0.015259771\n",
            "45 th batch - loss_value: 0.056532063\n",
            "46 th batch - loss_value: 0.018327327\n",
            "47 th batch - loss_value: 0.009636621\n",
            "48 th batch - loss_value: 0.18037213\n",
            "49 th batch - loss_value: 0.033040024\n",
            "# 50 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "50 th batch - loss_value: 0.04977418\n",
            "51 th batch - loss_value: 0.065573305\n",
            "52 th batch - loss_value: 0.053868152\n",
            "53 th batch - loss_value: 0.027414937\n",
            "54 th batch - loss_value: 0.0067221206\n",
            "55 th batch - loss_value: 0.022056121\n",
            "56 th batch - loss_value: 0.007948216\n",
            "57 th batch - loss_value: 0.027971169\n",
            "58 th batch - loss_value: 0.027984941\n",
            "59 th batch - loss_value: 0.0083843265\n",
            "# 60 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.8933333333333333\n",
            "60 th batch - loss_value: 0.08412339\n",
            "61 th batch - loss_value: 0.034026396\n",
            "62 th batch - loss_value: 0.06544469\n",
            "63 th batch - loss_value: 0.0073137977\n",
            "64 th batch - loss_value: 0.052153956\n",
            "65 th batch - loss_value: 0.04771205\n",
            "66 th batch - loss_value: 0.0436979\n",
            "67 th batch - loss_value: 0.03666688\n",
            "68 th batch - loss_value: 0.0032632912\n",
            "69 th batch - loss_value: 0.05328756\n",
            "# 70 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9\n",
            "70 th batch - loss_value: 0.062586024\n",
            "71 th batch - loss_value: 0.014233112\n",
            "72 th batch - loss_value: 0.00631761\n",
            "73 th batch - loss_value: 0.036990143\n",
            "74 th batch - loss_value: 0.017651286\n",
            "75 th batch - loss_value: 0.03695573\n",
            "76 th batch - loss_value: 0.01869201\n",
            "77 th batch - loss_value: 0.0052434886\n",
            "78 th batch - loss_value: 0.02066683\n",
            "79 th batch - loss_value: 0.0025445276\n",
            "# 80 th batch - train_accuracy: 1.0 , test_accuracy: 0.92\n",
            "80 th batch - loss_value: 0.004618706\n",
            "81 th batch - loss_value: 0.0019022609\n",
            "82 th batch - loss_value: 0.010879183\n",
            "83 th batch - loss_value: 0.044934954\n",
            "84 th batch - loss_value: 0.08452712\n",
            "85 th batch - loss_value: 0.18378188\n",
            "86 th batch - loss_value: 0.00020851078\n",
            "87 th batch - loss_value: 0.02585557\n",
            "88 th batch - loss_value: 0.004128611\n",
            "89 th batch - loss_value: 0.013260401\n",
            "# 90 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9533333333333334\n",
            "90 th batch - loss_value: 0.050567046\n",
            "91 th batch - loss_value: 0.12855679\n",
            "92 th batch - loss_value: 0.013190363\n",
            "93 th batch - loss_value: 0.020980308\n",
            "94 th batch - loss_value: 0.0997768\n",
            "95 th batch - loss_value: 0.022058627\n",
            "96 th batch - loss_value: 0.003269601\n",
            "97 th batch - loss_value: 0.085629836\n",
            "98 th batch - loss_value: 0.07748789\n",
            "99 th batch - loss_value: 0.020426808\n",
            "# 100 th batch - train_accuracy: 1.0 , test_accuracy: 0.9333333333333333\n",
            "100 th batch - loss_value: 0.01861623\n",
            "101 th batch - loss_value: 0.08481986\n",
            "102 th batch - loss_value: 0.008808003\n",
            "103 th batch - loss_value: 0.024660442\n",
            "104 th batch - loss_value: 0.2005028\n",
            "105 th batch - loss_value: 0.02643686\n",
            "106 th batch - loss_value: 0.0068372497\n",
            "107 th batch - loss_value: 0.017684879\n",
            "108 th batch - loss_value: 0.05764629\n",
            "109 th batch - loss_value: 0.06647143\n",
            "# 110 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.88\n",
            "110 th batch - loss_value: 0.026167078\n",
            "111 th batch - loss_value: 0.035386186\n",
            "112 th batch - loss_value: 0.024852514\n",
            "113 th batch - loss_value: 0.06660332\n",
            "114 th batch - loss_value: 0.087134734\n",
            "115 th batch - loss_value: 0.0053541544\n",
            "116 th batch - loss_value: 0.017991817\n",
            "117 th batch - loss_value: 0.020048302\n",
            "118 th batch - loss_value: 0.025080912\n",
            "119 th batch - loss_value: 0.034132216\n",
            "# 120 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "120 th batch - loss_value: 0.003153329\n",
            "121 th batch - loss_value: 0.001192628\n",
            "122 th batch - loss_value: 0.07387502\n",
            "123 th batch - loss_value: 0.032285657\n",
            "124 th batch - loss_value: 0.0117564015\n",
            "125 th batch - loss_value: 0.026861582\n",
            "126 th batch - loss_value: 0.013550409\n",
            "127 th batch - loss_value: 0.010196546\n",
            "128 th batch - loss_value: 0.023507353\n",
            "129 th batch - loss_value: 0.03754842\n",
            "# 130 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "130 th batch - loss_value: 0.08540817\n",
            "131 th batch - loss_value: 0.025622405\n",
            "132 th batch - loss_value: 0.0009746287\n",
            "133 th batch - loss_value: 0.018066801\n",
            "134 th batch - loss_value: 0.0010224502\n",
            "135 th batch - loss_value: 0.013388088\n",
            "136 th batch - loss_value: 0.013090545\n",
            "137 th batch - loss_value: 0.034501556\n",
            "138 th batch - loss_value: 0.037153997\n",
            "139 th batch - loss_value: 0.11792027\n",
            "# 140 th batch - train_accuracy: 1.0 , test_accuracy: 0.94\n",
            "140 th batch - loss_value: 0.005540839\n",
            "141 th batch - loss_value: 0.00204228\n",
            "142 th batch - loss_value: 0.09184745\n",
            "143 th batch - loss_value: 0.01752006\n",
            "144 th batch - loss_value: 0.06985536\n",
            "145 th batch - loss_value: 0.0030456556\n",
            "146 th batch - loss_value: 0.003562019\n",
            "147 th batch - loss_value: 0.030939508\n",
            "148 th batch - loss_value: 0.058148775\n",
            "149 th batch - loss_value: 0.016786916\n",
            "# 150 th batch - train_accuracy: 1.0 , test_accuracy: 0.8866666666666667\n",
            "150 th batch - loss_value: 0.03024351\n",
            "151 th batch - loss_value: 0.03152803\n",
            "152 th batch - loss_value: 0.0074253203\n",
            "153 th batch - loss_value: 0.0062005124\n",
            "154 th batch - loss_value: 0.0068274643\n",
            "155 th batch - loss_value: 0.05849576\n",
            "156 th batch - loss_value: 0.08397994\n",
            "157 th batch - loss_value: 0.02331491\n",
            "158 th batch - loss_value: 0.08947083\n",
            "159 th batch - loss_value: 0.01247595\n",
            "# 160 th batch - train_accuracy: 1.0 , test_accuracy: 0.9066666666666666\n",
            "160 th batch - loss_value: 0.008462091\n",
            "161 th batch - loss_value: 0.030052109\n",
            "162 th batch - loss_value: 0.0025369439\n",
            "163 th batch - loss_value: 0.008393178\n",
            "164 th batch - loss_value: 0.023310203\n",
            "165 th batch - loss_value: 0.012867695\n",
            "166 th batch - loss_value: 0.06893583\n",
            "167 th batch - loss_value: 0.011938659\n",
            "168 th batch - loss_value: 0.026585435\n",
            "169 th batch - loss_value: 0.022638468\n",
            "# 170 th batch - train_accuracy: 1.0 , test_accuracy: 0.96\n",
            "170 th batch - loss_value: 0.0053987512\n",
            "171 th batch - loss_value: 0.019235348\n",
            "172 th batch - loss_value: 0.0005437563\n",
            "173 th batch - loss_value: 0.008614017\n",
            "174 th batch - loss_value: 0.028725764\n",
            "175 th batch - loss_value: 0.014681288\n",
            "176 th batch - loss_value: 0.011342896\n",
            "177 th batch - loss_value: 0.053869832\n",
            "178 th batch - loss_value: 0.088738345\n",
            "179 th batch - loss_value: 0.047460537\n",
            "# 180 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.8666666666666667\n",
            "180 th batch - loss_value: 0.20928465\n",
            "181 th batch - loss_value: 0.019075165\n",
            "182 th batch - loss_value: 0.08149209\n",
            "183 th batch - loss_value: 0.08582934\n",
            "184 th batch - loss_value: 0.019570155\n",
            "185 th batch - loss_value: 0.012474427\n",
            "186 th batch - loss_value: 0.0027748123\n",
            "187 th batch - loss_value: 0.0029398322\n",
            "188 th batch - loss_value: 0.042347416\n",
            "189 th batch - loss_value: 0.17891671\n",
            "# 190 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9066666666666666\n",
            "190 th batch - loss_value: 0.038534965\n",
            "191 th batch - loss_value: 0.025717285\n",
            "192 th batch - loss_value: 0.05226439\n",
            "193 th batch - loss_value: 0.00634775\n",
            "194 th batch - loss_value: 0.03476912\n",
            "195 th batch - loss_value: 0.0035187996\n",
            "196 th batch - loss_value: 0.003087214\n",
            "197 th batch - loss_value: 0.0012835362\n",
            "198 th batch - loss_value: 0.010391502\n",
            "199 th batch - loss_value: 0.024703579\n",
            "# 200 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8733333333333333\n",
            "200 th batch - loss_value: 0.07320924\n",
            "201 th batch - loss_value: 0.3506241\n",
            "202 th batch - loss_value: 0.026587358\n",
            "203 th batch - loss_value: 0.014616002\n",
            "204 th batch - loss_value: 0.02495061\n",
            "205 th batch - loss_value: 0.004601649\n",
            "206 th batch - loss_value: 0.030386537\n",
            "207 th batch - loss_value: 0.01296751\n",
            "208 th batch - loss_value: 0.09721807\n",
            "209 th batch - loss_value: 0.01722108\n",
            "# 210 th batch - train_accuracy: 0.95 , test_accuracy: 0.9133333333333333\n",
            "210 th batch - loss_value: 0.19157489\n",
            "211 th batch - loss_value: 0.040476143\n",
            "212 th batch - loss_value: 0.23569116\n",
            "213 th batch - loss_value: 0.011874631\n",
            "214 th batch - loss_value: 0.022731131\n",
            "215 th batch - loss_value: 0.011609303\n",
            "216 th batch - loss_value: 0.0075269165\n",
            "217 th batch - loss_value: 0.020538045\n",
            "218 th batch - loss_value: 0.025672244\n",
            "219 th batch - loss_value: 0.034725543\n",
            "# 220 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.8266666666666667\n",
            "220 th batch - loss_value: 0.17055209\n",
            "221 th batch - loss_value: 0.049297206\n",
            "222 th batch - loss_value: 0.05705257\n",
            "223 th batch - loss_value: 0.116194725\n",
            "224 th batch - loss_value: 0.014391278\n",
            "225 th batch - loss_value: 0.043667037\n",
            "226 th batch - loss_value: 0.026903858\n",
            "227 th batch - loss_value: 0.010960455\n",
            "228 th batch - loss_value: 0.050861385\n",
            "229 th batch - loss_value: 0.02117443\n",
            "# 230 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8866666666666667\n",
            "230 th batch - loss_value: 0.013789938\n",
            "231 th batch - loss_value: 0.058524795\n",
            "232 th batch - loss_value: 0.031681113\n",
            "233 th batch - loss_value: 0.02923976\n",
            "234 th batch - loss_value: 0.019335007\n",
            "235 th batch - loss_value: 0.036891717\n",
            "236 th batch - loss_value: 0.20175694\n",
            "237 th batch - loss_value: 0.0060662692\n",
            "238 th batch - loss_value: 0.057899423\n",
            "239 th batch - loss_value: 0.054035623\n",
            "# 240 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8933333333333333\n",
            "240 th batch - loss_value: 0.022935672\n",
            "241 th batch - loss_value: 0.0150395725\n",
            "242 th batch - loss_value: 0.04268212\n",
            "243 th batch - loss_value: 0.07676138\n",
            "244 th batch - loss_value: 0.012085666\n",
            "245 th batch - loss_value: 0.050088763\n",
            "246 th batch - loss_value: 0.04789816\n",
            "247 th batch - loss_value: 0.005778785\n",
            "248 th batch - loss_value: 0.0036908914\n",
            "249 th batch - loss_value: 0.015345114\n",
            "# 250 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "250 th batch - loss_value: 0.033987176\n",
            "251 th batch - loss_value: 0.015997997\n",
            "252 th batch - loss_value: 0.046845023\n",
            "253 th batch - loss_value: 0.013182396\n",
            "254 th batch - loss_value: 0.033661548\n",
            "255 th batch - loss_value: 0.19554892\n",
            "256 th batch - loss_value: 0.046381246\n",
            "257 th batch - loss_value: 0.016964192\n",
            "258 th batch - loss_value: 0.048383255\n",
            "259 th batch - loss_value: 0.010726661\n",
            "# 260 th batch - train_accuracy: 1.0 , test_accuracy: 0.9533333333333334\n",
            "260 th batch - loss_value: 0.0018713181\n",
            "261 th batch - loss_value: 0.011653901\n",
            "262 th batch - loss_value: 0.034850676\n",
            "263 th batch - loss_value: 0.05465223\n",
            "264 th batch - loss_value: 0.014736181\n",
            "265 th batch - loss_value: 0.024895988\n",
            "266 th batch - loss_value: 0.0034304762\n",
            "267 th batch - loss_value: 0.017173512\n",
            "268 th batch - loss_value: 0.073953405\n",
            "269 th batch - loss_value: 0.06572739\n",
            "# 270 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.92\n",
            "270 th batch - loss_value: 0.06998951\n",
            "271 th batch - loss_value: 0.12536302\n",
            "272 th batch - loss_value: 0.016086657\n",
            "273 th batch - loss_value: 0.09126647\n",
            "274 th batch - loss_value: 0.009908253\n",
            "275 th batch - loss_value: 0.0069920053\n",
            "276 th batch - loss_value: 0.0069334134\n",
            "277 th batch - loss_value: 0.065463975\n",
            "278 th batch - loss_value: 0.04062829\n",
            "279 th batch - loss_value: 0.017423034\n",
            "# 280 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8933333333333333\n",
            "280 th batch - loss_value: 0.09017299\n",
            "281 th batch - loss_value: 0.010379729\n",
            "282 th batch - loss_value: 0.07402532\n",
            "283 th batch - loss_value: 0.15353101\n",
            "284 th batch - loss_value: 0.021891406\n",
            "285 th batch - loss_value: 0.007481744\n",
            "286 th batch - loss_value: 0.013136443\n",
            "287 th batch - loss_value: 0.027853202\n",
            "288 th batch - loss_value: 0.103484705\n",
            "289 th batch - loss_value: 0.05071928\n",
            "# 290 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.8666666666666667\n",
            "290 th batch - loss_value: 0.04516783\n",
            "291 th batch - loss_value: 0.028164782\n",
            "292 th batch - loss_value: 0.17902221\n",
            "293 th batch - loss_value: 0.14087829\n",
            "294 th batch - loss_value: 0.002492067\n",
            "295 th batch - loss_value: 0.089692995\n",
            "296 th batch - loss_value: 0.020786146\n",
            "297 th batch - loss_value: 0.08446779\n",
            "298 th batch - loss_value: 0.034164038\n",
            "299 th batch - loss_value: 0.08084914\n",
            "# 300 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.9466666666666667\n",
            "300 th batch - loss_value: 0.09808655\n",
            "301 th batch - loss_value: 0.032598134\n",
            "302 th batch - loss_value: 0.042552467\n",
            "303 th batch - loss_value: 0.014491876\n",
            "304 th batch - loss_value: 0.032772675\n",
            "305 th batch - loss_value: 0.06762712\n",
            "306 th batch - loss_value: 0.007872509\n",
            "307 th batch - loss_value: 0.07958195\n",
            "308 th batch - loss_value: 0.041840192\n",
            "309 th batch - loss_value: 0.049106304\n",
            "# 310 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.86\n",
            "310 th batch - loss_value: 0.019188229\n",
            "311 th batch - loss_value: 0.017011859\n",
            "312 th batch - loss_value: 0.05138506\n",
            "313 th batch - loss_value: 0.07724721\n",
            "314 th batch - loss_value: 0.12971713\n",
            "315 th batch - loss_value: 0.1803343\n",
            "316 th batch - loss_value: 0.0035771234\n",
            "317 th batch - loss_value: 0.034043677\n",
            "318 th batch - loss_value: 0.01121582\n",
            "319 th batch - loss_value: 0.037259426\n",
            "# 320 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.94\n",
            "320 th batch - loss_value: 0.11026739\n",
            "321 th batch - loss_value: 0.015222246\n",
            "322 th batch - loss_value: 0.054709814\n",
            "323 th batch - loss_value: 0.039209977\n",
            "324 th batch - loss_value: 0.0054128836\n",
            "325 th batch - loss_value: 0.006239578\n",
            "326 th batch - loss_value: 0.011665025\n",
            "327 th batch - loss_value: 0.06614812\n",
            "328 th batch - loss_value: 0.07389731\n",
            "329 th batch - loss_value: 0.04097575\n",
            "# 330 th batch - train_accuracy: 1.0 , test_accuracy: 0.8666666666666667\n",
            "330 th batch - loss_value: 0.008277516\n",
            "331 th batch - loss_value: 0.044615336\n",
            "332 th batch - loss_value: 0.0068537225\n",
            "333 th batch - loss_value: 0.004253445\n",
            "334 th batch - loss_value: 0.0722308\n",
            "335 th batch - loss_value: 0.003809559\n",
            "336 th batch - loss_value: 0.020151353\n",
            "337 th batch - loss_value: 0.005536502\n",
            "338 th batch - loss_value: 0.009432807\n",
            "339 th batch - loss_value: 0.014090889\n",
            "# 340 th batch - train_accuracy: 1.0 , test_accuracy: 0.8866666666666667\n",
            "340 th batch - loss_value: 0.0022714545\n",
            "341 th batch - loss_value: 0.03745357\n",
            "342 th batch - loss_value: 0.010443119\n",
            "343 th batch - loss_value: 0.0215293\n",
            "344 th batch - loss_value: 0.00225484\n",
            "345 th batch - loss_value: 0.0012096488\n",
            "346 th batch - loss_value: 0.014496376\n",
            "347 th batch - loss_value: 0.0667371\n",
            "348 th batch - loss_value: 0.053854246\n",
            "349 th batch - loss_value: 0.032597277\n",
            "# 350 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9133333333333333\n",
            "350 th batch - loss_value: 0.024264034\n",
            "351 th batch - loss_value: 0.012212188\n",
            "352 th batch - loss_value: 0.019625675\n",
            "353 th batch - loss_value: 0.02705755\n",
            "354 th batch - loss_value: 0.0013504331\n",
            "355 th batch - loss_value: 0.08049978\n",
            "356 th batch - loss_value: 0.008595695\n",
            "357 th batch - loss_value: 0.013768206\n",
            "358 th batch - loss_value: 0.02525924\n",
            "359 th batch - loss_value: 0.011022474\n",
            "# 360 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.94\n",
            "360 th batch - loss_value: 0.05211671\n",
            "361 th batch - loss_value: 0.003640408\n",
            "362 th batch - loss_value: 0.0011294271\n",
            "363 th batch - loss_value: 0.02015594\n",
            "364 th batch - loss_value: 0.017647875\n",
            "365 th batch - loss_value: 0.0029807652\n",
            "366 th batch - loss_value: 0.032766152\n",
            "367 th batch - loss_value: 0.036907505\n",
            "368 th batch - loss_value: 0.011757841\n",
            "369 th batch - loss_value: 0.034356542\n",
            "# 370 th batch - train_accuracy: 0.9833333333333333 , test_accuracy: 0.9\n",
            "370 th batch - loss_value: 0.12154283\n",
            "371 th batch - loss_value: 0.28093794\n",
            "372 th batch - loss_value: 0.02977691\n",
            "373 th batch - loss_value: 0.05978792\n",
            "374 th batch - loss_value: 0.007536524\n",
            "375 th batch - loss_value: 0.0029704778\n",
            "376 th batch - loss_value: 0.034046017\n",
            "377 th batch - loss_value: 0.014829605\n",
            "378 th batch - loss_value: 0.065607674\n",
            "379 th batch - loss_value: 0.025926886\n",
            "# 380 th batch - train_accuracy: 1.0 , test_accuracy: 0.9133333333333333\n",
            "380 th batch - loss_value: 0.005562636\n",
            "381 th batch - loss_value: 0.0213449\n",
            "382 th batch - loss_value: 0.034727585\n",
            "383 th batch - loss_value: 0.040420156\n",
            "384 th batch - loss_value: 0.00089234894\n",
            "385 th batch - loss_value: 0.0049255807\n",
            "386 th batch - loss_value: 0.06093004\n",
            "387 th batch - loss_value: 0.006994453\n",
            "388 th batch - loss_value: 0.055967033\n",
            "389 th batch - loss_value: 0.02605766\n",
            "# 390 th batch - train_accuracy: 0.9666666666666667 , test_accuracy: 0.8933333333333333\n",
            "390 th batch - loss_value: 0.19529837\n",
            "391 th batch - loss_value: 0.01724006\n",
            "392 th batch - loss_value: 0.0015383423\n",
            "393 th batch - loss_value: 0.17675287\n",
            "394 th batch - loss_value: 0.039447\n",
            "395 th batch - loss_value: 0.009876494\n",
            "396 th batch - loss_value: 0.025973588\n",
            "397 th batch - loss_value: 0.010191553\n",
            "398 th batch - loss_value: 0.003766361\n",
            "399 th batch - loss_value: 0.073983796\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: resnet18_lr3/assets\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: resnet18_lr4/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UTgUhK5R6gN"
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/gdrive/MyDrive/projectimg/resnet18_lr4\") #모델 불러오기."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nYpAmRYSWG0w",
        "outputId": "5df54eb9-c288-4bbb-9cb8-e5a05e52f441"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"/content/gdrive/MyDrive/projectimg/resnet18_lr4\") #tflite파일로 변환\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfPtrXwjWHE8"
      },
      "source": [
        "with open('./resnet18_orii.tflite', 'wb') as f: #resnet18_orii.tflitef라는 파일이름으로 현재위치에 저장.\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n_cwMb35VsI"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-PZ062Dnfay"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU0CvdlgWHam"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}